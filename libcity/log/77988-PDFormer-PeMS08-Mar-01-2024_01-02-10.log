2024-03-01 01:02:10,530 - INFO - Log directory: ./libcity/log
2024-03-01 01:02:10,530 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS08, exp_id=77988
2024-03-01 01:02:10,530 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS08', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 2776, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 21, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 6, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 1, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS08'], 'geo_file': 'PeMS08', 'rel_file': 'PeMS08', 'adp_file': 'PeMS08', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 77988}
2024-03-01 01:02:10,824 - INFO - Loaded file PeMS08.geo, num_nodes=170
2024-03-01 01:02:10,826 - INFO - set_weight_link_or_dist: link
2024-03-01 01:02:10,826 - INFO - init_weight_inf_or_zero: zero
2024-03-01 01:02:10,828 - INFO - Loaded file PeMS08.rel, shape=(170, 170)
2024-03-01 01:02:10,828 - INFO - Max adj_mx value = 1.0
2024-03-01 01:02:20,523 - INFO - Loading file PeMS08.dyna
2024-03-01 01:02:22,245 - INFO - Loaded file PeMS08.dyna, shape=(17856, 170, 1)
2024-03-01 01:02:22,265 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS08.npy
2024-03-01 01:02:22,266 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS08_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-03-01 01:02:29,013 - INFO - train	x: (10700, 12, 170, 9), y: (10700, 12, 170, 9), ind: (10700,)
2024-03-01 01:02:29,013 - INFO - eval	x: (3566, 12, 170, 9), y: (3566, 12, 170, 9), ind: (3566,)
2024-03-01 01:02:29,013 - INFO - test	x: (3567, 12, 170, 9), y: (3567, 12, 170, 9), ind: (3567,)
2024-03-01 01:02:29,435 - INFO - StandardScaler mean: 229.8431355598314, std: 145.62553066568907
2024-03-01 01:02:29,435 - INFO - NoneScaler
2024-03-01 01:02:30,628 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS08_21_3_16_5.npy
2024-03-01 01:02:30,630 - INFO - Use use_curriculum_learning!
2024-03-01 01:02:34,127 - INFO - Number of isolated points: 0
2024-03-01 01:02:34,137 - INFO - Number of isolated points: 0
2024-03-01 01:02:34,184 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-03-01 01:02:34,186 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-01 01:02:34,186 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,187 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,188 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 01:02:34,189 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,190 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,191 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 01:02:34,192 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,193 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,193 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,193 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 01:02:34,193 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,193 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 01:02:34,193 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,208 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 01:02:34,209 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,210 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,211 - INFO - encoder_blocks.5.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - encoder_blocks.5.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,212 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.5.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-03-01 01:02:34,213 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-03-01 01:02:34,214 - INFO - Total parameter numbers: 1104093
2024-03-01 01:02:34,216 - INFO - You select `adamw` optimizer.
2024-03-01 01:02:34,217 - INFO - You select `cosinelr` lr_scheduler.
2024-03-01 01:02:34,217 - WARNING - Received none train loss func and will use the loss func defined in the model.
2024-03-01 01:02:34,218 - INFO - Number of isolated points: 0
2024-03-01 01:02:34,229 - INFO - Start training ...
2024-03-01 01:02:34,229 - INFO - num_batches:669
2024-03-01 01:02:34,296 - INFO - Training: task_level increase from 0 to 1
2024-03-01 01:02:34,296 - INFO - Current batches_seen is 0
2024-03-01 01:04:50,032 - INFO - epoch complete!
2024-03-01 01:04:50,032 - INFO - evaluating now!
2024-03-01 01:05:02,515 - INFO - Epoch [0/300] (669) train_loss: 249.6562, val_loss: 249.6458, lr: 0.000201, 148.29s
2024-03-01 01:05:02,570 - INFO - Saved model at 0
2024-03-01 01:05:02,571 - INFO - Val loss decrease from inf to 249.6458, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch0.tar
2024-03-01 01:07:35,927 - INFO - epoch complete!
2024-03-01 01:07:35,928 - INFO - evaluating now!
2024-03-01 01:07:47,897 - INFO - Epoch [1/300] (1338) train_loss: 53.4116, val_loss: 193.7208, lr: 0.000401, 165.33s
2024-03-01 01:07:47,948 - INFO - Saved model at 1
2024-03-01 01:07:47,948 - INFO - Val loss decrease from 249.6458 to 193.7208, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch1.tar
2024-03-01 01:10:17,027 - INFO - epoch complete!
2024-03-01 01:10:17,028 - INFO - evaluating now!
2024-03-01 01:10:28,470 - INFO - Epoch [2/300] (2007) train_loss: 36.4620, val_loss: 195.9336, lr: 0.000600, 160.52s
2024-03-01 01:12:58,337 - INFO - epoch complete!
2024-03-01 01:12:58,337 - INFO - evaluating now!
2024-03-01 01:13:10,038 - INFO - Epoch [3/300] (2676) train_loss: 32.8160, val_loss: 194.8591, lr: 0.000800, 161.57s
2024-03-01 01:13:31,827 - INFO - Training: task_level increase from 1 to 2
2024-03-01 01:13:31,827 - INFO - Current batches_seen is 2776
2024-03-01 01:15:39,968 - INFO - epoch complete!
2024-03-01 01:15:39,969 - INFO - evaluating now!
2024-03-01 01:15:51,886 - INFO - Epoch [4/300] (3345) train_loss: 36.3388, val_loss: 171.4306, lr: 0.000999, 161.85s
2024-03-01 01:15:51,937 - INFO - Saved model at 4
2024-03-01 01:15:51,937 - INFO - Val loss decrease from 193.7208 to 171.4306, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch4.tar
2024-03-01 01:18:21,957 - INFO - epoch complete!
2024-03-01 01:18:21,958 - INFO - evaluating now!
2024-03-01 01:18:33,696 - INFO - Epoch [5/300] (4014) train_loss: 30.9543, val_loss: 172.4082, lr: 0.000999, 161.76s
2024-03-01 01:21:07,866 - INFO - epoch complete!
2024-03-01 01:21:07,866 - INFO - evaluating now!
2024-03-01 01:21:18,931 - INFO - Epoch [6/300] (4683) train_loss: 29.8620, val_loss: 172.4182, lr: 0.000999, 165.23s
2024-03-01 01:23:50,144 - INFO - epoch complete!
2024-03-01 01:23:50,145 - INFO - evaluating now!
2024-03-01 01:24:01,584 - INFO - Epoch [7/300] (5352) train_loss: 29.2565, val_loss: 173.1078, lr: 0.000998, 162.65s
2024-03-01 01:24:49,258 - INFO - Training: task_level increase from 2 to 3
2024-03-01 01:24:49,259 - INFO - Current batches_seen is 5552
2024-03-01 01:26:31,574 - INFO - epoch complete!
2024-03-01 01:26:31,574 - INFO - evaluating now!
2024-03-01 01:26:43,186 - INFO - Epoch [8/300] (6021) train_loss: 31.0257, val_loss: 155.7582, lr: 0.000998, 161.60s
2024-03-01 01:26:43,236 - INFO - Saved model at 8
2024-03-01 01:26:43,237 - INFO - Val loss decrease from 171.4306 to 155.7582, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch8.tar
2024-03-01 01:29:10,749 - INFO - epoch complete!
2024-03-01 01:29:10,750 - INFO - evaluating now!
2024-03-01 01:29:22,771 - INFO - Epoch [9/300] (6690) train_loss: 29.9293, val_loss: 155.6549, lr: 0.000998, 159.53s
2024-03-01 01:29:22,821 - INFO - Saved model at 9
2024-03-01 01:29:22,822 - INFO - Val loss decrease from 155.7582 to 155.6549, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch9.tar
2024-03-01 01:31:48,749 - INFO - epoch complete!
2024-03-01 01:31:48,749 - INFO - evaluating now!
2024-03-01 01:32:00,649 - INFO - Epoch [10/300] (7359) train_loss: 29.2154, val_loss: 156.1381, lr: 0.000997, 157.83s
2024-03-01 01:34:25,965 - INFO - epoch complete!
2024-03-01 01:34:25,966 - INFO - evaluating now!
2024-03-01 01:34:37,868 - INFO - Epoch [11/300] (8028) train_loss: 29.0211, val_loss: 156.5995, lr: 0.000996, 157.22s
2024-03-01 01:35:42,845 - INFO - Training: task_level increase from 3 to 4
2024-03-01 01:35:42,845 - INFO - Current batches_seen is 8328
2024-03-01 01:37:02,070 - INFO - epoch complete!
2024-03-01 01:37:02,070 - INFO - evaluating now!
2024-03-01 01:37:13,490 - INFO - Epoch [12/300] (8697) train_loss: 30.1373, val_loss: 143.0505, lr: 0.000996, 155.62s
2024-03-01 01:37:13,541 - INFO - Saved model at 12
2024-03-01 01:37:13,542 - INFO - Val loss decrease from 155.6549 to 143.0505, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch12.tar
2024-03-01 01:39:39,010 - INFO - epoch complete!
2024-03-01 01:39:39,011 - INFO - evaluating now!
2024-03-01 01:39:49,967 - INFO - Epoch [13/300] (9366) train_loss: 29.8883, val_loss: 143.8779, lr: 0.000995, 156.43s
2024-03-01 01:42:16,039 - INFO - epoch complete!
2024-03-01 01:42:16,039 - INFO - evaluating now!
2024-03-01 01:42:27,018 - INFO - Epoch [14/300] (10035) train_loss: 29.4312, val_loss: 142.3151, lr: 0.000994, 157.05s
2024-03-01 01:42:27,068 - INFO - Saved model at 14
2024-03-01 01:42:27,068 - INFO - Val loss decrease from 143.0505 to 142.3151, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch14.tar
2024-03-01 01:44:53,070 - INFO - epoch complete!
2024-03-01 01:44:53,071 - INFO - evaluating now!
2024-03-01 01:45:04,118 - INFO - Epoch [15/300] (10704) train_loss: 29.0028, val_loss: 142.3689, lr: 0.000994, 157.05s
2024-03-01 01:46:31,822 - INFO - Training: task_level increase from 4 to 5
2024-03-01 01:46:31,822 - INFO - Current batches_seen is 11104
2024-03-01 01:47:30,255 - INFO - epoch complete!
2024-03-01 01:47:30,255 - INFO - evaluating now!
2024-03-01 01:47:41,379 - INFO - Epoch [16/300] (11373) train_loss: 29.4772, val_loss: 126.5205, lr: 0.000993, 157.26s
2024-03-01 01:47:41,429 - INFO - Saved model at 16
2024-03-01 01:47:41,430 - INFO - Val loss decrease from 142.3151 to 126.5205, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch16.tar
2024-03-01 01:50:07,775 - INFO - epoch complete!
2024-03-01 01:50:07,776 - INFO - evaluating now!
2024-03-01 01:50:18,856 - INFO - Epoch [17/300] (12042) train_loss: 29.6896, val_loss: 126.1048, lr: 0.000992, 157.43s
2024-03-01 01:50:18,905 - INFO - Saved model at 17
2024-03-01 01:50:18,906 - INFO - Val loss decrease from 126.5205 to 126.1048, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch17.tar
2024-03-01 01:52:45,267 - INFO - epoch complete!
2024-03-01 01:52:45,267 - INFO - evaluating now!
2024-03-01 01:52:56,507 - INFO - Epoch [18/300] (12711) train_loss: 29.0815, val_loss: 125.3328, lr: 0.000991, 157.60s
2024-03-01 01:52:56,558 - INFO - Saved model at 18
2024-03-01 01:52:56,558 - INFO - Val loss decrease from 126.1048 to 125.3328, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch18.tar
2024-03-01 01:55:21,580 - INFO - epoch complete!
2024-03-01 01:55:21,581 - INFO - evaluating now!
2024-03-01 01:55:32,897 - INFO - Epoch [19/300] (13380) train_loss: 29.0248, val_loss: 125.1680, lr: 0.000990, 156.34s
2024-03-01 01:55:32,947 - INFO - Saved model at 19
2024-03-01 01:55:32,948 - INFO - Val loss decrease from 125.3328 to 125.1680, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch19.tar
2024-03-01 01:57:22,173 - INFO - Training: task_level increase from 5 to 6
2024-03-01 01:57:22,173 - INFO - Current batches_seen is 13880
2024-03-01 01:57:58,646 - INFO - epoch complete!
2024-03-01 01:57:58,647 - INFO - evaluating now!
2024-03-01 01:58:09,919 - INFO - Epoch [20/300] (14049) train_loss: 28.8195, val_loss: 123.0321, lr: 0.000989, 156.97s
2024-03-01 01:58:09,971 - INFO - Saved model at 20
2024-03-01 01:58:09,972 - INFO - Val loss decrease from 125.1680 to 123.0321, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch20.tar
2024-03-01 02:00:35,033 - INFO - epoch complete!
2024-03-01 02:00:35,034 - INFO - evaluating now!
2024-03-01 02:00:46,187 - INFO - Epoch [21/300] (14718) train_loss: 29.3747, val_loss: 123.2883, lr: 0.000988, 156.22s
2024-03-01 02:03:14,013 - INFO - epoch complete!
2024-03-01 02:03:14,013 - INFO - evaluating now!
2024-03-01 02:03:24,984 - INFO - Epoch [22/300] (15387) train_loss: 28.6922, val_loss: 123.6426, lr: 0.000987, 158.80s
2024-03-01 02:05:55,273 - INFO - epoch complete!
2024-03-01 02:05:55,273 - INFO - evaluating now!
2024-03-01 02:06:06,678 - INFO - Epoch [23/300] (16056) train_loss: 28.7336, val_loss: 123.5957, lr: 0.000986, 161.69s
2024-03-01 02:08:23,363 - INFO - Training: task_level increase from 6 to 7
2024-03-01 02:08:23,364 - INFO - Current batches_seen is 16656
2024-03-01 02:08:37,584 - INFO - epoch complete!
2024-03-01 02:08:37,585 - INFO - evaluating now!
2024-03-01 02:08:49,488 - INFO - Epoch [24/300] (16725) train_loss: 29.1384, val_loss: 106.8831, lr: 0.000985, 162.81s
2024-03-01 02:08:49,539 - INFO - Saved model at 24
2024-03-01 02:08:49,539 - INFO - Val loss decrease from 123.0321 to 106.8831, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch24.tar
2024-03-01 02:11:16,096 - INFO - epoch complete!
2024-03-01 02:11:16,096 - INFO - evaluating now!
2024-03-01 02:11:28,067 - INFO - Epoch [25/300] (17394) train_loss: 29.0852, val_loss: 106.9695, lr: 0.000983, 158.53s
2024-03-01 02:13:54,547 - INFO - epoch complete!
2024-03-01 02:13:54,547 - INFO - evaluating now!
2024-03-01 02:14:06,397 - INFO - Epoch [26/300] (18063) train_loss: 28.7690, val_loss: 106.4228, lr: 0.000982, 158.33s
2024-03-01 02:14:06,449 - INFO - Saved model at 26
2024-03-01 02:14:06,449 - INFO - Val loss decrease from 106.8831 to 106.4228, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch26.tar
2024-03-01 02:16:32,129 - INFO - epoch complete!
2024-03-01 02:16:32,129 - INFO - evaluating now!
2024-03-01 02:16:43,581 - INFO - Epoch [27/300] (18732) train_loss: 28.3750, val_loss: 106.6316, lr: 0.000981, 157.13s
2024-03-01 02:19:09,475 - INFO - epoch complete!
2024-03-01 02:19:09,475 - INFO - evaluating now!
2024-03-01 02:19:21,290 - INFO - Epoch [28/300] (19401) train_loss: 28.1626, val_loss: 106.7631, lr: 0.000979, 157.71s
2024-03-01 02:19:28,258 - INFO - Training: task_level increase from 7 to 8
2024-03-01 02:19:28,258 - INFO - Current batches_seen is 19432
2024-03-01 02:21:51,896 - INFO - epoch complete!
2024-03-01 02:21:51,897 - INFO - evaluating now!
2024-03-01 02:22:03,993 - INFO - Epoch [29/300] (20070) train_loss: 28.9617, val_loss: 91.0096, lr: 0.000978, 162.70s
2024-03-01 02:22:04,046 - INFO - Saved model at 29
2024-03-01 02:22:04,047 - INFO - Val loss decrease from 106.4228 to 91.0096, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch29.tar
2024-03-01 02:24:35,024 - INFO - epoch complete!
2024-03-01 02:24:35,025 - INFO - evaluating now!
2024-03-01 02:24:47,068 - INFO - Epoch [30/300] (20739) train_loss: 28.3727, val_loss: 91.4103, lr: 0.000976, 163.02s
2024-03-01 02:27:20,449 - INFO - epoch complete!
2024-03-01 02:27:20,450 - INFO - evaluating now!
2024-03-01 02:27:32,387 - INFO - Epoch [31/300] (21408) train_loss: 28.3585, val_loss: 90.8882, lr: 0.000975, 165.32s
2024-03-01 02:27:32,437 - INFO - Saved model at 31
2024-03-01 02:27:32,437 - INFO - Val loss decrease from 91.0096 to 90.8882, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch31.tar
2024-03-01 02:29:57,160 - INFO - epoch complete!
2024-03-01 02:29:57,161 - INFO - evaluating now!
2024-03-01 02:30:08,895 - INFO - Epoch [32/300] (22077) train_loss: 28.0463, val_loss: 91.2812, lr: 0.000973, 156.46s
2024-03-01 02:30:37,055 - INFO - Training: task_level increase from 8 to 9
2024-03-01 02:30:37,055 - INFO - Current batches_seen is 22208
2024-03-01 02:32:39,499 - INFO - epoch complete!
2024-03-01 02:32:39,499 - INFO - evaluating now!
2024-03-01 02:32:51,386 - INFO - Epoch [33/300] (22746) train_loss: 28.4996, val_loss: 76.7144, lr: 0.000972, 162.49s
2024-03-01 02:32:51,436 - INFO - Saved model at 33
2024-03-01 02:32:51,436 - INFO - Val loss decrease from 90.8882 to 76.7144, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch33.tar
2024-03-01 02:35:20,637 - INFO - epoch complete!
2024-03-01 02:35:20,638 - INFO - evaluating now!
2024-03-01 02:35:32,535 - INFO - Epoch [34/300] (23415) train_loss: 28.4429, val_loss: 76.7116, lr: 0.000970, 161.10s
2024-03-01 02:35:32,587 - INFO - Saved model at 34
2024-03-01 02:35:32,587 - INFO - Val loss decrease from 76.7144 to 76.7116, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch34.tar
2024-03-01 02:38:01,327 - INFO - epoch complete!
2024-03-01 02:38:01,327 - INFO - evaluating now!
2024-03-01 02:38:13,411 - INFO - Epoch [35/300] (24084) train_loss: 27.9816, val_loss: 76.9696, lr: 0.000968, 160.82s
2024-03-01 02:40:38,829 - INFO - epoch complete!
2024-03-01 02:40:38,830 - INFO - evaluating now!
2024-03-01 02:40:50,853 - INFO - Epoch [36/300] (24753) train_loss: 27.9689, val_loss: 76.6615, lr: 0.000967, 157.44s
2024-03-01 02:40:50,903 - INFO - Saved model at 36
2024-03-01 02:40:50,903 - INFO - Val loss decrease from 76.7116 to 76.6615, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch36.tar
2024-03-01 02:41:41,722 - INFO - Training: task_level increase from 9 to 10
2024-03-01 02:41:41,723 - INFO - Current batches_seen is 24984
2024-03-01 02:43:17,406 - INFO - epoch complete!
2024-03-01 02:43:17,407 - INFO - evaluating now!
2024-03-01 02:43:29,360 - INFO - Epoch [37/300] (25422) train_loss: 28.5317, val_loss: 61.1007, lr: 0.000965, 158.46s
2024-03-01 02:43:29,410 - INFO - Saved model at 37
2024-03-01 02:43:29,410 - INFO - Val loss decrease from 76.6615 to 61.1007, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch37.tar
2024-03-01 02:45:54,623 - INFO - epoch complete!
2024-03-01 02:45:54,623 - INFO - evaluating now!
2024-03-01 02:46:06,759 - INFO - Epoch [38/300] (26091) train_loss: 28.2715, val_loss: 60.8789, lr: 0.000963, 157.35s
2024-03-01 02:46:06,945 - INFO - Saved model at 38
2024-03-01 02:46:06,946 - INFO - Val loss decrease from 61.1007 to 60.8789, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch38.tar
2024-03-01 02:48:32,557 - INFO - epoch complete!
2024-03-01 02:48:32,558 - INFO - evaluating now!
2024-03-01 02:48:44,479 - INFO - Epoch [39/300] (26760) train_loss: 28.2030, val_loss: 60.3006, lr: 0.000961, 157.53s
2024-03-01 02:48:44,529 - INFO - Saved model at 39
2024-03-01 02:48:44,529 - INFO - Val loss decrease from 60.8789 to 60.3006, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch39.tar
2024-03-01 02:51:10,991 - INFO - epoch complete!
2024-03-01 02:51:10,991 - INFO - evaluating now!
2024-03-01 02:51:22,877 - INFO - Epoch [40/300] (27429) train_loss: 28.1210, val_loss: 60.4230, lr: 0.000959, 158.35s
2024-03-01 02:52:35,817 - INFO - Training: task_level increase from 10 to 11
2024-03-01 02:52:35,817 - INFO - Current batches_seen is 27760
2024-03-01 02:53:48,932 - INFO - epoch complete!
2024-03-01 02:53:48,933 - INFO - evaluating now!
2024-03-01 02:54:00,878 - INFO - Epoch [41/300] (28098) train_loss: 28.1923, val_loss: 45.5194, lr: 0.000957, 158.00s
2024-03-01 02:54:00,927 - INFO - Saved model at 41
2024-03-01 02:54:00,928 - INFO - Val loss decrease from 60.3006 to 45.5194, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch41.tar
2024-03-01 02:56:25,965 - INFO - epoch complete!
2024-03-01 02:56:25,966 - INFO - evaluating now!
2024-03-01 02:56:37,245 - INFO - Epoch [42/300] (28767) train_loss: 28.2750, val_loss: 46.2494, lr: 0.000955, 156.32s
2024-03-01 02:59:01,462 - INFO - epoch complete!
2024-03-01 02:59:01,462 - INFO - evaluating now!
2024-03-01 02:59:13,273 - INFO - Epoch [43/300] (29436) train_loss: 28.0754, val_loss: 45.3211, lr: 0.000953, 156.03s
2024-03-01 02:59:13,323 - INFO - Saved model at 43
2024-03-01 02:59:13,323 - INFO - Val loss decrease from 45.5194 to 45.3211, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch43.tar
2024-03-01 03:01:38,162 - INFO - epoch complete!
2024-03-01 03:01:38,162 - INFO - evaluating now!
2024-03-01 03:01:50,106 - INFO - Epoch [44/300] (30105) train_loss: 28.0107, val_loss: 45.1933, lr: 0.000951, 156.78s
2024-03-01 03:01:50,159 - INFO - Saved model at 44
2024-03-01 03:01:50,159 - INFO - Val loss decrease from 45.3211 to 45.1933, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch44.tar
2024-03-01 03:03:23,844 - INFO - Training: task_level increase from 11 to 12
2024-03-01 03:03:23,844 - INFO - Current batches_seen is 30536
2024-03-01 03:04:15,582 - INFO - epoch complete!
2024-03-01 03:04:15,582 - INFO - evaluating now!
2024-03-01 03:04:27,491 - INFO - Epoch [45/300] (30774) train_loss: 28.2883, val_loss: 28.5873, lr: 0.000949, 157.33s
2024-03-01 03:04:27,541 - INFO - Saved model at 45
2024-03-01 03:04:27,541 - INFO - Val loss decrease from 45.1933 to 28.5873, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch45.tar
2024-03-01 03:06:53,685 - INFO - epoch complete!
2024-03-01 03:06:53,686 - INFO - evaluating now!
2024-03-01 03:07:05,386 - INFO - Epoch [46/300] (31443) train_loss: 28.1819, val_loss: 27.7788, lr: 0.000947, 157.84s
2024-03-01 03:07:05,437 - INFO - Saved model at 46
2024-03-01 03:07:05,438 - INFO - Val loss decrease from 28.5873 to 27.7788, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch46.tar
2024-03-01 03:09:29,314 - INFO - epoch complete!
2024-03-01 03:09:29,315 - INFO - evaluating now!
2024-03-01 03:09:41,278 - INFO - Epoch [47/300] (32112) train_loss: 28.3628, val_loss: 27.8500, lr: 0.000944, 155.84s
2024-03-01 03:12:06,390 - INFO - epoch complete!
2024-03-01 03:12:06,391 - INFO - evaluating now!
2024-03-01 03:12:18,333 - INFO - Epoch [48/300] (32781) train_loss: 28.0310, val_loss: 28.2278, lr: 0.000942, 157.05s
2024-03-01 03:14:43,616 - INFO - epoch complete!
2024-03-01 03:14:43,616 - INFO - evaluating now!
2024-03-01 03:14:55,514 - INFO - Epoch [49/300] (33450) train_loss: 28.0951, val_loss: 27.2280, lr: 0.000940, 157.18s
2024-03-01 03:14:55,564 - INFO - Saved model at 49
2024-03-01 03:14:55,564 - INFO - Val loss decrease from 27.7788 to 27.2280, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch49.tar
2024-03-01 03:17:19,384 - INFO - epoch complete!
2024-03-01 03:17:19,385 - INFO - evaluating now!
2024-03-01 03:17:30,853 - INFO - Epoch [50/300] (34119) train_loss: 27.8492, val_loss: 27.7275, lr: 0.000937, 155.29s
2024-03-01 03:19:55,216 - INFO - epoch complete!
2024-03-01 03:19:55,217 - INFO - evaluating now!
2024-03-01 03:20:06,862 - INFO - Epoch [51/300] (34788) train_loss: 27.8245, val_loss: 28.1717, lr: 0.000935, 156.01s
2024-03-01 03:22:31,900 - INFO - epoch complete!
2024-03-01 03:22:31,901 - INFO - evaluating now!
2024-03-01 03:22:43,810 - INFO - Epoch [52/300] (35457) train_loss: 27.6464, val_loss: 27.4762, lr: 0.000932, 156.95s
2024-03-01 03:25:11,467 - INFO - epoch complete!
2024-03-01 03:25:11,467 - INFO - evaluating now!
2024-03-01 03:25:23,303 - INFO - Epoch [53/300] (36126) train_loss: 27.6245, val_loss: 27.3196, lr: 0.000930, 159.49s
2024-03-01 03:27:48,279 - INFO - epoch complete!
2024-03-01 03:27:48,280 - INFO - evaluating now!
2024-03-01 03:27:59,753 - INFO - Epoch [54/300] (36795) train_loss: 27.6372, val_loss: 27.7068, lr: 0.000927, 156.45s
2024-03-01 03:30:24,867 - INFO - epoch complete!
2024-03-01 03:30:24,868 - INFO - evaluating now!
2024-03-01 03:30:36,181 - INFO - Epoch [55/300] (37464) train_loss: 27.5375, val_loss: 28.2929, lr: 0.000925, 156.43s
2024-03-01 03:33:00,088 - INFO - epoch complete!
2024-03-01 03:33:00,089 - INFO - evaluating now!
2024-03-01 03:33:11,842 - INFO - Epoch [56/300] (38133) train_loss: 27.5065, val_loss: 28.2988, lr: 0.000922, 155.66s
2024-03-01 03:35:36,244 - INFO - epoch complete!
2024-03-01 03:35:36,244 - INFO - evaluating now!
2024-03-01 03:35:47,665 - INFO - Epoch [57/300] (38802) train_loss: 27.4182, val_loss: 27.1429, lr: 0.000920, 155.82s
2024-03-01 03:35:47,715 - INFO - Saved model at 57
2024-03-01 03:35:47,715 - INFO - Val loss decrease from 27.2280 to 27.1429, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch57.tar
2024-03-01 03:38:11,656 - INFO - epoch complete!
2024-03-01 03:38:11,657 - INFO - evaluating now!
2024-03-01 03:38:23,550 - INFO - Epoch [58/300] (39471) train_loss: 27.3665, val_loss: 27.0594, lr: 0.000917, 155.83s
2024-03-01 03:38:23,602 - INFO - Saved model at 58
2024-03-01 03:38:23,602 - INFO - Val loss decrease from 27.1429 to 27.0594, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch58.tar
2024-03-01 03:40:48,509 - INFO - epoch complete!
2024-03-01 03:40:48,510 - INFO - evaluating now!
2024-03-01 03:41:00,493 - INFO - Epoch [59/300] (40140) train_loss: 27.4351, val_loss: 27.0890, lr: 0.000914, 156.89s
2024-03-01 03:43:25,704 - INFO - epoch complete!
2024-03-01 03:43:25,705 - INFO - evaluating now!
2024-03-01 03:43:37,641 - INFO - Epoch [60/300] (40809) train_loss: 27.3362, val_loss: 26.8427, lr: 0.000911, 157.15s
2024-03-01 03:43:37,692 - INFO - Saved model at 60
2024-03-01 03:43:37,692 - INFO - Val loss decrease from 27.0594 to 26.8427, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch60.tar
2024-03-01 03:46:02,380 - INFO - epoch complete!
2024-03-01 03:46:02,380 - INFO - evaluating now!
2024-03-01 03:46:14,176 - INFO - Epoch [61/300] (41478) train_loss: 27.2395, val_loss: 27.0837, lr: 0.000908, 156.48s
2024-03-01 03:48:38,994 - INFO - epoch complete!
2024-03-01 03:48:38,995 - INFO - evaluating now!
2024-03-01 03:48:50,916 - INFO - Epoch [62/300] (42147) train_loss: 27.2239, val_loss: 27.3745, lr: 0.000906, 156.74s
2024-03-01 03:51:15,512 - INFO - epoch complete!
2024-03-01 03:51:15,513 - INFO - evaluating now!
2024-03-01 03:51:27,377 - INFO - Epoch [63/300] (42816) train_loss: 27.1492, val_loss: 28.1659, lr: 0.000903, 156.46s
2024-03-01 03:53:51,834 - INFO - epoch complete!
2024-03-01 03:53:51,835 - INFO - evaluating now!
2024-03-01 03:54:03,687 - INFO - Epoch [64/300] (43485) train_loss: 27.1574, val_loss: 27.1756, lr: 0.000900, 156.31s
2024-03-01 03:56:28,778 - INFO - epoch complete!
2024-03-01 03:56:28,779 - INFO - evaluating now!
2024-03-01 03:56:40,638 - INFO - Epoch [65/300] (44154) train_loss: 27.0324, val_loss: 26.7720, lr: 0.000897, 156.95s
2024-03-01 03:56:40,689 - INFO - Saved model at 65
2024-03-01 03:56:40,689 - INFO - Val loss decrease from 26.8427 to 26.7720, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch65.tar
2024-03-01 03:59:05,025 - INFO - epoch complete!
2024-03-01 03:59:05,025 - INFO - evaluating now!
2024-03-01 03:59:16,501 - INFO - Epoch [66/300] (44823) train_loss: 27.0810, val_loss: 27.9519, lr: 0.000894, 155.81s
2024-03-01 04:01:40,818 - INFO - epoch complete!
2024-03-01 04:01:40,818 - INFO - evaluating now!
2024-03-01 04:01:51,896 - INFO - Epoch [67/300] (45492) train_loss: 27.0851, val_loss: 26.7935, lr: 0.000891, 155.39s
2024-03-01 04:04:17,220 - INFO - epoch complete!
2024-03-01 04:04:17,221 - INFO - evaluating now!
2024-03-01 04:04:28,366 - INFO - Epoch [68/300] (46161) train_loss: 26.8909, val_loss: 28.1298, lr: 0.000888, 156.47s
2024-03-01 04:06:53,801 - INFO - epoch complete!
2024-03-01 04:06:53,801 - INFO - evaluating now!
2024-03-01 04:07:04,905 - INFO - Epoch [69/300] (46830) train_loss: 26.9946, val_loss: 27.3887, lr: 0.000884, 156.54s
2024-03-01 04:09:30,616 - INFO - epoch complete!
2024-03-01 04:09:30,616 - INFO - evaluating now!
2024-03-01 04:09:41,825 - INFO - Epoch [70/300] (47499) train_loss: 26.7948, val_loss: 27.7336, lr: 0.000881, 156.92s
2024-03-01 04:12:07,422 - INFO - epoch complete!
2024-03-01 04:12:07,423 - INFO - evaluating now!
2024-03-01 04:12:18,515 - INFO - Epoch [71/300] (48168) train_loss: 26.8866, val_loss: 27.3080, lr: 0.000878, 156.69s
2024-03-01 04:14:44,995 - INFO - epoch complete!
2024-03-01 04:14:44,996 - INFO - evaluating now!
2024-03-01 04:14:56,110 - INFO - Epoch [72/300] (48837) train_loss: 26.8736, val_loss: 26.7234, lr: 0.000875, 157.59s
2024-03-01 04:14:56,159 - INFO - Saved model at 72
2024-03-01 04:14:56,159 - INFO - Val loss decrease from 26.7720 to 26.7234, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch72.tar
2024-03-01 04:17:21,732 - INFO - epoch complete!
2024-03-01 04:17:21,733 - INFO - evaluating now!
2024-03-01 04:17:32,939 - INFO - Epoch [73/300] (49506) train_loss: 26.8400, val_loss: 26.7178, lr: 0.000872, 156.78s
2024-03-01 04:17:32,989 - INFO - Saved model at 73
2024-03-01 04:17:32,989 - INFO - Val loss decrease from 26.7234 to 26.7178, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch73.tar
2024-03-01 04:19:58,994 - INFO - epoch complete!
2024-03-01 04:19:58,994 - INFO - evaluating now!
2024-03-01 04:20:10,162 - INFO - Epoch [74/300] (50175) train_loss: 26.7109, val_loss: 26.9716, lr: 0.000868, 157.17s
2024-03-01 04:22:35,544 - INFO - epoch complete!
2024-03-01 04:22:35,544 - INFO - evaluating now!
2024-03-01 04:22:46,524 - INFO - Epoch [75/300] (50844) train_loss: 26.7186, val_loss: 26.8984, lr: 0.000865, 156.36s
2024-03-01 04:25:11,842 - INFO - epoch complete!
2024-03-01 04:25:11,842 - INFO - evaluating now!
2024-03-01 04:25:22,834 - INFO - Epoch [76/300] (51513) train_loss: 26.6611, val_loss: 27.1459, lr: 0.000861, 156.31s
2024-03-01 04:27:47,710 - INFO - epoch complete!
2024-03-01 04:27:47,711 - INFO - evaluating now!
2024-03-01 04:27:58,783 - INFO - Epoch [77/300] (52182) train_loss: 26.5784, val_loss: 26.5243, lr: 0.000858, 155.95s
2024-03-01 04:27:58,832 - INFO - Saved model at 77
2024-03-01 04:27:58,833 - INFO - Val loss decrease from 26.7178 to 26.5243, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch77.tar
2024-03-01 04:30:23,974 - INFO - epoch complete!
2024-03-01 04:30:23,975 - INFO - evaluating now!
2024-03-01 04:30:35,315 - INFO - Epoch [78/300] (52851) train_loss: 26.6079, val_loss: 27.1924, lr: 0.000855, 156.48s
2024-03-01 04:33:00,161 - INFO - epoch complete!
2024-03-01 04:33:00,162 - INFO - evaluating now!
2024-03-01 04:33:11,411 - INFO - Epoch [79/300] (53520) train_loss: 26.6001, val_loss: 27.0261, lr: 0.000851, 156.10s
2024-03-01 04:35:34,747 - INFO - epoch complete!
2024-03-01 04:35:34,747 - INFO - evaluating now!
2024-03-01 04:35:45,937 - INFO - Epoch [80/300] (54189) train_loss: 26.5054, val_loss: 26.0319, lr: 0.000848, 154.53s
2024-03-01 04:35:45,987 - INFO - Saved model at 80
2024-03-01 04:35:45,987 - INFO - Val loss decrease from 26.5243 to 26.0319, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch80.tar
2024-03-01 04:38:09,955 - INFO - epoch complete!
2024-03-01 04:38:09,956 - INFO - evaluating now!
2024-03-01 04:38:21,720 - INFO - Epoch [81/300] (54858) train_loss: 26.4373, val_loss: 26.7600, lr: 0.000844, 155.73s
2024-03-01 04:40:47,609 - INFO - epoch complete!
2024-03-01 04:40:47,609 - INFO - evaluating now!
2024-03-01 04:40:59,473 - INFO - Epoch [82/300] (55527) train_loss: 26.5437, val_loss: 27.2590, lr: 0.000840, 157.75s
2024-03-01 04:43:24,340 - INFO - epoch complete!
2024-03-01 04:43:24,341 - INFO - evaluating now!
2024-03-01 04:43:36,274 - INFO - Epoch [83/300] (56196) train_loss: 26.4272, val_loss: 26.8499, lr: 0.000837, 156.80s
2024-03-01 04:46:00,521 - INFO - epoch complete!
2024-03-01 04:46:00,522 - INFO - evaluating now!
2024-03-01 04:46:11,911 - INFO - Epoch [84/300] (56865) train_loss: 26.4555, val_loss: 26.7913, lr: 0.000833, 155.64s
2024-03-01 04:48:35,177 - INFO - epoch complete!
2024-03-01 04:48:35,178 - INFO - evaluating now!
2024-03-01 04:48:47,066 - INFO - Epoch [85/300] (57534) train_loss: 26.3987, val_loss: 26.9545, lr: 0.000830, 155.15s
2024-03-01 04:51:11,537 - INFO - epoch complete!
2024-03-01 04:51:11,538 - INFO - evaluating now!
2024-03-01 04:51:22,787 - INFO - Epoch [86/300] (58203) train_loss: 26.4842, val_loss: 27.2863, lr: 0.000826, 155.72s
2024-03-01 04:53:45,900 - INFO - epoch complete!
2024-03-01 04:53:45,901 - INFO - evaluating now!
2024-03-01 04:53:57,822 - INFO - Epoch [87/300] (58872) train_loss: 26.3796, val_loss: 26.4008, lr: 0.000822, 155.03s
2024-03-01 04:56:22,950 - INFO - epoch complete!
2024-03-01 04:56:22,950 - INFO - evaluating now!
2024-03-01 04:56:34,343 - INFO - Epoch [88/300] (59541) train_loss: 26.2135, val_loss: 26.5180, lr: 0.000818, 156.52s
2024-03-01 04:58:59,259 - INFO - epoch complete!
2024-03-01 04:58:59,259 - INFO - evaluating now!
2024-03-01 04:59:11,022 - INFO - Epoch [89/300] (60210) train_loss: 26.2760, val_loss: 26.1128, lr: 0.000815, 156.68s
2024-03-01 05:01:35,629 - INFO - epoch complete!
2024-03-01 05:01:35,629 - INFO - evaluating now!
2024-03-01 05:01:47,496 - INFO - Epoch [90/300] (60879) train_loss: 26.2976, val_loss: 26.6184, lr: 0.000811, 156.47s
2024-03-01 05:04:15,590 - INFO - epoch complete!
2024-03-01 05:04:15,591 - INFO - evaluating now!
2024-03-01 05:04:27,519 - INFO - Epoch [91/300] (61548) train_loss: 26.2047, val_loss: 26.5694, lr: 0.000807, 160.02s
2024-03-01 05:06:52,787 - INFO - epoch complete!
2024-03-01 05:06:52,788 - INFO - evaluating now!
2024-03-01 05:07:04,620 - INFO - Epoch [92/300] (62217) train_loss: 26.1641, val_loss: 26.6010, lr: 0.000803, 157.10s
2024-03-01 05:09:29,287 - INFO - epoch complete!
2024-03-01 05:09:29,288 - INFO - evaluating now!
2024-03-01 05:09:41,148 - INFO - Epoch [93/300] (62886) train_loss: 26.1294, val_loss: 26.1695, lr: 0.000799, 156.53s
2024-03-01 05:12:05,374 - INFO - epoch complete!
2024-03-01 05:12:05,375 - INFO - evaluating now!
2024-03-01 05:12:17,223 - INFO - Epoch [94/300] (63555) train_loss: 26.1611, val_loss: 26.7633, lr: 0.000795, 156.07s
2024-03-01 05:14:42,265 - INFO - epoch complete!
2024-03-01 05:14:42,265 - INFO - evaluating now!
2024-03-01 05:14:54,158 - INFO - Epoch [95/300] (64224) train_loss: 26.0744, val_loss: 26.3650, lr: 0.000791, 156.93s
2024-03-01 05:17:19,162 - INFO - epoch complete!
2024-03-01 05:17:19,163 - INFO - evaluating now!
2024-03-01 05:17:31,066 - INFO - Epoch [96/300] (64893) train_loss: 26.1194, val_loss: 26.3028, lr: 0.000787, 156.91s
2024-03-01 05:19:55,630 - INFO - epoch complete!
2024-03-01 05:19:55,631 - INFO - evaluating now!
2024-03-01 05:20:07,476 - INFO - Epoch [97/300] (65562) train_loss: 26.0659, val_loss: 26.3214, lr: 0.000783, 156.41s
2024-03-01 05:22:32,085 - INFO - epoch complete!
2024-03-01 05:22:32,086 - INFO - evaluating now!
2024-03-01 05:22:43,553 - INFO - Epoch [98/300] (66231) train_loss: 26.1175, val_loss: 26.6403, lr: 0.000779, 156.08s
2024-03-01 05:25:07,763 - INFO - epoch complete!
2024-03-01 05:25:07,763 - INFO - evaluating now!
2024-03-01 05:25:19,188 - INFO - Epoch [99/300] (66900) train_loss: 25.9723, val_loss: 26.1813, lr: 0.000775, 155.63s
2024-03-01 05:27:47,284 - INFO - epoch complete!
2024-03-01 05:27:47,285 - INFO - evaluating now!
2024-03-01 05:27:58,790 - INFO - Epoch [100/300] (67569) train_loss: 25.9235, val_loss: 26.1972, lr: 0.000771, 159.60s
2024-03-01 05:30:22,998 - INFO - epoch complete!
2024-03-01 05:30:22,999 - INFO - evaluating now!
2024-03-01 05:30:34,269 - INFO - Epoch [101/300] (68238) train_loss: 26.0682, val_loss: 26.2407, lr: 0.000767, 155.48s
2024-03-01 05:32:58,467 - INFO - epoch complete!
2024-03-01 05:32:58,467 - INFO - evaluating now!
2024-03-01 05:33:10,278 - INFO - Epoch [102/300] (68907) train_loss: 25.8656, val_loss: 26.0963, lr: 0.000763, 156.01s
2024-03-01 05:35:35,279 - INFO - epoch complete!
2024-03-01 05:35:35,280 - INFO - evaluating now!
2024-03-01 05:35:47,247 - INFO - Epoch [103/300] (69576) train_loss: 25.9857, val_loss: 26.7286, lr: 0.000758, 156.97s
2024-03-01 05:38:12,685 - INFO - epoch complete!
2024-03-01 05:38:12,686 - INFO - evaluating now!
2024-03-01 05:38:24,663 - INFO - Epoch [104/300] (70245) train_loss: 25.8505, val_loss: 26.2151, lr: 0.000754, 157.41s
2024-03-01 05:40:49,517 - INFO - epoch complete!
2024-03-01 05:40:49,518 - INFO - evaluating now!
2024-03-01 05:41:01,490 - INFO - Epoch [105/300] (70914) train_loss: 25.8150, val_loss: 27.5822, lr: 0.000750, 156.83s
2024-03-01 05:43:31,966 - INFO - epoch complete!
2024-03-01 05:43:31,966 - INFO - evaluating now!
2024-03-01 05:43:43,281 - INFO - Epoch [106/300] (71583) train_loss: 25.7356, val_loss: 26.3882, lr: 0.000746, 161.79s
2024-03-01 05:46:08,775 - INFO - epoch complete!
2024-03-01 05:46:08,776 - INFO - evaluating now!
2024-03-01 05:46:20,660 - INFO - Epoch [107/300] (72252) train_loss: 25.8003, val_loss: 26.6443, lr: 0.000742, 157.38s
2024-03-01 05:48:46,141 - INFO - epoch complete!
2024-03-01 05:48:46,142 - INFO - evaluating now!
2024-03-01 05:48:58,016 - INFO - Epoch [108/300] (72921) train_loss: 25.7532, val_loss: 26.3902, lr: 0.000737, 157.36s
2024-03-01 05:51:23,421 - INFO - epoch complete!
2024-03-01 05:51:23,421 - INFO - evaluating now!
2024-03-01 05:51:34,726 - INFO - Epoch [109/300] (73590) train_loss: 25.8127, val_loss: 26.0450, lr: 0.000733, 156.71s
2024-03-01 05:54:08,520 - INFO - epoch complete!
2024-03-01 05:54:08,521 - INFO - evaluating now!
2024-03-01 05:54:20,551 - INFO - Epoch [110/300] (74259) train_loss: 25.7005, val_loss: 26.5595, lr: 0.000729, 165.82s
2024-03-01 05:56:49,739 - INFO - epoch complete!
2024-03-01 05:56:49,739 - INFO - evaluating now!
2024-03-01 05:57:01,743 - INFO - Epoch [111/300] (74928) train_loss: 25.7324, val_loss: 26.1531, lr: 0.000724, 161.19s
2024-03-01 05:59:33,562 - INFO - epoch complete!
2024-03-01 05:59:33,562 - INFO - evaluating now!
2024-03-01 05:59:45,534 - INFO - Epoch [112/300] (75597) train_loss: 25.7312, val_loss: 26.1346, lr: 0.000720, 163.79s
2024-03-01 06:02:18,625 - INFO - epoch complete!
2024-03-01 06:02:18,625 - INFO - evaluating now!
2024-03-01 06:02:30,586 - INFO - Epoch [113/300] (76266) train_loss: 25.6259, val_loss: 25.9781, lr: 0.000716, 165.05s
2024-03-01 06:02:30,637 - INFO - Saved model at 113
2024-03-01 06:02:30,637 - INFO - Val loss decrease from 26.0319 to 25.9781, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch113.tar
2024-03-01 06:05:03,172 - INFO - epoch complete!
2024-03-01 06:05:03,173 - INFO - evaluating now!
2024-03-01 06:05:15,212 - INFO - Epoch [114/300] (76935) train_loss: 25.4981, val_loss: 26.0363, lr: 0.000711, 164.57s
2024-03-01 06:07:42,105 - INFO - epoch complete!
2024-03-01 06:07:42,106 - INFO - evaluating now!
2024-03-01 06:07:54,094 - INFO - Epoch [115/300] (77604) train_loss: 25.6025, val_loss: 25.8485, lr: 0.000707, 158.88s
2024-03-01 06:07:54,146 - INFO - Saved model at 115
2024-03-01 06:07:54,146 - INFO - Val loss decrease from 25.9781 to 25.8485, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch115.tar
2024-03-01 06:10:20,712 - INFO - epoch complete!
2024-03-01 06:10:20,712 - INFO - evaluating now!
2024-03-01 06:10:32,649 - INFO - Epoch [116/300] (78273) train_loss: 25.6302, val_loss: 26.1021, lr: 0.000702, 158.50s
2024-03-01 06:13:04,748 - INFO - epoch complete!
2024-03-01 06:13:04,749 - INFO - evaluating now!
2024-03-01 06:13:16,082 - INFO - Epoch [117/300] (78942) train_loss: 25.5754, val_loss: 26.5154, lr: 0.000698, 163.43s
2024-03-01 06:15:49,809 - INFO - epoch complete!
2024-03-01 06:15:49,810 - INFO - evaluating now!
2024-03-01 06:16:01,369 - INFO - Epoch [118/300] (79611) train_loss: 25.5623, val_loss: 26.6589, lr: 0.000694, 165.29s
2024-03-01 06:18:32,648 - INFO - epoch complete!
2024-03-01 06:18:32,649 - INFO - evaluating now!
2024-03-01 06:18:44,631 - INFO - Epoch [119/300] (80280) train_loss: 25.5558, val_loss: 25.9413, lr: 0.000689, 163.26s
2024-03-01 06:21:12,394 - INFO - epoch complete!
2024-03-01 06:21:12,394 - INFO - evaluating now!
2024-03-01 06:21:24,453 - INFO - Epoch [120/300] (80949) train_loss: 25.5282, val_loss: 26.1799, lr: 0.000685, 159.82s
2024-03-01 06:23:50,984 - INFO - epoch complete!
2024-03-01 06:23:50,985 - INFO - evaluating now!
2024-03-01 06:24:03,043 - INFO - Epoch [121/300] (81618) train_loss: 25.5301, val_loss: 25.8698, lr: 0.000680, 158.59s
2024-03-01 06:26:30,551 - INFO - epoch complete!
2024-03-01 06:26:30,552 - INFO - evaluating now!
2024-03-01 06:26:42,613 - INFO - Epoch [122/300] (82287) train_loss: 25.4878, val_loss: 25.9464, lr: 0.000676, 159.57s
2024-03-01 06:29:09,813 - INFO - epoch complete!
2024-03-01 06:29:09,814 - INFO - evaluating now!
2024-03-01 06:29:21,846 - INFO - Epoch [123/300] (82956) train_loss: 25.5006, val_loss: 26.0975, lr: 0.000671, 159.23s
2024-03-01 06:31:48,518 - INFO - epoch complete!
2024-03-01 06:31:48,518 - INFO - evaluating now!
2024-03-01 06:32:00,565 - INFO - Epoch [124/300] (83625) train_loss: 25.4493, val_loss: 26.0502, lr: 0.000666, 158.72s
2024-03-01 06:34:27,211 - INFO - epoch complete!
2024-03-01 06:34:27,212 - INFO - evaluating now!
2024-03-01 06:34:39,328 - INFO - Epoch [125/300] (84294) train_loss: 25.3376, val_loss: 25.7204, lr: 0.000662, 158.76s
2024-03-01 06:34:39,380 - INFO - Saved model at 125
2024-03-01 06:34:39,380 - INFO - Val loss decrease from 25.8485 to 25.7204, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch125.tar
2024-03-01 06:37:04,714 - INFO - epoch complete!
2024-03-01 06:37:04,715 - INFO - evaluating now!
2024-03-01 06:37:16,303 - INFO - Epoch [126/300] (84963) train_loss: 25.2842, val_loss: 25.7319, lr: 0.000657, 156.92s
2024-03-01 06:39:41,651 - INFO - epoch complete!
2024-03-01 06:39:41,652 - INFO - evaluating now!
2024-03-01 06:39:53,164 - INFO - Epoch [127/300] (85632) train_loss: 25.3820, val_loss: 25.9818, lr: 0.000653, 156.86s
2024-03-01 06:42:19,841 - INFO - epoch complete!
2024-03-01 06:42:19,842 - INFO - evaluating now!
2024-03-01 06:42:31,449 - INFO - Epoch [128/300] (86301) train_loss: 25.2791, val_loss: 25.6033, lr: 0.000648, 158.28s
2024-03-01 06:42:31,499 - INFO - Saved model at 128
2024-03-01 06:42:31,499 - INFO - Val loss decrease from 25.7204 to 25.6033, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch128.tar
2024-03-01 06:45:02,953 - INFO - epoch complete!
2024-03-01 06:45:02,953 - INFO - evaluating now!
2024-03-01 06:45:14,674 - INFO - Epoch [129/300] (86970) train_loss: 25.3290, val_loss: 26.3225, lr: 0.000644, 163.17s
2024-03-01 06:47:39,768 - INFO - epoch complete!
2024-03-01 06:47:39,768 - INFO - evaluating now!
2024-03-01 06:47:51,324 - INFO - Epoch [130/300] (87639) train_loss: 25.3465, val_loss: 26.3748, lr: 0.000639, 156.65s
2024-03-01 06:50:18,048 - INFO - epoch complete!
2024-03-01 06:50:18,048 - INFO - evaluating now!
2024-03-01 06:50:29,619 - INFO - Epoch [131/300] (88308) train_loss: 25.3177, val_loss: 26.3782, lr: 0.000634, 158.29s
2024-03-01 06:53:01,602 - INFO - epoch complete!
2024-03-01 06:53:01,603 - INFO - evaluating now!
2024-03-01 06:53:13,587 - INFO - Epoch [132/300] (88977) train_loss: 25.2248, val_loss: 26.0443, lr: 0.000630, 163.97s
2024-03-01 06:55:39,202 - INFO - epoch complete!
2024-03-01 06:55:39,202 - INFO - evaluating now!
2024-03-01 06:55:51,172 - INFO - Epoch [133/300] (89646) train_loss: 25.2843, val_loss: 25.9233, lr: 0.000625, 157.58s
2024-03-01 06:58:15,826 - INFO - epoch complete!
2024-03-01 06:58:15,826 - INFO - evaluating now!
2024-03-01 06:58:27,189 - INFO - Epoch [134/300] (90315) train_loss: 25.2205, val_loss: 25.5236, lr: 0.000620, 156.02s
2024-03-01 06:58:27,241 - INFO - Saved model at 134
2024-03-01 06:58:27,242 - INFO - Val loss decrease from 25.6033 to 25.5236, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch134.tar
2024-03-01 07:00:52,045 - INFO - epoch complete!
2024-03-01 07:00:52,045 - INFO - evaluating now!
2024-03-01 07:01:04,036 - INFO - Epoch [135/300] (90984) train_loss: 25.1058, val_loss: 25.7958, lr: 0.000616, 156.79s
2024-03-01 07:03:29,551 - INFO - epoch complete!
2024-03-01 07:03:29,551 - INFO - evaluating now!
2024-03-01 07:03:41,443 - INFO - Epoch [136/300] (91653) train_loss: 25.2554, val_loss: 25.8774, lr: 0.000611, 157.41s
2024-03-01 07:06:06,936 - INFO - epoch complete!
2024-03-01 07:06:06,937 - INFO - evaluating now!
2024-03-01 07:06:18,247 - INFO - Epoch [137/300] (92322) train_loss: 25.1277, val_loss: 25.6586, lr: 0.000606, 156.80s
2024-03-01 07:08:42,586 - INFO - epoch complete!
2024-03-01 07:08:42,587 - INFO - evaluating now!
2024-03-01 07:08:54,264 - INFO - Epoch [138/300] (92991) train_loss: 25.1713, val_loss: 25.5626, lr: 0.000602, 156.02s
2024-03-01 07:11:18,082 - INFO - epoch complete!
2024-03-01 07:11:18,083 - INFO - evaluating now!
2024-03-01 07:11:29,956 - INFO - Epoch [139/300] (93660) train_loss: 25.1465, val_loss: 25.6572, lr: 0.000597, 155.69s
2024-03-01 07:13:54,978 - INFO - epoch complete!
2024-03-01 07:13:54,979 - INFO - evaluating now!
2024-03-01 07:14:06,841 - INFO - Epoch [140/300] (94329) train_loss: 24.9873, val_loss: 25.3995, lr: 0.000592, 156.88s
2024-03-01 07:14:06,893 - INFO - Saved model at 140
2024-03-01 07:14:06,893 - INFO - Val loss decrease from 25.5236 to 25.3995, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch140.tar
2024-03-01 07:16:32,105 - INFO - epoch complete!
2024-03-01 07:16:32,106 - INFO - evaluating now!
2024-03-01 07:16:44,000 - INFO - Epoch [141/300] (94998) train_loss: 25.0597, val_loss: 25.8209, lr: 0.000588, 157.11s
2024-03-01 07:19:08,514 - INFO - epoch complete!
2024-03-01 07:19:08,514 - INFO - evaluating now!
2024-03-01 07:19:20,158 - INFO - Epoch [142/300] (95667) train_loss: 25.0987, val_loss: 25.7304, lr: 0.000583, 156.16s
2024-03-01 07:21:44,802 - INFO - epoch complete!
2024-03-01 07:21:44,803 - INFO - evaluating now!
2024-03-01 07:21:56,537 - INFO - Epoch [143/300] (96336) train_loss: 24.9355, val_loss: 25.6923, lr: 0.000578, 156.38s
2024-03-01 07:24:20,993 - INFO - epoch complete!
2024-03-01 07:24:20,993 - INFO - evaluating now!
2024-03-01 07:24:32,223 - INFO - Epoch [144/300] (97005) train_loss: 25.0639, val_loss: 25.7040, lr: 0.000574, 155.69s
2024-03-01 07:26:57,780 - INFO - epoch complete!
2024-03-01 07:26:57,781 - INFO - evaluating now!
2024-03-01 07:27:09,004 - INFO - Epoch [145/300] (97674) train_loss: 24.9130, val_loss: 25.6178, lr: 0.000569, 156.78s
2024-03-01 07:29:34,703 - INFO - epoch complete!
2024-03-01 07:29:34,704 - INFO - evaluating now!
2024-03-01 07:29:46,055 - INFO - Epoch [146/300] (98343) train_loss: 24.9507, val_loss: 25.6026, lr: 0.000564, 157.05s
2024-03-01 07:32:12,493 - INFO - epoch complete!
2024-03-01 07:32:12,493 - INFO - evaluating now!
2024-03-01 07:32:23,985 - INFO - Epoch [147/300] (99012) train_loss: 24.9555, val_loss: 25.5044, lr: 0.000559, 157.93s
2024-03-01 07:34:48,868 - INFO - epoch complete!
2024-03-01 07:34:48,868 - INFO - evaluating now!
2024-03-01 07:34:59,854 - INFO - Epoch [148/300] (99681) train_loss: 24.9192, val_loss: 26.1912, lr: 0.000555, 155.87s
2024-03-01 07:37:25,338 - INFO - epoch complete!
2024-03-01 07:37:25,339 - INFO - evaluating now!
2024-03-01 07:37:36,598 - INFO - Epoch [149/300] (100350) train_loss: 24.8619, val_loss: 25.4464, lr: 0.000550, 156.74s
2024-03-01 07:40:01,781 - INFO - epoch complete!
2024-03-01 07:40:01,782 - INFO - evaluating now!
2024-03-01 07:40:12,987 - INFO - Epoch [150/300] (101019) train_loss: 24.8914, val_loss: 25.8208, lr: 0.000545, 156.39s
2024-03-01 07:42:39,092 - INFO - epoch complete!
2024-03-01 07:42:39,092 - INFO - evaluating now!
2024-03-01 07:42:50,249 - INFO - Epoch [151/300] (101688) train_loss: 24.8573, val_loss: 25.9005, lr: 0.000541, 157.26s
2024-03-01 07:45:15,849 - INFO - epoch complete!
2024-03-01 07:45:15,850 - INFO - evaluating now!
2024-03-01 07:45:26,887 - INFO - Epoch [152/300] (102357) train_loss: 24.8215, val_loss: 25.5793, lr: 0.000536, 156.64s
2024-03-01 07:47:54,568 - INFO - epoch complete!
2024-03-01 07:47:54,568 - INFO - evaluating now!
2024-03-01 07:48:05,592 - INFO - Epoch [153/300] (103026) train_loss: 24.7823, val_loss: 25.3951, lr: 0.000531, 158.70s
2024-03-01 07:48:05,648 - INFO - Saved model at 153
2024-03-01 07:48:05,648 - INFO - Val loss decrease from 25.3995 to 25.3951, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch153.tar
2024-03-01 07:50:31,814 - INFO - epoch complete!
2024-03-01 07:50:31,814 - INFO - evaluating now!
2024-03-01 07:50:42,885 - INFO - Epoch [154/300] (103695) train_loss: 24.8223, val_loss: 25.4013, lr: 0.000526, 157.24s
2024-03-01 07:53:09,696 - INFO - epoch complete!
2024-03-01 07:53:09,697 - INFO - evaluating now!
2024-03-01 07:53:20,923 - INFO - Epoch [155/300] (104364) train_loss: 24.7991, val_loss: 26.1254, lr: 0.000522, 158.04s
2024-03-01 07:55:46,801 - INFO - epoch complete!
2024-03-01 07:55:46,801 - INFO - evaluating now!
2024-03-01 07:55:58,099 - INFO - Epoch [156/300] (105033) train_loss: 24.8244, val_loss: 25.9253, lr: 0.000517, 157.17s
2024-03-01 07:58:24,272 - INFO - epoch complete!
2024-03-01 07:58:24,272 - INFO - evaluating now!
2024-03-01 07:58:35,750 - INFO - Epoch [157/300] (105702) train_loss: 24.7289, val_loss: 25.4118, lr: 0.000512, 157.65s
2024-03-01 08:01:01,532 - INFO - epoch complete!
2024-03-01 08:01:01,532 - INFO - evaluating now!
2024-03-01 08:01:13,191 - INFO - Epoch [158/300] (106371) train_loss: 24.7131, val_loss: 25.5517, lr: 0.000508, 157.44s
2024-03-01 08:03:40,865 - INFO - epoch complete!
2024-03-01 08:03:40,866 - INFO - evaluating now!
2024-03-01 08:03:52,742 - INFO - Epoch [159/300] (107040) train_loss: 24.7076, val_loss: 25.4304, lr: 0.000503, 159.55s
2024-03-01 08:06:20,138 - INFO - epoch complete!
2024-03-01 08:06:20,139 - INFO - evaluating now!
2024-03-01 08:06:32,115 - INFO - Epoch [160/300] (107709) train_loss: 24.6747, val_loss: 26.1195, lr: 0.000498, 159.37s
2024-03-01 08:08:56,885 - INFO - epoch complete!
2024-03-01 08:08:56,885 - INFO - evaluating now!
2024-03-01 08:09:08,915 - INFO - Epoch [161/300] (108378) train_loss: 24.6448, val_loss: 25.6588, lr: 0.000494, 156.80s
2024-03-01 08:11:34,522 - INFO - epoch complete!
2024-03-01 08:11:34,523 - INFO - evaluating now!
2024-03-01 08:11:46,498 - INFO - Epoch [162/300] (109047) train_loss: 24.7198, val_loss: 25.2848, lr: 0.000489, 157.58s
2024-03-01 08:11:46,548 - INFO - Saved model at 162
2024-03-01 08:11:46,548 - INFO - Val loss decrease from 25.3951 to 25.2848, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch162.tar
2024-03-01 08:14:11,939 - INFO - epoch complete!
2024-03-01 08:14:11,939 - INFO - evaluating now!
2024-03-01 08:14:23,878 - INFO - Epoch [163/300] (109716) train_loss: 24.6354, val_loss: 25.2536, lr: 0.000484, 157.33s
2024-03-01 08:14:23,931 - INFO - Saved model at 163
2024-03-01 08:14:23,931 - INFO - Val loss decrease from 25.2848 to 25.2536, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch163.tar
2024-03-01 08:16:57,084 - INFO - epoch complete!
2024-03-01 08:16:57,084 - INFO - evaluating now!
2024-03-01 08:17:09,113 - INFO - Epoch [164/300] (110385) train_loss: 24.5863, val_loss: 25.2727, lr: 0.000480, 165.18s
2024-03-01 08:19:37,501 - INFO - epoch complete!
2024-03-01 08:19:37,502 - INFO - evaluating now!
2024-03-01 08:19:49,452 - INFO - Epoch [165/300] (111054) train_loss: 24.6603, val_loss: 25.3628, lr: 0.000475, 160.34s
2024-03-01 08:22:15,560 - INFO - epoch complete!
2024-03-01 08:22:15,561 - INFO - evaluating now!
2024-03-01 08:22:27,485 - INFO - Epoch [166/300] (111723) train_loss: 24.5660, val_loss: 25.5607, lr: 0.000470, 158.03s
2024-03-01 08:24:53,451 - INFO - epoch complete!
2024-03-01 08:24:53,452 - INFO - evaluating now!
2024-03-01 08:25:05,521 - INFO - Epoch [167/300] (112392) train_loss: 24.5513, val_loss: 25.4600, lr: 0.000466, 158.04s
2024-03-01 08:27:32,046 - INFO - epoch complete!
2024-03-01 08:27:32,046 - INFO - evaluating now!
2024-03-01 08:27:44,034 - INFO - Epoch [168/300] (113061) train_loss: 24.5368, val_loss: 25.6213, lr: 0.000461, 158.51s
2024-03-01 08:30:14,350 - INFO - epoch complete!
2024-03-01 08:30:14,351 - INFO - evaluating now!
2024-03-01 08:30:26,336 - INFO - Epoch [169/300] (113730) train_loss: 24.4984, val_loss: 25.3941, lr: 0.000456, 162.30s
2024-03-01 08:33:03,364 - INFO - epoch complete!
2024-03-01 08:33:03,365 - INFO - evaluating now!
2024-03-01 08:33:15,431 - INFO - Epoch [170/300] (114399) train_loss: 24.4950, val_loss: 25.0972, lr: 0.000452, 169.09s
2024-03-01 08:33:15,482 - INFO - Saved model at 170
2024-03-01 08:33:15,482 - INFO - Val loss decrease from 25.2536 to 25.0972, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch170.tar
2024-03-01 08:35:47,216 - INFO - epoch complete!
2024-03-01 08:35:47,216 - INFO - evaluating now!
2024-03-01 08:35:59,140 - INFO - Epoch [171/300] (115068) train_loss: 24.5079, val_loss: 25.4355, lr: 0.000447, 163.66s
2024-03-01 08:38:27,666 - INFO - epoch complete!
2024-03-01 08:38:27,667 - INFO - evaluating now!
2024-03-01 08:38:39,571 - INFO - Epoch [172/300] (115737) train_loss: 24.4881, val_loss: 25.3230, lr: 0.000443, 160.43s
2024-03-01 08:41:14,605 - INFO - epoch complete!
2024-03-01 08:41:14,605 - INFO - evaluating now!
2024-03-01 08:41:26,597 - INFO - Epoch [173/300] (116406) train_loss: 24.5048, val_loss: 25.3934, lr: 0.000438, 167.02s
2024-03-01 08:43:52,561 - INFO - epoch complete!
2024-03-01 08:43:52,562 - INFO - evaluating now!
2024-03-01 08:44:04,572 - INFO - Epoch [174/300] (117075) train_loss: 24.4084, val_loss: 25.6678, lr: 0.000434, 157.97s
2024-03-01 08:46:34,071 - INFO - epoch complete!
2024-03-01 08:46:34,072 - INFO - evaluating now!
2024-03-01 08:46:46,041 - INFO - Epoch [175/300] (117744) train_loss: 24.3917, val_loss: 25.7130, lr: 0.000429, 161.47s
2024-03-01 08:49:15,412 - INFO - epoch complete!
2024-03-01 08:49:15,413 - INFO - evaluating now!
2024-03-01 08:49:27,358 - INFO - Epoch [176/300] (118413) train_loss: 24.4344, val_loss: 25.1987, lr: 0.000424, 161.32s
2024-03-01 08:51:53,583 - INFO - epoch complete!
2024-03-01 08:51:53,584 - INFO - evaluating now!
2024-03-01 08:52:04,939 - INFO - Epoch [177/300] (119082) train_loss: 24.3926, val_loss: 25.3585, lr: 0.000420, 157.58s
2024-03-01 08:54:39,483 - INFO - epoch complete!
2024-03-01 08:54:39,484 - INFO - evaluating now!
2024-03-01 08:54:51,508 - INFO - Epoch [178/300] (119751) train_loss: 24.4050, val_loss: 25.5757, lr: 0.000415, 166.57s
2024-03-01 08:57:17,187 - INFO - epoch complete!
2024-03-01 08:57:17,188 - INFO - evaluating now!
2024-03-01 08:57:29,157 - INFO - Epoch [179/300] (120420) train_loss: 24.3904, val_loss: 25.4206, lr: 0.000411, 157.65s
2024-03-01 08:59:54,510 - INFO - epoch complete!
2024-03-01 08:59:54,511 - INFO - evaluating now!
2024-03-01 09:00:06,261 - INFO - Epoch [180/300] (121089) train_loss: 24.3547, val_loss: 25.4087, lr: 0.000406, 157.10s
2024-03-01 09:02:35,963 - INFO - epoch complete!
2024-03-01 09:02:35,963 - INFO - evaluating now!
2024-03-01 09:02:47,887 - INFO - Epoch [181/300] (121758) train_loss: 24.2865, val_loss: 25.2660, lr: 0.000402, 161.63s
2024-03-01 09:05:13,945 - INFO - epoch complete!
2024-03-01 09:05:13,945 - INFO - evaluating now!
2024-03-01 09:05:25,949 - INFO - Epoch [182/300] (122427) train_loss: 24.2843, val_loss: 25.6201, lr: 0.000398, 158.06s
2024-03-01 09:07:51,951 - INFO - epoch complete!
2024-03-01 09:07:51,951 - INFO - evaluating now!
2024-03-01 09:08:03,367 - INFO - Epoch [183/300] (123096) train_loss: 24.2707, val_loss: 25.5513, lr: 0.000393, 157.42s
2024-03-01 09:10:31,887 - INFO - epoch complete!
2024-03-01 09:10:31,887 - INFO - evaluating now!
2024-03-01 09:10:43,658 - INFO - Epoch [184/300] (123765) train_loss: 24.2074, val_loss: 25.2310, lr: 0.000389, 160.29s
2024-03-01 09:13:11,106 - INFO - epoch complete!
2024-03-01 09:13:11,106 - INFO - evaluating now!
2024-03-01 09:13:23,102 - INFO - Epoch [185/300] (124434) train_loss: 24.2512, val_loss: 25.4037, lr: 0.000384, 159.44s
2024-03-01 09:15:54,718 - INFO - epoch complete!
2024-03-01 09:15:54,719 - INFO - evaluating now!
2024-03-01 09:16:06,198 - INFO - Epoch [186/300] (125103) train_loss: 24.2085, val_loss: 25.4740, lr: 0.000380, 163.10s
2024-03-01 09:18:32,403 - INFO - epoch complete!
2024-03-01 09:18:32,404 - INFO - evaluating now!
2024-03-01 09:18:43,674 - INFO - Epoch [187/300] (125772) train_loss: 24.2290, val_loss: 25.3389, lr: 0.000376, 157.47s
2024-03-01 09:21:15,658 - INFO - epoch complete!
2024-03-01 09:21:15,659 - INFO - evaluating now!
2024-03-01 09:21:27,045 - INFO - Epoch [188/300] (126441) train_loss: 24.2241, val_loss: 25.4493, lr: 0.000371, 163.37s
2024-03-01 09:23:55,938 - INFO - epoch complete!
2024-03-01 09:23:55,939 - INFO - evaluating now!
2024-03-01 09:24:08,027 - INFO - Epoch [189/300] (127110) train_loss: 24.1624, val_loss: 25.1417, lr: 0.000367, 160.98s
2024-03-01 09:26:37,913 - INFO - epoch complete!
2024-03-01 09:26:37,914 - INFO - evaluating now!
2024-03-01 09:26:49,985 - INFO - Epoch [190/300] (127779) train_loss: 24.1659, val_loss: 25.1715, lr: 0.000363, 161.96s
2024-03-01 09:29:24,437 - INFO - epoch complete!
2024-03-01 09:29:24,438 - INFO - evaluating now!
2024-03-01 09:29:36,421 - INFO - Epoch [191/300] (128448) train_loss: 24.1305, val_loss: 25.3316, lr: 0.000358, 166.44s
2024-03-01 09:32:03,482 - INFO - epoch complete!
2024-03-01 09:32:03,482 - INFO - evaluating now!
2024-03-01 09:32:15,567 - INFO - Epoch [192/300] (129117) train_loss: 24.1426, val_loss: 25.0918, lr: 0.000354, 159.14s
2024-03-01 09:32:15,619 - INFO - Saved model at 192
2024-03-01 09:32:15,619 - INFO - Val loss decrease from 25.0972 to 25.0918, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch192.tar
2024-03-01 09:34:46,245 - INFO - epoch complete!
2024-03-01 09:34:46,246 - INFO - evaluating now!
2024-03-01 09:34:58,244 - INFO - Epoch [193/300] (129786) train_loss: 24.1048, val_loss: 25.3581, lr: 0.000350, 162.62s
2024-03-01 09:37:26,294 - INFO - epoch complete!
2024-03-01 09:37:26,295 - INFO - evaluating now!
2024-03-01 09:37:38,261 - INFO - Epoch [194/300] (130455) train_loss: 24.0940, val_loss: 25.2481, lr: 0.000346, 160.02s
2024-03-01 09:40:04,871 - INFO - epoch complete!
2024-03-01 09:40:04,872 - INFO - evaluating now!
2024-03-01 09:40:16,264 - INFO - Epoch [195/300] (131124) train_loss: 24.1112, val_loss: 25.0616, lr: 0.000342, 158.00s
2024-03-01 09:40:16,315 - INFO - Saved model at 195
2024-03-01 09:40:16,315 - INFO - Val loss decrease from 25.0918 to 25.0616, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch195.tar
2024-03-01 09:42:44,324 - INFO - epoch complete!
2024-03-01 09:42:44,325 - INFO - evaluating now!
2024-03-01 09:42:56,331 - INFO - Epoch [196/300] (131793) train_loss: 24.0685, val_loss: 25.4033, lr: 0.000337, 160.02s
2024-03-01 09:45:25,344 - INFO - epoch complete!
2024-03-01 09:45:25,344 - INFO - evaluating now!
2024-03-01 09:45:37,361 - INFO - Epoch [197/300] (132462) train_loss: 24.0850, val_loss: 25.2721, lr: 0.000333, 161.03s
2024-03-01 09:48:06,799 - INFO - epoch complete!
2024-03-01 09:48:06,800 - INFO - evaluating now!
2024-03-01 09:48:18,769 - INFO - Epoch [198/300] (133131) train_loss: 24.0290, val_loss: 25.2713, lr: 0.000329, 161.41s
2024-03-01 09:50:56,032 - INFO - epoch complete!
2024-03-01 09:50:56,032 - INFO - evaluating now!
2024-03-01 09:51:08,243 - INFO - Epoch [199/300] (133800) train_loss: 24.0102, val_loss: 25.2847, lr: 0.000325, 169.47s
2024-03-01 09:53:34,363 - INFO - epoch complete!
2024-03-01 09:53:34,363 - INFO - evaluating now!
2024-03-01 09:53:46,365 - INFO - Epoch [200/300] (134469) train_loss: 23.9765, val_loss: 25.2642, lr: 0.000321, 158.12s
2024-03-01 09:56:12,544 - INFO - epoch complete!
2024-03-01 09:56:12,545 - INFO - evaluating now!
2024-03-01 09:56:24,515 - INFO - Epoch [201/300] (135138) train_loss: 23.9840, val_loss: 25.1789, lr: 0.000317, 158.15s
2024-03-01 09:58:50,730 - INFO - epoch complete!
2024-03-01 09:58:50,730 - INFO - evaluating now!
2024-03-01 09:59:02,810 - INFO - Epoch [202/300] (135807) train_loss: 23.9859, val_loss: 25.5651, lr: 0.000313, 158.29s
2024-03-01 10:01:27,787 - INFO - epoch complete!
2024-03-01 10:01:27,787 - INFO - evaluating now!
2024-03-01 10:01:39,144 - INFO - Epoch [203/300] (136476) train_loss: 23.9805, val_loss: 25.3395, lr: 0.000309, 156.33s
2024-03-01 10:04:07,886 - INFO - epoch complete!
2024-03-01 10:04:07,886 - INFO - evaluating now!
2024-03-01 10:04:19,274 - INFO - Epoch [204/300] (137145) train_loss: 23.9341, val_loss: 25.5144, lr: 0.000305, 160.13s
2024-03-01 10:06:46,728 - INFO - epoch complete!
2024-03-01 10:06:46,728 - INFO - evaluating now!
2024-03-01 10:06:58,692 - INFO - Epoch [205/300] (137814) train_loss: 23.9474, val_loss: 25.3466, lr: 0.000301, 159.42s
2024-03-01 10:09:28,145 - INFO - epoch complete!
2024-03-01 10:09:28,145 - INFO - evaluating now!
2024-03-01 10:09:40,164 - INFO - Epoch [206/300] (138483) train_loss: 23.9101, val_loss: 25.3333, lr: 0.000297, 161.47s
2024-03-01 10:12:12,253 - INFO - epoch complete!
2024-03-01 10:12:12,253 - INFO - evaluating now!
2024-03-01 10:12:24,253 - INFO - Epoch [207/300] (139152) train_loss: 23.9137, val_loss: 24.9632, lr: 0.000293, 164.09s
2024-03-01 10:12:24,304 - INFO - Saved model at 207
2024-03-01 10:12:24,304 - INFO - Val loss decrease from 25.0616 to 24.9632, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch207.tar
2024-03-01 10:14:51,876 - INFO - epoch complete!
2024-03-01 10:14:51,876 - INFO - evaluating now!
2024-03-01 10:15:03,733 - INFO - Epoch [208/300] (139821) train_loss: 23.8708, val_loss: 25.2262, lr: 0.000289, 159.43s
2024-03-01 10:17:37,657 - INFO - epoch complete!
2024-03-01 10:17:37,658 - INFO - evaluating now!
2024-03-01 10:17:49,517 - INFO - Epoch [209/300] (140490) train_loss: 23.8498, val_loss: 25.3372, lr: 0.000285, 165.78s
2024-03-01 10:20:14,466 - INFO - epoch complete!
2024-03-01 10:20:14,467 - INFO - evaluating now!
2024-03-01 10:20:25,770 - INFO - Epoch [210/300] (141159) train_loss: 23.8449, val_loss: 25.1338, lr: 0.000282, 156.25s
2024-03-01 10:22:50,461 - INFO - epoch complete!
2024-03-01 10:22:50,462 - INFO - evaluating now!
2024-03-01 10:23:02,346 - INFO - Epoch [211/300] (141828) train_loss: 23.8625, val_loss: 25.3823, lr: 0.000278, 156.58s
2024-03-01 10:25:27,005 - INFO - epoch complete!
2024-03-01 10:25:27,006 - INFO - evaluating now!
2024-03-01 10:25:38,950 - INFO - Epoch [212/300] (142497) train_loss: 23.8084, val_loss: 25.2676, lr: 0.000274, 156.60s
2024-03-01 10:28:03,692 - INFO - epoch complete!
2024-03-01 10:28:03,692 - INFO - evaluating now!
2024-03-01 10:28:15,578 - INFO - Epoch [213/300] (143166) train_loss: 23.8325, val_loss: 25.4390, lr: 0.000270, 156.63s
2024-03-01 10:30:40,220 - INFO - epoch complete!
2024-03-01 10:30:40,221 - INFO - evaluating now!
2024-03-01 10:30:52,117 - INFO - Epoch [214/300] (143835) train_loss: 23.7813, val_loss: 25.0028, lr: 0.000267, 156.54s
2024-03-01 10:33:17,298 - INFO - epoch complete!
2024-03-01 10:33:17,299 - INFO - evaluating now!
2024-03-01 10:33:29,229 - INFO - Epoch [215/300] (144504) train_loss: 23.7936, val_loss: 25.4271, lr: 0.000263, 157.11s
2024-03-01 10:35:54,133 - INFO - epoch complete!
2024-03-01 10:35:54,134 - INFO - evaluating now!
2024-03-01 10:36:05,924 - INFO - Epoch [216/300] (145173) train_loss: 23.8166, val_loss: 25.0673, lr: 0.000260, 156.69s
2024-03-01 10:38:36,625 - INFO - epoch complete!
2024-03-01 10:38:36,626 - INFO - evaluating now!
2024-03-01 10:38:48,590 - INFO - Epoch [217/300] (145842) train_loss: 23.7903, val_loss: 25.0334, lr: 0.000256, 162.67s
2024-03-01 10:40:36,440 - INFO - epoch complete!
2024-03-01 10:40:36,441 - INFO - evaluating now!
2024-03-01 10:40:42,668 - INFO - Epoch [218/300] (146511) train_loss: 23.7456, val_loss: 25.1169, lr: 0.000252, 114.08s
2024-03-01 10:42:13,957 - INFO - epoch complete!
2024-03-01 10:42:13,957 - INFO - evaluating now!
2024-03-01 10:42:20,183 - INFO - Epoch [219/300] (147180) train_loss: 23.7287, val_loss: 25.1273, lr: 0.000249, 97.51s
2024-03-01 10:43:51,391 - INFO - epoch complete!
2024-03-01 10:43:51,392 - INFO - evaluating now!
2024-03-01 10:43:57,610 - INFO - Epoch [220/300] (147849) train_loss: 23.7194, val_loss: 25.6408, lr: 0.000245, 97.43s
2024-03-01 10:45:28,928 - INFO - epoch complete!
2024-03-01 10:45:28,929 - INFO - evaluating now!
2024-03-01 10:45:35,152 - INFO - Epoch [221/300] (148518) train_loss: 23.7040, val_loss: 24.9901, lr: 0.000242, 97.54s
2024-03-01 10:47:06,276 - INFO - epoch complete!
2024-03-01 10:47:06,276 - INFO - evaluating now!
2024-03-01 10:47:12,534 - INFO - Epoch [222/300] (149187) train_loss: 23.6706, val_loss: 25.0775, lr: 0.000239, 97.38s
2024-03-01 10:48:43,974 - INFO - epoch complete!
2024-03-01 10:48:43,975 - INFO - evaluating now!
2024-03-01 10:48:50,213 - INFO - Epoch [223/300] (149856) train_loss: 23.6977, val_loss: 25.3308, lr: 0.000235, 97.68s
2024-03-01 10:50:21,605 - INFO - epoch complete!
2024-03-01 10:50:21,606 - INFO - evaluating now!
2024-03-01 10:50:27,836 - INFO - Epoch [224/300] (150525) train_loss: 23.6736, val_loss: 25.2403, lr: 0.000232, 97.62s
2024-03-01 10:51:59,447 - INFO - epoch complete!
2024-03-01 10:51:59,447 - INFO - evaluating now!
2024-03-01 10:52:05,759 - INFO - Epoch [225/300] (151194) train_loss: 23.6466, val_loss: 25.0645, lr: 0.000228, 97.92s
2024-03-01 10:53:44,886 - INFO - epoch complete!
2024-03-01 10:53:44,887 - INFO - evaluating now!
2024-03-01 10:53:51,110 - INFO - Epoch [226/300] (151863) train_loss: 23.6645, val_loss: 25.1930, lr: 0.000225, 105.35s
2024-03-01 10:55:21,991 - INFO - epoch complete!
2024-03-01 10:55:21,992 - INFO - evaluating now!
2024-03-01 10:55:28,216 - INFO - Epoch [227/300] (152532) train_loss: 23.6311, val_loss: 25.0439, lr: 0.000222, 97.11s
2024-03-01 10:56:59,894 - INFO - epoch complete!
2024-03-01 10:56:59,895 - INFO - evaluating now!
2024-03-01 10:57:06,129 - INFO - Epoch [228/300] (153201) train_loss: 23.6184, val_loss: 25.0395, lr: 0.000219, 97.91s
2024-03-01 10:58:37,676 - INFO - epoch complete!
2024-03-01 10:58:37,677 - INFO - evaluating now!
2024-03-01 10:58:43,879 - INFO - Epoch [229/300] (153870) train_loss: 23.5926, val_loss: 25.2316, lr: 0.000216, 97.75s
2024-03-01 11:00:15,010 - INFO - epoch complete!
2024-03-01 11:00:15,011 - INFO - evaluating now!
2024-03-01 11:00:21,212 - INFO - Epoch [230/300] (154539) train_loss: 23.6123, val_loss: 25.0379, lr: 0.000212, 97.33s
2024-03-01 11:01:52,576 - INFO - epoch complete!
2024-03-01 11:01:52,577 - INFO - evaluating now!
2024-03-01 11:01:58,771 - INFO - Epoch [231/300] (155208) train_loss: 23.5778, val_loss: 25.4006, lr: 0.000209, 97.56s
2024-03-01 11:03:29,380 - INFO - epoch complete!
2024-03-01 11:03:29,381 - INFO - evaluating now!
2024-03-01 11:03:35,573 - INFO - Epoch [232/300] (155877) train_loss: 23.5751, val_loss: 25.0676, lr: 0.000206, 96.80s
2024-03-01 11:05:04,886 - INFO - epoch complete!
2024-03-01 11:05:04,887 - INFO - evaluating now!
2024-03-01 11:05:11,112 - INFO - Epoch [233/300] (156546) train_loss: 23.5574, val_loss: 25.0550, lr: 0.000203, 95.54s
2024-03-01 11:06:40,299 - INFO - epoch complete!
2024-03-01 11:06:40,300 - INFO - evaluating now!
2024-03-01 11:06:46,504 - INFO - Epoch [234/300] (157215) train_loss: 23.5436, val_loss: 24.9855, lr: 0.000200, 95.39s
2024-03-01 11:08:15,956 - INFO - epoch complete!
2024-03-01 11:08:15,956 - INFO - evaluating now!
2024-03-01 11:08:22,152 - INFO - Epoch [235/300] (157884) train_loss: 23.5168, val_loss: 25.0724, lr: 0.000197, 95.65s
2024-03-01 11:09:51,169 - INFO - epoch complete!
2024-03-01 11:09:51,170 - INFO - evaluating now!
2024-03-01 11:09:57,375 - INFO - Epoch [236/300] (158553) train_loss: 23.5433, val_loss: 24.9760, lr: 0.000194, 95.22s
2024-03-01 11:11:26,700 - INFO - epoch complete!
2024-03-01 11:11:26,700 - INFO - evaluating now!
2024-03-01 11:11:32,908 - INFO - Epoch [237/300] (159222) train_loss: 23.5333, val_loss: 24.9541, lr: 0.000192, 95.53s
2024-03-01 11:11:32,959 - INFO - Saved model at 237
2024-03-01 11:11:32,959 - INFO - Val loss decrease from 24.9632 to 24.9541, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch237.tar
2024-03-01 11:13:02,083 - INFO - epoch complete!
2024-03-01 11:13:02,084 - INFO - evaluating now!
2024-03-01 11:13:08,343 - INFO - Epoch [238/300] (159891) train_loss: 23.4996, val_loss: 25.0627, lr: 0.000189, 95.38s
2024-03-01 11:14:37,812 - INFO - epoch complete!
2024-03-01 11:14:37,813 - INFO - evaluating now!
2024-03-01 11:14:44,009 - INFO - Epoch [239/300] (160560) train_loss: 23.4663, val_loss: 24.9718, lr: 0.000186, 95.67s
2024-03-01 11:16:13,280 - INFO - epoch complete!
2024-03-01 11:16:13,281 - INFO - evaluating now!
2024-03-01 11:16:19,484 - INFO - Epoch [240/300] (161229) train_loss: 23.4712, val_loss: 25.1445, lr: 0.000183, 95.47s
2024-03-01 11:17:48,559 - INFO - epoch complete!
2024-03-01 11:17:48,560 - INFO - evaluating now!
2024-03-01 11:17:54,763 - INFO - Epoch [241/300] (161898) train_loss: 23.4689, val_loss: 25.1537, lr: 0.000180, 95.28s
2024-03-01 11:19:23,809 - INFO - epoch complete!
2024-03-01 11:19:23,810 - INFO - evaluating now!
2024-03-01 11:19:30,025 - INFO - Epoch [242/300] (162567) train_loss: 23.4341, val_loss: 25.1379, lr: 0.000178, 95.26s
2024-03-01 11:20:59,695 - INFO - epoch complete!
2024-03-01 11:20:59,696 - INFO - evaluating now!
2024-03-01 11:21:05,939 - INFO - Epoch [243/300] (163236) train_loss: 23.4238, val_loss: 25.0822, lr: 0.000175, 95.91s
2024-03-01 11:22:35,175 - INFO - epoch complete!
2024-03-01 11:22:35,175 - INFO - evaluating now!
2024-03-01 11:22:41,389 - INFO - Epoch [244/300] (163905) train_loss: 23.4346, val_loss: 25.0374, lr: 0.000173, 95.45s
2024-03-01 11:24:10,533 - INFO - epoch complete!
2024-03-01 11:24:10,534 - INFO - evaluating now!
2024-03-01 11:24:16,766 - INFO - Epoch [245/300] (164574) train_loss: 23.4023, val_loss: 25.0641, lr: 0.000170, 95.38s
2024-03-01 11:25:45,917 - INFO - epoch complete!
2024-03-01 11:25:45,918 - INFO - evaluating now!
2024-03-01 11:25:52,139 - INFO - Epoch [246/300] (165243) train_loss: 23.4130, val_loss: 25.0579, lr: 0.000168, 95.37s
2024-03-01 11:27:33,503 - INFO - epoch complete!
2024-03-01 11:27:33,504 - INFO - evaluating now!
2024-03-01 11:27:39,706 - INFO - Epoch [247/300] (165912) train_loss: 23.4066, val_loss: 25.1172, lr: 0.000165, 107.57s
2024-03-01 11:29:08,267 - INFO - epoch complete!
2024-03-01 11:29:08,268 - INFO - evaluating now!
2024-03-01 11:29:14,467 - INFO - Epoch [248/300] (166581) train_loss: 23.3857, val_loss: 25.0588, lr: 0.000163, 94.76s
2024-03-01 11:30:43,436 - INFO - epoch complete!
2024-03-01 11:30:43,437 - INFO - evaluating now!
2024-03-01 11:30:49,642 - INFO - Epoch [249/300] (167250) train_loss: 23.3714, val_loss: 25.1980, lr: 0.000160, 95.17s
2024-03-01 11:32:18,353 - INFO - epoch complete!
2024-03-01 11:32:18,353 - INFO - evaluating now!
2024-03-01 11:32:24,556 - INFO - Epoch [250/300] (167919) train_loss: 23.3572, val_loss: 24.9442, lr: 0.000158, 94.91s
2024-03-01 11:32:24,606 - INFO - Saved model at 250
2024-03-01 11:32:24,606 - INFO - Val loss decrease from 24.9541 to 24.9442, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch250.tar
2024-03-01 11:33:53,256 - INFO - epoch complete!
2024-03-01 11:33:53,256 - INFO - evaluating now!
2024-03-01 11:33:59,433 - INFO - Epoch [251/300] (168588) train_loss: 23.3457, val_loss: 24.9423, lr: 0.000156, 94.83s
2024-03-01 11:33:59,482 - INFO - Saved model at 251
2024-03-01 11:33:59,482 - INFO - Val loss decrease from 24.9442 to 24.9423, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch251.tar
2024-03-01 11:35:28,262 - INFO - epoch complete!
2024-03-01 11:35:28,263 - INFO - evaluating now!
2024-03-01 11:35:34,450 - INFO - Epoch [252/300] (169257) train_loss: 23.3504, val_loss: 24.9890, lr: 0.000153, 94.97s
2024-03-01 11:37:03,143 - INFO - epoch complete!
2024-03-01 11:37:03,144 - INFO - evaluating now!
2024-03-01 11:37:09,379 - INFO - Epoch [253/300] (169926) train_loss: 23.3319, val_loss: 25.0791, lr: 0.000151, 94.93s
2024-03-01 11:38:37,949 - INFO - epoch complete!
2024-03-01 11:38:37,949 - INFO - evaluating now!
2024-03-01 11:38:44,143 - INFO - Epoch [254/300] (170595) train_loss: 23.3433, val_loss: 24.9187, lr: 0.000149, 94.76s
2024-03-01 11:38:44,194 - INFO - Saved model at 254
2024-03-01 11:38:44,194 - INFO - Val loss decrease from 24.9423 to 24.9187, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch254.tar
2024-03-01 11:40:13,138 - INFO - epoch complete!
2024-03-01 11:40:13,139 - INFO - evaluating now!
2024-03-01 11:40:19,325 - INFO - Epoch [255/300] (171264) train_loss: 23.3223, val_loss: 24.8879, lr: 0.000147, 95.13s
2024-03-01 11:40:19,375 - INFO - Saved model at 255
2024-03-01 11:40:19,375 - INFO - Val loss decrease from 24.9187 to 24.8879, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch255.tar
2024-03-01 11:41:47,979 - INFO - epoch complete!
2024-03-01 11:41:47,979 - INFO - evaluating now!
2024-03-01 11:41:54,170 - INFO - Epoch [256/300] (171933) train_loss: 23.3187, val_loss: 24.9761, lr: 0.000145, 94.79s
2024-03-01 11:43:23,390 - INFO - epoch complete!
2024-03-01 11:43:23,390 - INFO - evaluating now!
2024-03-01 11:43:29,582 - INFO - Epoch [257/300] (172602) train_loss: 23.2973, val_loss: 24.9663, lr: 0.000143, 95.41s
2024-03-01 11:45:11,005 - INFO - epoch complete!
2024-03-01 11:45:11,006 - INFO - evaluating now!
2024-03-01 11:45:17,228 - INFO - Epoch [258/300] (173271) train_loss: 23.2863, val_loss: 24.9340, lr: 0.000141, 107.65s
2024-03-01 11:46:46,217 - INFO - epoch complete!
2024-03-01 11:46:46,218 - INFO - evaluating now!
2024-03-01 11:46:52,410 - INFO - Epoch [259/300] (173940) train_loss: 23.2889, val_loss: 24.9900, lr: 0.000139, 95.18s
2024-03-01 11:48:21,606 - INFO - epoch complete!
2024-03-01 11:48:21,606 - INFO - evaluating now!
2024-03-01 11:48:27,795 - INFO - Epoch [260/300] (174609) train_loss: 23.2772, val_loss: 24.9528, lr: 0.000137, 95.38s
2024-03-01 11:49:56,904 - INFO - epoch complete!
2024-03-01 11:49:56,904 - INFO - evaluating now!
2024-03-01 11:50:03,123 - INFO - Epoch [261/300] (175278) train_loss: 23.2659, val_loss: 24.9892, lr: 0.000135, 95.33s
2024-03-01 11:51:31,853 - INFO - epoch complete!
2024-03-01 11:51:31,853 - INFO - evaluating now!
2024-03-01 11:51:38,056 - INFO - Epoch [262/300] (175947) train_loss: 23.2795, val_loss: 24.9883, lr: 0.000133, 94.93s
2024-03-01 11:53:07,178 - INFO - epoch complete!
2024-03-01 11:53:07,179 - INFO - evaluating now!
2024-03-01 11:53:13,368 - INFO - Epoch [263/300] (176616) train_loss: 23.2745, val_loss: 24.8858, lr: 0.000132, 95.31s
2024-03-01 11:53:13,418 - INFO - Saved model at 263
2024-03-01 11:53:13,418 - INFO - Val loss decrease from 24.8879 to 24.8858, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch263.tar
2024-03-01 11:54:44,984 - INFO - epoch complete!
2024-03-01 11:54:44,985 - INFO - evaluating now!
2024-03-01 11:54:51,191 - INFO - Epoch [264/300] (177285) train_loss: 23.2544, val_loss: 24.9701, lr: 0.000130, 97.77s
2024-03-01 11:56:20,749 - INFO - epoch complete!
2024-03-01 11:56:20,750 - INFO - evaluating now!
2024-03-01 11:56:26,955 - INFO - Epoch [265/300] (177954) train_loss: 23.2375, val_loss: 25.0214, lr: 0.000128, 95.76s
2024-03-01 11:57:55,553 - INFO - epoch complete!
2024-03-01 11:57:55,554 - INFO - evaluating now!
2024-03-01 11:58:01,749 - INFO - Epoch [266/300] (178623) train_loss: 23.2332, val_loss: 24.9198, lr: 0.000127, 94.79s
2024-03-01 11:59:44,493 - INFO - epoch complete!
2024-03-01 11:59:44,494 - INFO - evaluating now!
2024-03-01 11:59:50,704 - INFO - Epoch [267/300] (179292) train_loss: 23.2301, val_loss: 24.9316, lr: 0.000125, 108.95s
2024-03-01 12:01:19,425 - INFO - epoch complete!
2024-03-01 12:01:19,426 - INFO - evaluating now!
2024-03-01 12:01:25,615 - INFO - Epoch [268/300] (179961) train_loss: 23.2221, val_loss: 25.0671, lr: 0.000124, 94.91s
2024-03-01 12:02:54,366 - INFO - epoch complete!
2024-03-01 12:02:54,367 - INFO - evaluating now!
2024-03-01 12:03:00,572 - INFO - Epoch [269/300] (180630) train_loss: 23.2275, val_loss: 25.0093, lr: 0.000122, 94.96s
2024-03-01 12:04:29,328 - INFO - epoch complete!
2024-03-01 12:04:29,329 - INFO - evaluating now!
2024-03-01 12:04:35,541 - INFO - Epoch [270/300] (181299) train_loss: 23.1914, val_loss: 24.9874, lr: 0.000121, 94.97s
2024-03-01 12:06:04,413 - INFO - epoch complete!
2024-03-01 12:06:04,414 - INFO - evaluating now!
2024-03-01 12:06:10,626 - INFO - Epoch [271/300] (181968) train_loss: 23.1942, val_loss: 24.9807, lr: 0.000119, 95.09s
2024-03-01 12:07:39,315 - INFO - epoch complete!
2024-03-01 12:07:39,316 - INFO - evaluating now!
2024-03-01 12:07:45,506 - INFO - Epoch [272/300] (182637) train_loss: 23.1865, val_loss: 24.9022, lr: 0.000118, 94.88s
2024-03-01 12:09:15,408 - INFO - epoch complete!
2024-03-01 12:09:15,409 - INFO - evaluating now!
2024-03-01 12:09:21,601 - INFO - Epoch [273/300] (183306) train_loss: 23.1881, val_loss: 24.9205, lr: 0.000117, 96.09s
2024-03-01 12:10:52,170 - INFO - epoch complete!
2024-03-01 12:10:52,171 - INFO - evaluating now!
2024-03-01 12:10:58,363 - INFO - Epoch [274/300] (183975) train_loss: 23.1957, val_loss: 24.9928, lr: 0.000115, 96.76s
2024-03-01 12:12:28,471 - INFO - epoch complete!
2024-03-01 12:12:28,472 - INFO - evaluating now!
2024-03-01 12:12:34,704 - INFO - Epoch [275/300] (184644) train_loss: 23.1579, val_loss: 24.9701, lr: 0.000114, 96.34s
2024-03-01 12:14:05,962 - INFO - epoch complete!
2024-03-01 12:14:05,962 - INFO - evaluating now!
2024-03-01 12:14:12,210 - INFO - Epoch [276/300] (185313) train_loss: 23.1732, val_loss: 24.9919, lr: 0.000113, 97.51s
2024-03-01 12:15:43,750 - INFO - epoch complete!
2024-03-01 12:15:43,751 - INFO - evaluating now!
2024-03-01 12:15:50,019 - INFO - Epoch [277/300] (185982) train_loss: 23.1695, val_loss: 24.9834, lr: 0.000112, 97.81s
2024-03-01 12:17:20,946 - INFO - epoch complete!
2024-03-01 12:17:20,947 - INFO - evaluating now!
2024-03-01 12:17:27,181 - INFO - Epoch [278/300] (186651) train_loss: 23.1547, val_loss: 25.0822, lr: 0.000111, 97.16s
2024-03-01 12:18:58,453 - INFO - epoch complete!
2024-03-01 12:18:58,453 - INFO - evaluating now!
2024-03-01 12:19:04,740 - INFO - Epoch [279/300] (187320) train_loss: 23.1522, val_loss: 24.8880, lr: 0.000110, 97.56s
2024-03-01 12:20:36,063 - INFO - epoch complete!
2024-03-01 12:20:36,063 - INFO - evaluating now!
2024-03-01 12:20:42,291 - INFO - Epoch [280/300] (187989) train_loss: 23.1550, val_loss: 24.9199, lr: 0.000109, 97.55s
2024-03-01 12:22:13,264 - INFO - epoch complete!
2024-03-01 12:22:13,265 - INFO - evaluating now!
2024-03-01 12:22:19,506 - INFO - Epoch [281/300] (188658) train_loss: 23.1518, val_loss: 25.0221, lr: 0.000108, 97.21s
2024-03-01 12:23:51,011 - INFO - epoch complete!
2024-03-01 12:23:51,012 - INFO - evaluating now!
2024-03-01 12:23:57,239 - INFO - Epoch [282/300] (189327) train_loss: 23.1560, val_loss: 24.9960, lr: 0.000107, 97.73s
2024-03-01 12:25:28,537 - INFO - epoch complete!
2024-03-01 12:25:28,538 - INFO - evaluating now!
2024-03-01 12:25:34,763 - INFO - Epoch [283/300] (189996) train_loss: 23.1329, val_loss: 24.9992, lr: 0.000106, 97.52s
2024-03-01 12:27:06,367 - INFO - epoch complete!
2024-03-01 12:27:06,368 - INFO - evaluating now!
2024-03-01 12:27:12,601 - INFO - Epoch [284/300] (190665) train_loss: 23.1348, val_loss: 24.9775, lr: 0.000106, 97.84s
2024-03-01 12:28:59,254 - INFO - epoch complete!
2024-03-01 12:28:59,255 - INFO - evaluating now!
2024-03-01 12:29:05,599 - INFO - Epoch [285/300] (191334) train_loss: 23.1175, val_loss: 25.0448, lr: 0.000105, 113.00s
2024-03-01 12:30:36,766 - INFO - epoch complete!
2024-03-01 12:30:36,767 - INFO - evaluating now!
2024-03-01 12:30:43,007 - INFO - Epoch [286/300] (192003) train_loss: 23.1137, val_loss: 24.9074, lr: 0.000104, 97.41s
2024-03-01 12:32:14,075 - INFO - epoch complete!
2024-03-01 12:32:14,075 - INFO - evaluating now!
2024-03-01 12:32:20,292 - INFO - Epoch [287/300] (192672) train_loss: 23.1113, val_loss: 24.8616, lr: 0.000104, 97.28s
2024-03-01 12:32:20,341 - INFO - Saved model at 287
2024-03-01 12:32:20,342 - INFO - Val loss decrease from 24.8858 to 24.8616, saving to ./libcity/cache/77988/model_cache/PDFormer_PeMS08_epoch287.tar
2024-03-01 12:33:51,415 - INFO - epoch complete!
2024-03-01 12:33:51,416 - INFO - evaluating now!
2024-03-01 12:33:57,640 - INFO - Epoch [288/300] (193341) train_loss: 23.1256, val_loss: 25.0002, lr: 0.000103, 97.30s
2024-03-01 12:35:31,120 - INFO - epoch complete!
2024-03-01 12:35:31,121 - INFO - evaluating now!
2024-03-01 12:35:37,366 - INFO - Epoch [289/300] (194010) train_loss: 23.1152, val_loss: 24.9724, lr: 0.000102, 99.73s
2024-03-01 12:37:08,144 - INFO - epoch complete!
2024-03-01 12:37:08,145 - INFO - evaluating now!
2024-03-01 12:37:14,355 - INFO - Epoch [290/300] (194679) train_loss: 23.1234, val_loss: 24.9670, lr: 0.000102, 96.99s
2024-03-01 12:38:45,358 - INFO - epoch complete!
2024-03-01 12:38:45,359 - INFO - evaluating now!
2024-03-01 12:38:51,559 - INFO - Epoch [291/300] (195348) train_loss: 23.0811, val_loss: 24.9265, lr: 0.000102, 97.20s
2024-03-01 12:40:22,270 - INFO - epoch complete!
2024-03-01 12:40:22,270 - INFO - evaluating now!
2024-03-01 12:40:28,459 - INFO - Epoch [292/300] (196017) train_loss: 23.1250, val_loss: 24.9708, lr: 0.000101, 96.90s
2024-03-01 12:41:59,584 - INFO - epoch complete!
2024-03-01 12:41:59,585 - INFO - evaluating now!
2024-03-01 12:42:05,835 - INFO - Epoch [293/300] (196686) train_loss: 23.0977, val_loss: 24.9373, lr: 0.000101, 97.37s
2024-03-01 12:43:36,701 - INFO - epoch complete!
2024-03-01 12:43:36,702 - INFO - evaluating now!
2024-03-01 12:43:42,921 - INFO - Epoch [294/300] (197355) train_loss: 23.0782, val_loss: 25.0168, lr: 0.000101, 97.09s
2024-03-01 12:45:13,852 - INFO - epoch complete!
2024-03-01 12:45:13,852 - INFO - evaluating now!
2024-03-01 12:45:20,054 - INFO - Epoch [295/300] (198024) train_loss: 23.0821, val_loss: 24.9827, lr: 0.000100, 97.13s
2024-03-01 12:46:50,977 - INFO - epoch complete!
2024-03-01 12:46:50,977 - INFO - evaluating now!
2024-03-01 12:46:57,176 - INFO - Epoch [296/300] (198693) train_loss: 23.0967, val_loss: 24.9987, lr: 0.000100, 97.12s
2024-03-01 12:48:27,907 - INFO - epoch complete!
2024-03-01 12:48:27,908 - INFO - evaluating now!
2024-03-01 12:48:34,115 - INFO - Epoch [297/300] (199362) train_loss: 23.1013, val_loss: 24.8916, lr: 0.000100, 96.94s
2024-03-01 12:50:04,912 - INFO - epoch complete!
2024-03-01 12:50:04,912 - INFO - evaluating now!
2024-03-01 12:50:11,149 - INFO - Epoch [298/300] (200031) train_loss: 23.0990, val_loss: 24.8996, lr: 0.000100, 97.03s
2024-03-01 12:51:42,141 - INFO - epoch complete!
2024-03-01 12:51:42,142 - INFO - evaluating now!
2024-03-01 12:51:48,339 - INFO - Epoch [299/300] (200700) train_loss: 23.0905, val_loss: 24.9598, lr: 0.000100, 97.19s
2024-03-01 12:51:48,340 - INFO - Trained totally 300 epochs, average train time is 131.633s, average eval time is 10.202s
2024-03-01 12:51:48,394 - INFO - Loaded model at 287
2024-03-01 12:51:48,395 - INFO - Saved model at ./libcity/cache/77988/model_cache/PDFormer_PeMS08.m
2024-03-01 12:51:48,442 - INFO - Start evaluating ...
2024-03-01 12:52:01,722 - INFO - Note that you select the average mode to evaluate!
2024-03-01 12:52:01,730 - INFO - Evaluate result is saved at ./libcity/cache/77988/evaluate_cache/2024_03_01_12_52_01_PDFormer_PeMS08_average.csv
2024-03-01 12:52:01,738 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   11.593058   inf  19.531469   11.609567     0.076243    19.427502
2   11.797006   inf  20.086662   11.814039     0.077393    19.986710
3   11.994059   inf  20.551325   12.011694     0.078612    20.454695
4   12.160835   inf  20.957354   12.179009     0.079615    20.863998
5   12.312041   inf  21.306002   12.330720     0.080581    21.215811
6   12.449018   inf  21.623304   12.468039     0.081458    21.534275
7   12.577375   inf  21.913349   12.596676     0.082311    21.825245
8   12.701881   inf  22.186798   12.721396     0.083171    22.099098
9   12.820017   inf  22.440559   12.839742     0.083965    22.353230
10  12.930862   inf  22.670408   12.950791     0.084772    22.583401
11  13.042087   inf  22.870859   13.062179     0.085601    22.784054
12  13.159331   inf  23.076691   13.179639     0.086456    22.989935
