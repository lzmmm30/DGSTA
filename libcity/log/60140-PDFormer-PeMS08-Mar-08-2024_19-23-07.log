2024-03-08 19:23:07,378 - INFO - Log directory: ./libcity/log
2024-03-08 19:23:07,378 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS08, exp_id=60140
2024-03-08 19:23:07,378 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS08', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 2776, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 21, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 6, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 2, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS08'], 'geo_file': 'PeMS08', 'rel_file': 'PeMS08', 'adp_file': 'PeMS08', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 60140}
2024-03-08 19:23:07,938 - INFO - Loaded file PeMS08.geo, num_nodes=170
2024-03-08 19:23:07,941 - INFO - set_weight_link_or_dist: link
2024-03-08 19:23:07,941 - INFO - init_weight_inf_or_zero: zero
2024-03-08 19:23:07,945 - INFO - Loaded file PeMS08.rel, shape=(170, 170)
2024-03-08 19:23:07,945 - INFO - Max adj_mx value = 1.0
2024-03-08 19:23:31,583 - INFO - Loading file PeMS08.dyna
2024-03-08 19:23:35,768 - INFO - Loaded file PeMS08.dyna, shape=(17856, 170, 1)
2024-03-08 19:23:35,796 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS08.npy
2024-03-08 19:23:35,797 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS08_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-03-08 19:23:57,642 - INFO - train	x: (10700, 12, 170, 9), y: (10700, 12, 170, 9), ind: (10700,)
2024-03-08 19:23:57,643 - INFO - eval	x: (3566, 12, 170, 9), y: (3566, 12, 170, 9), ind: (3566,)
2024-03-08 19:23:57,643 - INFO - test	x: (3567, 12, 170, 9), y: (3567, 12, 170, 9), ind: (3567,)
2024-03-08 19:23:58,989 - INFO - StandardScaler mean: 229.8431355598314, std: 145.62553066568907
2024-03-08 19:23:58,990 - INFO - NoneScaler
2024-03-08 19:24:02,405 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS08_21_3_16_5.npy
2024-03-08 19:24:02,418 - INFO - Use use_curriculum_learning!
2024-03-08 19:24:12,540 - INFO - Number of isolated points: 0
2024-03-08 19:24:12,567 - INFO - Number of isolated points: 0
2024-03-08 19:24:12,675 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-03-08 19:24:12,682 - INFO - a	torch.Size([1])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-08 19:24:12,683 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,684 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,685 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,686 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,687 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,688 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,689 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-08 19:24:12,690 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-08 19:24:12,691 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,692 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,693 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,694 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-08 19:24:12,695 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,696 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,697 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,698 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,704 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,704 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-08 19:24:12,704 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,704 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-08 19:24:12,704 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,704 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,705 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,706 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,707 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,708 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.5.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,709 - INFO - encoder_blocks.5.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,710 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,711 - INFO - encoder_blocks.5.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,712 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - encoder_blocks.5.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-08 19:24:12,713 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.5.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-08 19:24:12,714 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2024-03-08 19:24:12,715 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-03-08 19:24:12,715 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-03-08 19:24:12,715 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-03-08 19:24:12,715 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-03-08 19:24:12,717 - INFO - Total parameter numbers: 1104094
2024-03-08 19:24:12,721 - INFO - You select `adamw` optimizer.
2024-03-08 19:24:12,724 - INFO - You select `cosinelr` lr_scheduler.
2024-03-08 19:24:12,724 - WARNING - Received none train loss func and will use the loss func defined in the model.
2024-03-08 19:24:12,726 - INFO - Number of isolated points: 0
2024-03-08 19:24:12,760 - INFO - Start training ...
2024-03-08 19:24:12,761 - INFO - num_batches:669
2024-03-08 19:24:12,938 - INFO - Training: task_level increase from 0 to 1
2024-03-08 19:24:12,938 - INFO - Current batches_seen is 0
2024-03-08 19:26:42,092 - INFO - epoch complete!
2024-03-08 19:26:42,094 - INFO - evaluating now!
2024-03-08 19:26:53,613 - INFO - Epoch [0/300] (669) train_loss: 240.1655, val_loss: 244.1414, lr: 0.000201, 160.85s
2024-03-08 19:26:53,727 - INFO - Saved model at 0
2024-03-08 19:26:53,728 - INFO - Val loss decrease from inf to 244.1414, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch0.tar
2024-03-08 19:29:08,798 - INFO - epoch complete!
2024-03-08 19:29:08,799 - INFO - evaluating now!
2024-03-08 19:29:20,489 - INFO - Epoch [1/300] (1338) train_loss: 52.7134, val_loss: 170.3031, lr: 0.000401, 146.76s
2024-03-08 19:29:20,603 - INFO - Saved model at 1
2024-03-08 19:29:20,604 - INFO - Val loss decrease from 244.1414 to 170.3031, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch1.tar
2024-03-08 19:31:36,314 - INFO - epoch complete!
2024-03-08 19:31:36,315 - INFO - evaluating now!
2024-03-08 19:31:48,036 - INFO - Epoch [2/300] (2007) train_loss: 36.0003, val_loss: 169.5788, lr: 0.000600, 147.43s
2024-03-08 19:31:48,154 - INFO - Saved model at 2
2024-03-08 19:31:48,155 - INFO - Val loss decrease from 170.3031 to 169.5788, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch2.tar
2024-03-08 19:34:05,680 - INFO - epoch complete!
2024-03-08 19:34:05,681 - INFO - evaluating now!
2024-03-08 19:34:17,294 - INFO - Epoch [3/300] (2676) train_loss: 31.9150, val_loss: 169.7355, lr: 0.000800, 149.14s
2024-03-08 19:34:38,015 - INFO - Training: task_level increase from 1 to 2
2024-03-08 19:34:38,016 - INFO - Current batches_seen is 2776
2024-03-08 19:36:37,031 - INFO - epoch complete!
2024-03-08 19:36:37,032 - INFO - evaluating now!
2024-03-08 19:36:48,606 - INFO - Epoch [4/300] (3345) train_loss: 37.8437, val_loss: 152.5181, lr: 0.000999, 151.31s
2024-03-08 19:36:48,717 - INFO - Saved model at 4
2024-03-08 19:36:48,718 - INFO - Val loss decrease from 169.5788 to 152.5181, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch4.tar
2024-03-08 19:39:05,657 - INFO - epoch complete!
2024-03-08 19:39:05,658 - INFO - evaluating now!
2024-03-08 19:39:17,346 - INFO - Epoch [5/300] (4014) train_loss: 31.0589, val_loss: 154.1899, lr: 0.000999, 148.63s
2024-03-08 19:41:34,270 - INFO - epoch complete!
2024-03-08 19:41:34,271 - INFO - evaluating now!
2024-03-08 19:41:45,914 - INFO - Epoch [6/300] (4683) train_loss: 29.8815, val_loss: 155.4823, lr: 0.000999, 148.57s
2024-03-08 19:44:02,946 - INFO - epoch complete!
2024-03-08 19:44:02,947 - INFO - evaluating now!
2024-03-08 19:44:14,570 - INFO - Epoch [7/300] (5352) train_loss: 29.2659, val_loss: 156.5245, lr: 0.000998, 148.65s
2024-03-08 19:44:55,654 - INFO - Training: task_level increase from 2 to 3
2024-03-08 19:44:55,654 - INFO - Current batches_seen is 5552
2024-03-08 19:46:31,540 - INFO - epoch complete!
2024-03-08 19:46:31,541 - INFO - evaluating now!
2024-03-08 19:46:43,119 - INFO - Epoch [8/300] (6021) train_loss: 30.5907, val_loss: 141.2729, lr: 0.000998, 148.55s
2024-03-08 19:46:43,231 - INFO - Saved model at 8
2024-03-08 19:46:43,232 - INFO - Val loss decrease from 152.5181 to 141.2729, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch8.tar
2024-03-08 19:48:59,486 - INFO - epoch complete!
2024-03-08 19:48:59,487 - INFO - evaluating now!
2024-03-08 19:49:11,156 - INFO - Epoch [9/300] (6690) train_loss: 29.6632, val_loss: 142.5439, lr: 0.000998, 147.92s
2024-03-08 19:51:28,171 - INFO - epoch complete!
2024-03-08 19:51:28,172 - INFO - evaluating now!
2024-03-08 19:51:39,911 - INFO - Epoch [10/300] (7359) train_loss: 29.5605, val_loss: 143.1013, lr: 0.000997, 148.75s
2024-03-08 19:53:57,641 - INFO - epoch complete!
2024-03-08 19:53:57,642 - INFO - evaluating now!
2024-03-08 19:54:09,420 - INFO - Epoch [11/300] (8028) train_loss: 29.0333, val_loss: 143.0614, lr: 0.000996, 149.51s
2024-03-08 19:55:11,176 - INFO - Training: task_level increase from 3 to 4
2024-03-08 19:55:11,176 - INFO - Current batches_seen is 8328
2024-03-08 19:56:26,751 - INFO - epoch complete!
2024-03-08 19:56:26,752 - INFO - evaluating now!
2024-03-08 19:56:38,499 - INFO - Epoch [12/300] (8697) train_loss: 29.9268, val_loss: 133.5769, lr: 0.000996, 149.08s
2024-03-08 19:56:38,611 - INFO - Saved model at 12
2024-03-08 19:56:38,612 - INFO - Val loss decrease from 141.2729 to 133.5769, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch12.tar
2024-03-08 19:58:55,151 - INFO - epoch complete!
2024-03-08 19:58:55,152 - INFO - evaluating now!
2024-03-08 19:59:06,786 - INFO - Epoch [13/300] (9366) train_loss: 29.9323, val_loss: 133.1175, lr: 0.000995, 148.17s
2024-03-08 19:59:06,900 - INFO - Saved model at 13
2024-03-08 19:59:06,901 - INFO - Val loss decrease from 133.5769 to 133.1175, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch13.tar
2024-03-08 20:01:22,695 - INFO - epoch complete!
2024-03-08 20:01:22,695 - INFO - evaluating now!
2024-03-08 20:01:34,366 - INFO - Epoch [14/300] (10035) train_loss: 29.3928, val_loss: 133.9852, lr: 0.000994, 147.46s
2024-03-08 20:03:50,868 - INFO - epoch complete!
2024-03-08 20:03:50,869 - INFO - evaluating now!
2024-03-08 20:04:02,588 - INFO - Epoch [15/300] (10704) train_loss: 29.3184, val_loss: 133.2542, lr: 0.000994, 148.22s
2024-03-08 20:05:24,407 - INFO - Training: task_level increase from 4 to 5
2024-03-08 20:05:24,407 - INFO - Current batches_seen is 11104
2024-03-08 20:06:19,231 - INFO - epoch complete!
2024-03-08 20:06:19,232 - INFO - evaluating now!
2024-03-08 20:06:30,843 - INFO - Epoch [16/300] (11373) train_loss: 29.7349, val_loss: 121.4755, lr: 0.000993, 148.25s
2024-03-08 20:06:30,957 - INFO - Saved model at 16
2024-03-08 20:06:30,958 - INFO - Val loss decrease from 133.1175 to 121.4755, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch16.tar
2024-03-08 20:08:47,155 - INFO - epoch complete!
2024-03-08 20:08:47,156 - INFO - evaluating now!
2024-03-08 20:08:58,698 - INFO - Epoch [17/300] (12042) train_loss: 29.7623, val_loss: 122.6033, lr: 0.000992, 147.74s
2024-03-08 20:11:15,313 - INFO - epoch complete!
2024-03-08 20:11:15,314 - INFO - evaluating now!
2024-03-08 20:11:26,874 - INFO - Epoch [18/300] (12711) train_loss: 29.3975, val_loss: 122.3076, lr: 0.000991, 148.18s
2024-03-08 20:13:43,378 - INFO - epoch complete!
2024-03-08 20:13:43,379 - INFO - evaluating now!
2024-03-08 20:13:54,871 - INFO - Epoch [19/300] (13380) train_loss: 29.1767, val_loss: 123.4945, lr: 0.000990, 148.00s
2024-03-08 20:15:36,008 - INFO - Training: task_level increase from 5 to 6
2024-03-08 20:15:36,009 - INFO - Current batches_seen is 13880
2024-03-08 20:16:10,135 - INFO - epoch complete!
2024-03-08 20:16:10,136 - INFO - evaluating now!
2024-03-08 20:16:21,683 - INFO - Epoch [20/300] (14049) train_loss: 28.9095, val_loss: 122.8255, lr: 0.000989, 146.81s
2024-03-08 20:18:37,116 - INFO - epoch complete!
2024-03-08 20:18:37,117 - INFO - evaluating now!
2024-03-08 20:18:48,644 - INFO - Epoch [21/300] (14718) train_loss: 29.3839, val_loss: 122.8899, lr: 0.000988, 146.96s
2024-03-08 20:21:04,214 - INFO - epoch complete!
2024-03-08 20:21:04,215 - INFO - evaluating now!
2024-03-08 20:21:15,703 - INFO - Epoch [22/300] (15387) train_loss: 29.0441, val_loss: 123.2555, lr: 0.000987, 147.06s
2024-03-08 20:23:30,873 - INFO - epoch complete!
2024-03-08 20:23:30,874 - INFO - evaluating now!
2024-03-08 20:23:42,458 - INFO - Epoch [23/300] (16056) train_loss: 28.7046, val_loss: 123.4610, lr: 0.000986, 146.75s
2024-03-08 20:25:44,822 - INFO - Training: task_level increase from 6 to 7
2024-03-08 20:25:44,822 - INFO - Current batches_seen is 16656
2024-03-08 20:25:58,883 - INFO - epoch complete!
2024-03-08 20:25:58,883 - INFO - evaluating now!
2024-03-08 20:26:10,481 - INFO - Epoch [24/300] (16725) train_loss: 29.1601, val_loss: 106.9971, lr: 0.000985, 148.02s
2024-03-08 20:26:10,592 - INFO - Saved model at 24
2024-03-08 20:26:10,593 - INFO - Val loss decrease from 121.4755 to 106.9971, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch24.tar
2024-03-08 20:28:27,192 - INFO - epoch complete!
2024-03-08 20:28:27,193 - INFO - evaluating now!
2024-03-08 20:28:38,795 - INFO - Epoch [25/300] (17394) train_loss: 29.0905, val_loss: 106.2502, lr: 0.000983, 148.20s
2024-03-08 20:28:38,905 - INFO - Saved model at 25
2024-03-08 20:28:38,907 - INFO - Val loss decrease from 106.9971 to 106.2502, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch25.tar
2024-03-08 20:30:55,560 - INFO - epoch complete!
2024-03-08 20:30:55,561 - INFO - evaluating now!
2024-03-08 20:31:07,195 - INFO - Epoch [26/300] (18063) train_loss: 29.1577, val_loss: 106.2931, lr: 0.000982, 148.29s
2024-03-08 20:33:23,792 - INFO - epoch complete!
2024-03-08 20:33:23,793 - INFO - evaluating now!
2024-03-08 20:33:35,366 - INFO - Epoch [27/300] (18732) train_loss: 28.5650, val_loss: 107.0620, lr: 0.000981, 148.17s
2024-03-08 20:35:51,991 - INFO - epoch complete!
2024-03-08 20:35:51,992 - INFO - evaluating now!
2024-03-08 20:36:03,632 - INFO - Epoch [28/300] (19401) train_loss: 28.4807, val_loss: 106.7355, lr: 0.000979, 148.27s
2024-03-08 20:36:10,012 - INFO - Training: task_level increase from 7 to 8
2024-03-08 20:36:10,012 - INFO - Current batches_seen is 19432
2024-03-08 20:38:20,521 - INFO - epoch complete!
2024-03-08 20:38:20,522 - INFO - evaluating now!
2024-03-08 20:38:32,212 - INFO - Epoch [29/300] (20070) train_loss: 29.3449, val_loss: 94.1880, lr: 0.000978, 148.58s
2024-03-08 20:38:32,326 - INFO - Saved model at 29
2024-03-08 20:38:32,326 - INFO - Val loss decrease from 106.2502 to 94.1880, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch29.tar
2024-03-08 20:40:49,067 - INFO - epoch complete!
2024-03-08 20:40:49,068 - INFO - evaluating now!
2024-03-08 20:41:00,659 - INFO - Epoch [30/300] (20739) train_loss: 28.7904, val_loss: 94.2490, lr: 0.000976, 148.33s
2024-03-08 20:43:17,697 - INFO - epoch complete!
2024-03-08 20:43:17,698 - INFO - evaluating now!
2024-03-08 20:43:29,351 - INFO - Epoch [31/300] (21408) train_loss: 28.3696, val_loss: 95.1004, lr: 0.000975, 148.69s
2024-03-08 20:45:46,038 - INFO - epoch complete!
2024-03-08 20:45:46,039 - INFO - evaluating now!
2024-03-08 20:45:57,601 - INFO - Epoch [32/300] (22077) train_loss: 28.2620, val_loss: 93.8599, lr: 0.000973, 148.25s
2024-03-08 20:45:57,712 - INFO - Saved model at 32
2024-03-08 20:45:57,713 - INFO - Val loss decrease from 94.1880 to 93.8599, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch32.tar
2024-03-08 20:46:24,402 - INFO - Training: task_level increase from 8 to 9
2024-03-08 20:46:24,402 - INFO - Current batches_seen is 22208
2024-03-08 20:48:14,072 - INFO - epoch complete!
2024-03-08 20:48:14,073 - INFO - evaluating now!
2024-03-08 20:48:25,691 - INFO - Epoch [33/300] (22746) train_loss: 28.9182, val_loss: 76.5527, lr: 0.000972, 147.98s
2024-03-08 20:48:25,808 - INFO - Saved model at 33
2024-03-08 20:48:25,809 - INFO - Val loss decrease from 93.8599 to 76.5527, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch33.tar
2024-03-08 20:50:42,858 - INFO - epoch complete!
2024-03-08 20:50:42,859 - INFO - evaluating now!
2024-03-08 20:50:54,531 - INFO - Epoch [34/300] (23415) train_loss: 28.5367, val_loss: 76.3525, lr: 0.000970, 148.72s
2024-03-08 20:50:54,651 - INFO - Saved model at 34
2024-03-08 20:50:54,652 - INFO - Val loss decrease from 76.5527 to 76.3525, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch34.tar
2024-03-08 20:53:11,383 - INFO - epoch complete!
2024-03-08 20:53:11,384 - INFO - evaluating now!
2024-03-08 20:53:23,128 - INFO - Epoch [35/300] (24084) train_loss: 28.2821, val_loss: 76.7784, lr: 0.000968, 148.47s
2024-03-08 20:55:39,838 - INFO - epoch complete!
2024-03-08 20:55:39,839 - INFO - evaluating now!
2024-03-08 20:55:51,408 - INFO - Epoch [36/300] (24753) train_loss: 28.2314, val_loss: 77.4876, lr: 0.000967, 148.28s
2024-03-08 20:56:38,488 - INFO - Training: task_level increase from 9 to 10
2024-03-08 20:56:38,489 - INFO - Current batches_seen is 24984
2024-03-08 20:58:07,969 - INFO - epoch complete!
2024-03-08 20:58:07,970 - INFO - evaluating now!
2024-03-08 20:58:19,510 - INFO - Epoch [37/300] (25422) train_loss: 28.6589, val_loss: 62.1849, lr: 0.000965, 148.10s
2024-03-08 20:58:19,627 - INFO - Saved model at 37
2024-03-08 20:58:19,628 - INFO - Val loss decrease from 76.3525 to 62.1849, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch37.tar
2024-03-08 21:00:36,106 - INFO - epoch complete!
2024-03-08 21:00:36,107 - INFO - evaluating now!
2024-03-08 21:00:47,613 - INFO - Epoch [38/300] (26091) train_loss: 28.6594, val_loss: 62.2705, lr: 0.000963, 147.98s
2024-03-08 21:03:04,012 - INFO - epoch complete!
2024-03-08 21:03:04,014 - INFO - evaluating now!
2024-03-08 21:03:15,652 - INFO - Epoch [39/300] (26760) train_loss: 28.6064, val_loss: 62.5423, lr: 0.000961, 148.04s
2024-03-08 21:05:32,012 - INFO - epoch complete!
2024-03-08 21:05:32,013 - INFO - evaluating now!
2024-03-08 21:05:43,670 - INFO - Epoch [40/300] (27429) train_loss: 28.2764, val_loss: 62.5125, lr: 0.000959, 148.02s
2024-03-08 21:06:51,105 - INFO - Training: task_level increase from 10 to 11
2024-03-08 21:06:51,105 - INFO - Current batches_seen is 27760
2024-03-08 21:07:59,780 - INFO - epoch complete!
2024-03-08 21:07:59,781 - INFO - evaluating now!
2024-03-08 21:08:11,356 - INFO - Epoch [41/300] (28098) train_loss: 28.8035, val_loss: 45.2604, lr: 0.000957, 147.68s
2024-03-08 21:08:11,470 - INFO - Saved model at 41
2024-03-08 21:08:11,471 - INFO - Val loss decrease from 62.1849 to 45.2604, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch41.tar
2024-03-08 21:10:27,804 - INFO - epoch complete!
2024-03-08 21:10:27,805 - INFO - evaluating now!
2024-03-08 21:10:39,390 - INFO - Epoch [42/300] (28767) train_loss: 28.5887, val_loss: 45.9554, lr: 0.000955, 147.92s
2024-03-08 21:12:55,598 - INFO - epoch complete!
2024-03-08 21:12:55,599 - INFO - evaluating now!
2024-03-08 21:13:07,079 - INFO - Epoch [43/300] (29436) train_loss: 28.4292, val_loss: 45.4167, lr: 0.000953, 147.69s
2024-03-08 21:15:23,067 - INFO - epoch complete!
2024-03-08 21:15:23,068 - INFO - evaluating now!
2024-03-08 21:15:34,646 - INFO - Epoch [44/300] (30105) train_loss: 28.1995, val_loss: 45.1954, lr: 0.000951, 147.57s
2024-03-08 21:15:34,761 - INFO - Saved model at 44
2024-03-08 21:15:34,762 - INFO - Val loss decrease from 45.2604 to 45.1954, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch44.tar
2024-03-08 21:17:02,394 - INFO - Training: task_level increase from 11 to 12
2024-03-08 21:17:02,395 - INFO - Current batches_seen is 30536
2024-03-08 21:17:50,608 - INFO - epoch complete!
2024-03-08 21:17:50,609 - INFO - evaluating now!
2024-03-08 21:18:02,142 - INFO - Epoch [45/300] (30774) train_loss: 28.6874, val_loss: 28.8901, lr: 0.000949, 147.38s
2024-03-08 21:18:02,259 - INFO - Saved model at 45
2024-03-08 21:18:02,260 - INFO - Val loss decrease from 45.1954 to 28.8901, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch45.tar
2024-03-08 21:20:18,131 - INFO - epoch complete!
2024-03-08 21:20:18,132 - INFO - evaluating now!
2024-03-08 21:20:29,722 - INFO - Epoch [46/300] (31443) train_loss: 28.7545, val_loss: 28.7124, lr: 0.000947, 147.46s
2024-03-08 21:20:29,836 - INFO - Saved model at 46
2024-03-08 21:20:29,837 - INFO - Val loss decrease from 28.8901 to 28.7124, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch46.tar
2024-03-08 21:22:46,568 - INFO - epoch complete!
2024-03-08 21:22:46,569 - INFO - evaluating now!
2024-03-08 21:22:58,253 - INFO - Epoch [47/300] (32112) train_loss: 28.5265, val_loss: 28.5504, lr: 0.000944, 148.42s
2024-03-08 21:22:58,360 - INFO - Saved model at 47
2024-03-08 21:22:58,362 - INFO - Val loss decrease from 28.7124 to 28.5504, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch47.tar
2024-03-08 21:25:16,328 - INFO - epoch complete!
2024-03-08 21:25:16,329 - INFO - evaluating now!
2024-03-08 21:25:27,921 - INFO - Epoch [48/300] (32781) train_loss: 28.4379, val_loss: 27.7414, lr: 0.000942, 149.56s
2024-03-08 21:25:28,403 - INFO - Saved model at 48
2024-03-08 21:25:28,404 - INFO - Val loss decrease from 28.5504 to 27.7414, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch48.tar
2024-03-08 21:27:45,555 - INFO - epoch complete!
2024-03-08 21:27:45,555 - INFO - evaluating now!
2024-03-08 21:27:57,115 - INFO - Epoch [49/300] (33450) train_loss: 28.3731, val_loss: 28.2261, lr: 0.000940, 148.71s
2024-03-08 21:30:13,183 - INFO - epoch complete!
2024-03-08 21:30:13,184 - INFO - evaluating now!
2024-03-08 21:30:24,883 - INFO - Epoch [50/300] (34119) train_loss: 28.1510, val_loss: 27.6450, lr: 0.000937, 147.77s
2024-03-08 21:30:24,998 - INFO - Saved model at 50
2024-03-08 21:30:25,000 - INFO - Val loss decrease from 27.7414 to 27.6450, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch50.tar
2024-03-08 21:32:41,594 - INFO - epoch complete!
2024-03-08 21:32:41,595 - INFO - evaluating now!
2024-03-08 21:32:53,286 - INFO - Epoch [51/300] (34788) train_loss: 28.3307, val_loss: 27.9881, lr: 0.000935, 148.29s
2024-03-08 21:35:10,241 - INFO - epoch complete!
2024-03-08 21:35:10,242 - INFO - evaluating now!
2024-03-08 21:35:21,787 - INFO - Epoch [52/300] (35457) train_loss: 28.2410, val_loss: 27.7677, lr: 0.000932, 148.50s
2024-03-08 21:37:39,155 - INFO - epoch complete!
2024-03-08 21:37:39,156 - INFO - evaluating now!
2024-03-08 21:37:50,934 - INFO - Epoch [53/300] (36126) train_loss: 28.1029, val_loss: 27.8767, lr: 0.000930, 149.15s
2024-03-08 21:40:08,833 - INFO - epoch complete!
2024-03-08 21:40:08,834 - INFO - evaluating now!
2024-03-08 21:40:20,587 - INFO - Epoch [54/300] (36795) train_loss: 27.9515, val_loss: 27.8699, lr: 0.000927, 149.65s
2024-03-08 21:42:38,695 - INFO - epoch complete!
2024-03-08 21:42:38,696 - INFO - evaluating now!
2024-03-08 21:42:50,457 - INFO - Epoch [55/300] (37464) train_loss: 27.8816, val_loss: 27.3572, lr: 0.000925, 149.87s
2024-03-08 21:42:50,576 - INFO - Saved model at 55
2024-03-08 21:42:50,577 - INFO - Val loss decrease from 27.6450 to 27.3572, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch55.tar
2024-03-08 21:45:08,538 - INFO - epoch complete!
2024-03-08 21:45:08,539 - INFO - evaluating now!
2024-03-08 21:45:20,150 - INFO - Epoch [56/300] (38133) train_loss: 27.7799, val_loss: 27.6285, lr: 0.000922, 149.57s
2024-03-08 21:47:38,394 - INFO - epoch complete!
2024-03-08 21:47:38,395 - INFO - evaluating now!
2024-03-08 21:47:50,201 - INFO - Epoch [57/300] (38802) train_loss: 27.8734, val_loss: 27.4272, lr: 0.000920, 150.05s
2024-03-08 21:50:08,162 - INFO - epoch complete!
2024-03-08 21:50:08,163 - INFO - evaluating now!
2024-03-08 21:50:19,845 - INFO - Epoch [58/300] (39471) train_loss: 27.6791, val_loss: 27.0074, lr: 0.000917, 149.64s
2024-03-08 21:50:19,965 - INFO - Saved model at 58
2024-03-08 21:50:19,966 - INFO - Val loss decrease from 27.3572 to 27.0074, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch58.tar
2024-03-08 21:52:37,865 - INFO - epoch complete!
2024-03-08 21:52:37,866 - INFO - evaluating now!
2024-03-08 21:52:49,576 - INFO - Epoch [59/300] (40140) train_loss: 27.6298, val_loss: 27.7307, lr: 0.000914, 149.61s
2024-03-08 21:55:07,742 - INFO - epoch complete!
2024-03-08 21:55:07,743 - INFO - evaluating now!
2024-03-08 21:55:19,518 - INFO - Epoch [60/300] (40809) train_loss: 27.4738, val_loss: 27.6641, lr: 0.000911, 149.94s
2024-03-08 21:57:37,936 - INFO - epoch complete!
2024-03-08 21:57:37,937 - INFO - evaluating now!
2024-03-08 21:57:49,750 - INFO - Epoch [61/300] (41478) train_loss: 27.5263, val_loss: 27.3746, lr: 0.000908, 150.23s
2024-03-08 22:00:08,640 - INFO - epoch complete!
2024-03-08 22:00:08,640 - INFO - evaluating now!
2024-03-08 22:00:20,453 - INFO - Epoch [62/300] (42147) train_loss: 27.4210, val_loss: 27.7766, lr: 0.000906, 150.70s
2024-03-08 22:02:39,077 - INFO - epoch complete!
2024-03-08 22:02:39,078 - INFO - evaluating now!
2024-03-08 22:02:50,748 - INFO - Epoch [63/300] (42816) train_loss: 27.5152, val_loss: 27.9186, lr: 0.000903, 150.29s
2024-03-08 22:05:09,446 - INFO - epoch complete!
2024-03-08 22:05:09,447 - INFO - evaluating now!
2024-03-08 22:05:21,260 - INFO - Epoch [64/300] (43485) train_loss: 27.3204, val_loss: 27.2379, lr: 0.000900, 150.51s
2024-03-08 22:07:39,855 - INFO - epoch complete!
2024-03-08 22:07:39,856 - INFO - evaluating now!
2024-03-08 22:07:51,668 - INFO - Epoch [65/300] (44154) train_loss: 27.3884, val_loss: 27.4211, lr: 0.000897, 150.41s
2024-03-08 22:10:09,178 - INFO - epoch complete!
2024-03-08 22:10:09,179 - INFO - evaluating now!
2024-03-08 22:10:20,807 - INFO - Epoch [66/300] (44823) train_loss: 27.2322, val_loss: 26.9182, lr: 0.000894, 149.14s
2024-03-08 22:10:20,926 - INFO - Saved model at 66
2024-03-08 22:10:20,927 - INFO - Val loss decrease from 27.0074 to 26.9182, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch66.tar
2024-03-08 22:12:38,338 - INFO - epoch complete!
2024-03-08 22:12:38,339 - INFO - evaluating now!
2024-03-08 22:12:50,089 - INFO - Epoch [67/300] (45492) train_loss: 27.1984, val_loss: 27.1594, lr: 0.000891, 149.16s
2024-03-08 22:15:07,347 - INFO - epoch complete!
2024-03-08 22:15:07,348 - INFO - evaluating now!
2024-03-08 22:15:19,051 - INFO - Epoch [68/300] (46161) train_loss: 27.2403, val_loss: 28.3269, lr: 0.000888, 148.96s
2024-03-08 22:17:35,693 - INFO - epoch complete!
2024-03-08 22:17:35,694 - INFO - evaluating now!
2024-03-08 22:17:47,383 - INFO - Epoch [69/300] (46830) train_loss: 27.1829, val_loss: 27.3360, lr: 0.000884, 148.33s
2024-03-08 22:20:04,029 - INFO - epoch complete!
2024-03-08 22:20:04,030 - INFO - evaluating now!
2024-03-08 22:20:15,777 - INFO - Epoch [70/300] (47499) train_loss: 27.0577, val_loss: 27.1369, lr: 0.000881, 148.39s
2024-03-08 22:22:32,594 - INFO - epoch complete!
2024-03-08 22:22:32,594 - INFO - evaluating now!
2024-03-08 22:22:44,309 - INFO - Epoch [71/300] (48168) train_loss: 27.0685, val_loss: 27.8229, lr: 0.000878, 148.53s
2024-03-08 22:25:01,202 - INFO - epoch complete!
2024-03-08 22:25:01,203 - INFO - evaluating now!
2024-03-08 22:25:12,879 - INFO - Epoch [72/300] (48837) train_loss: 27.0376, val_loss: 27.1630, lr: 0.000875, 148.57s
2024-03-08 22:27:30,250 - INFO - epoch complete!
2024-03-08 22:27:30,251 - INFO - evaluating now!
2024-03-08 22:27:41,956 - INFO - Epoch [73/300] (49506) train_loss: 27.1267, val_loss: 26.9197, lr: 0.000872, 149.08s
2024-03-08 22:29:59,035 - INFO - epoch complete!
2024-03-08 22:29:59,036 - INFO - evaluating now!
2024-03-08 22:30:10,719 - INFO - Epoch [74/300] (50175) train_loss: 26.9483, val_loss: 27.0704, lr: 0.000868, 148.76s
2024-03-08 22:32:27,488 - INFO - epoch complete!
2024-03-08 22:32:27,489 - INFO - evaluating now!
2024-03-08 22:32:39,164 - INFO - Epoch [75/300] (50844) train_loss: 26.9965, val_loss: 26.7241, lr: 0.000865, 148.44s
2024-03-08 22:32:39,281 - INFO - Saved model at 75
2024-03-08 22:32:39,283 - INFO - Val loss decrease from 26.9182 to 26.7241, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch75.tar
2024-03-08 22:34:56,029 - INFO - epoch complete!
2024-03-08 22:34:56,030 - INFO - evaluating now!
2024-03-08 22:35:07,610 - INFO - Epoch [76/300] (51513) train_loss: 26.8387, val_loss: 26.9593, lr: 0.000861, 148.33s
2024-03-08 22:37:23,663 - INFO - epoch complete!
2024-03-08 22:37:23,664 - INFO - evaluating now!
2024-03-08 22:37:35,238 - INFO - Epoch [77/300] (52182) train_loss: 26.9195, val_loss: 27.1782, lr: 0.000858, 147.63s
2024-03-08 22:39:51,281 - INFO - epoch complete!
2024-03-08 22:39:51,283 - INFO - evaluating now!
2024-03-08 22:40:02,860 - INFO - Epoch [78/300] (52851) train_loss: 26.8240, val_loss: 26.9016, lr: 0.000855, 147.62s
2024-03-08 22:42:19,724 - INFO - epoch complete!
2024-03-08 22:42:19,725 - INFO - evaluating now!
2024-03-08 22:42:31,450 - INFO - Epoch [79/300] (53520) train_loss: 26.6853, val_loss: 27.2776, lr: 0.000851, 148.59s
2024-03-08 22:44:48,573 - INFO - epoch complete!
2024-03-08 22:44:48,574 - INFO - evaluating now!
2024-03-08 22:45:00,235 - INFO - Epoch [80/300] (54189) train_loss: 26.7765, val_loss: 26.9622, lr: 0.000848, 148.78s
2024-03-08 22:47:17,235 - INFO - epoch complete!
2024-03-08 22:47:17,236 - INFO - evaluating now!
2024-03-08 22:47:28,963 - INFO - Epoch [81/300] (54858) train_loss: 26.6297, val_loss: 27.1670, lr: 0.000844, 148.73s
2024-03-08 22:49:45,861 - INFO - epoch complete!
2024-03-08 22:49:45,862 - INFO - evaluating now!
2024-03-08 22:49:57,551 - INFO - Epoch [82/300] (55527) train_loss: 26.6764, val_loss: 26.5716, lr: 0.000840, 148.59s
2024-03-08 22:49:57,670 - INFO - Saved model at 82
2024-03-08 22:49:57,671 - INFO - Val loss decrease from 26.7241 to 26.5716, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch82.tar
2024-03-08 22:52:15,057 - INFO - epoch complete!
2024-03-08 22:52:15,058 - INFO - evaluating now!
2024-03-08 22:52:26,827 - INFO - Epoch [83/300] (56196) train_loss: 26.7624, val_loss: 26.8868, lr: 0.000837, 149.16s
2024-03-08 22:54:43,821 - INFO - epoch complete!
2024-03-08 22:54:43,822 - INFO - evaluating now!
2024-03-08 22:54:55,518 - INFO - Epoch [84/300] (56865) train_loss: 26.6039, val_loss: 26.7584, lr: 0.000833, 148.69s
2024-03-08 22:57:12,576 - INFO - epoch complete!
2024-03-08 22:57:12,577 - INFO - evaluating now!
2024-03-08 22:57:24,266 - INFO - Epoch [85/300] (57534) train_loss: 26.5680, val_loss: 26.8904, lr: 0.000830, 148.75s
2024-03-08 22:59:41,347 - INFO - epoch complete!
2024-03-08 22:59:41,348 - INFO - evaluating now!
2024-03-08 22:59:53,087 - INFO - Epoch [86/300] (58203) train_loss: 26.6456, val_loss: 27.1474, lr: 0.000826, 148.82s
2024-03-08 23:02:08,413 - INFO - epoch complete!
2024-03-08 23:02:08,414 - INFO - evaluating now!
2024-03-08 23:02:19,950 - INFO - Epoch [87/300] (58872) train_loss: 26.4974, val_loss: 26.7011, lr: 0.000822, 146.86s
2024-03-08 23:04:36,351 - INFO - epoch complete!
2024-03-08 23:04:36,352 - INFO - evaluating now!
2024-03-08 23:04:47,999 - INFO - Epoch [88/300] (59541) train_loss: 26.6074, val_loss: 26.8320, lr: 0.000818, 148.05s
2024-03-08 23:07:04,683 - INFO - epoch complete!
2024-03-08 23:07:04,684 - INFO - evaluating now!
2024-03-08 23:07:16,263 - INFO - Epoch [89/300] (60210) train_loss: 26.3966, val_loss: 26.7776, lr: 0.000815, 148.26s
2024-03-08 23:09:32,638 - INFO - epoch complete!
2024-03-08 23:09:32,639 - INFO - evaluating now!
2024-03-08 23:09:44,243 - INFO - Epoch [90/300] (60879) train_loss: 26.3796, val_loss: 26.9505, lr: 0.000811, 147.98s
2024-03-08 23:12:00,634 - INFO - epoch complete!
2024-03-08 23:12:00,635 - INFO - evaluating now!
2024-03-08 23:12:12,348 - INFO - Epoch [91/300] (61548) train_loss: 26.3345, val_loss: 26.6536, lr: 0.000807, 148.10s
2024-03-08 23:14:28,854 - INFO - epoch complete!
2024-03-08 23:14:28,855 - INFO - evaluating now!
2024-03-08 23:14:40,491 - INFO - Epoch [92/300] (62217) train_loss: 26.4848, val_loss: 27.0359, lr: 0.000803, 148.14s
2024-03-08 23:16:57,043 - INFO - epoch complete!
2024-03-08 23:16:57,044 - INFO - evaluating now!
2024-03-08 23:17:08,638 - INFO - Epoch [93/300] (62886) train_loss: 26.3780, val_loss: 28.2100, lr: 0.000799, 148.15s
2024-03-08 23:19:25,685 - INFO - epoch complete!
2024-03-08 23:19:25,686 - INFO - evaluating now!
2024-03-08 23:19:37,362 - INFO - Epoch [94/300] (63555) train_loss: 26.4220, val_loss: 27.0401, lr: 0.000795, 148.72s
2024-03-08 23:21:54,285 - INFO - epoch complete!
2024-03-08 23:21:54,286 - INFO - evaluating now!
2024-03-08 23:22:06,039 - INFO - Epoch [95/300] (64224) train_loss: 26.2842, val_loss: 26.7453, lr: 0.000791, 148.68s
2024-03-08 23:24:22,821 - INFO - epoch complete!
2024-03-08 23:24:22,822 - INFO - evaluating now!
2024-03-08 23:24:34,556 - INFO - Epoch [96/300] (64893) train_loss: 26.2584, val_loss: 26.4705, lr: 0.000787, 148.52s
2024-03-08 23:24:34,674 - INFO - Saved model at 96
2024-03-08 23:24:34,675 - INFO - Val loss decrease from 26.5716 to 26.4705, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch96.tar
2024-03-08 23:26:51,664 - INFO - epoch complete!
2024-03-08 23:26:51,665 - INFO - evaluating now!
2024-03-08 23:27:03,277 - INFO - Epoch [97/300] (65562) train_loss: 26.3743, val_loss: 26.4340, lr: 0.000783, 148.60s
2024-03-08 23:27:03,391 - INFO - Saved model at 97
2024-03-08 23:27:03,392 - INFO - Val loss decrease from 26.4705 to 26.4340, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch97.tar
2024-03-08 23:29:20,736 - INFO - epoch complete!
2024-03-08 23:29:20,737 - INFO - evaluating now!
2024-03-08 23:29:32,511 - INFO - Epoch [98/300] (66231) train_loss: 26.3059, val_loss: 26.2153, lr: 0.000779, 149.12s
2024-03-08 23:29:32,629 - INFO - Saved model at 98
2024-03-08 23:29:32,630 - INFO - Val loss decrease from 26.4340 to 26.2153, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch98.tar
2024-03-08 23:31:50,267 - INFO - epoch complete!
2024-03-08 23:31:50,268 - INFO - evaluating now!
2024-03-08 23:32:01,946 - INFO - Epoch [99/300] (66900) train_loss: 26.1203, val_loss: 26.1036, lr: 0.000775, 149.32s
2024-03-08 23:32:02,063 - INFO - Saved model at 99
2024-03-08 23:32:02,064 - INFO - Val loss decrease from 26.2153 to 26.1036, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch99.tar
2024-03-08 23:34:19,539 - INFO - epoch complete!
2024-03-08 23:34:19,540 - INFO - evaluating now!
2024-03-08 23:34:31,248 - INFO - Epoch [100/300] (67569) train_loss: 26.0415, val_loss: 26.5681, lr: 0.000771, 149.18s
2024-03-08 23:36:48,385 - INFO - epoch complete!
2024-03-08 23:36:48,387 - INFO - evaluating now!
2024-03-08 23:37:00,088 - INFO - Epoch [101/300] (68238) train_loss: 26.1315, val_loss: 26.3739, lr: 0.000767, 148.84s
2024-03-08 23:39:16,668 - INFO - epoch complete!
2024-03-08 23:39:16,669 - INFO - evaluating now!
2024-03-08 23:39:28,291 - INFO - Epoch [102/300] (68907) train_loss: 26.1125, val_loss: 27.1267, lr: 0.000763, 148.20s
2024-03-08 23:41:45,164 - INFO - epoch complete!
2024-03-08 23:41:45,165 - INFO - evaluating now!
2024-03-08 23:41:56,796 - INFO - Epoch [103/300] (69576) train_loss: 26.0685, val_loss: 26.3005, lr: 0.000758, 148.50s
2024-03-08 23:44:13,145 - INFO - epoch complete!
2024-03-08 23:44:13,146 - INFO - evaluating now!
2024-03-08 23:44:24,757 - INFO - Epoch [104/300] (70245) train_loss: 26.0865, val_loss: 26.1260, lr: 0.000754, 147.96s
2024-03-08 23:46:41,453 - INFO - epoch complete!
2024-03-08 23:46:41,454 - INFO - evaluating now!
2024-03-08 23:46:53,191 - INFO - Epoch [105/300] (70914) train_loss: 26.0623, val_loss: 26.0929, lr: 0.000750, 148.43s
2024-03-08 23:46:53,309 - INFO - Saved model at 105
2024-03-08 23:46:53,310 - INFO - Val loss decrease from 26.1036 to 26.0929, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch105.tar
2024-03-08 23:49:10,207 - INFO - epoch complete!
2024-03-08 23:49:10,208 - INFO - evaluating now!
2024-03-08 23:49:21,895 - INFO - Epoch [106/300] (71583) train_loss: 25.9747, val_loss: 26.2123, lr: 0.000746, 148.58s
2024-03-08 23:51:38,806 - INFO - epoch complete!
2024-03-08 23:51:38,807 - INFO - evaluating now!
2024-03-08 23:51:50,562 - INFO - Epoch [107/300] (72252) train_loss: 25.8910, val_loss: 26.4181, lr: 0.000742, 148.67s
2024-03-08 23:54:07,829 - INFO - epoch complete!
2024-03-08 23:54:07,830 - INFO - evaluating now!
2024-03-08 23:54:19,543 - INFO - Epoch [108/300] (72921) train_loss: 25.8746, val_loss: 26.2656, lr: 0.000737, 148.98s
2024-03-08 23:56:36,402 - INFO - epoch complete!
2024-03-08 23:56:36,403 - INFO - evaluating now!
2024-03-08 23:56:48,050 - INFO - Epoch [109/300] (73590) train_loss: 25.9825, val_loss: 26.3659, lr: 0.000733, 148.51s
2024-03-08 23:59:05,391 - INFO - epoch complete!
2024-03-08 23:59:05,392 - INFO - evaluating now!
2024-03-08 23:59:17,117 - INFO - Epoch [110/300] (74259) train_loss: 25.8983, val_loss: 26.5256, lr: 0.000729, 149.07s
2024-03-09 00:01:33,844 - INFO - epoch complete!
2024-03-09 00:01:33,845 - INFO - evaluating now!
2024-03-09 00:01:45,495 - INFO - Epoch [111/300] (74928) train_loss: 25.8649, val_loss: 25.9265, lr: 0.000724, 148.38s
2024-03-09 00:01:45,607 - INFO - Saved model at 111
2024-03-09 00:01:45,608 - INFO - Val loss decrease from 26.0929 to 25.9265, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch111.tar
2024-03-09 00:04:02,553 - INFO - epoch complete!
2024-03-09 00:04:02,554 - INFO - evaluating now!
2024-03-09 00:04:14,255 - INFO - Epoch [112/300] (75597) train_loss: 25.7831, val_loss: 27.9704, lr: 0.000720, 148.65s
2024-03-09 00:06:31,568 - INFO - epoch complete!
2024-03-09 00:06:31,569 - INFO - evaluating now!
2024-03-09 00:06:43,313 - INFO - Epoch [113/300] (76266) train_loss: 25.7975, val_loss: 26.2386, lr: 0.000716, 149.06s
2024-03-09 00:09:00,581 - INFO - epoch complete!
2024-03-09 00:09:00,582 - INFO - evaluating now!
2024-03-09 00:09:12,274 - INFO - Epoch [114/300] (76935) train_loss: 25.7778, val_loss: 25.7962, lr: 0.000711, 148.96s
2024-03-09 00:09:12,395 - INFO - Saved model at 114
2024-03-09 00:09:12,396 - INFO - Val loss decrease from 25.9265 to 25.7962, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch114.tar
2024-03-09 00:11:29,006 - INFO - epoch complete!
2024-03-09 00:11:29,007 - INFO - evaluating now!
2024-03-09 00:11:40,700 - INFO - Epoch [115/300] (77604) train_loss: 25.8057, val_loss: 26.8981, lr: 0.000707, 148.30s
2024-03-09 00:13:57,969 - INFO - epoch complete!
2024-03-09 00:13:57,970 - INFO - evaluating now!
2024-03-09 00:14:09,684 - INFO - Epoch [116/300] (78273) train_loss: 25.5764, val_loss: 26.1689, lr: 0.000702, 148.98s
2024-03-09 00:16:27,044 - INFO - epoch complete!
2024-03-09 00:16:27,045 - INFO - evaluating now!
2024-03-09 00:16:38,787 - INFO - Epoch [117/300] (78942) train_loss: 25.8179, val_loss: 26.3007, lr: 0.000698, 149.10s
2024-03-09 00:18:56,078 - INFO - epoch complete!
2024-03-09 00:18:56,079 - INFO - evaluating now!
2024-03-09 00:19:07,793 - INFO - Epoch [118/300] (79611) train_loss: 25.7315, val_loss: 26.0176, lr: 0.000694, 149.01s
2024-03-09 00:21:24,859 - INFO - epoch complete!
2024-03-09 00:21:24,860 - INFO - evaluating now!
2024-03-09 00:21:36,557 - INFO - Epoch [119/300] (80280) train_loss: 25.6951, val_loss: 26.3011, lr: 0.000689, 148.76s
2024-03-09 00:23:53,201 - INFO - epoch complete!
2024-03-09 00:23:53,202 - INFO - evaluating now!
2024-03-09 00:24:04,867 - INFO - Epoch [120/300] (80949) train_loss: 25.6547, val_loss: 26.1991, lr: 0.000685, 148.31s
2024-03-09 00:26:21,824 - INFO - epoch complete!
2024-03-09 00:26:21,824 - INFO - evaluating now!
2024-03-09 00:26:33,487 - INFO - Epoch [121/300] (81618) train_loss: 25.6555, val_loss: 26.4235, lr: 0.000680, 148.62s
2024-03-09 00:28:50,441 - INFO - epoch complete!
2024-03-09 00:28:50,442 - INFO - evaluating now!
2024-03-09 00:29:02,123 - INFO - Epoch [122/300] (82287) train_loss: 25.6745, val_loss: 26.2989, lr: 0.000676, 148.64s
2024-03-09 00:31:19,767 - INFO - epoch complete!
2024-03-09 00:31:19,768 - INFO - evaluating now!
2024-03-09 00:31:31,540 - INFO - Epoch [123/300] (82956) train_loss: 25.5501, val_loss: 26.2628, lr: 0.000671, 149.42s
2024-03-09 00:33:49,437 - INFO - epoch complete!
2024-03-09 00:33:49,438 - INFO - evaluating now!
2024-03-09 00:34:01,171 - INFO - Epoch [124/300] (83625) train_loss: 25.5840, val_loss: 26.1929, lr: 0.000666, 149.63s
2024-03-09 00:36:18,373 - INFO - epoch complete!
2024-03-09 00:36:18,374 - INFO - evaluating now!
2024-03-09 00:36:30,004 - INFO - Epoch [125/300] (84294) train_loss: 25.4871, val_loss: 25.9067, lr: 0.000662, 148.83s
2024-03-09 00:38:46,555 - INFO - epoch complete!
2024-03-09 00:38:46,556 - INFO - evaluating now!
2024-03-09 00:38:58,166 - INFO - Epoch [126/300] (84963) train_loss: 25.4545, val_loss: 26.0837, lr: 0.000657, 148.16s
2024-03-09 00:41:14,659 - INFO - epoch complete!
2024-03-09 00:41:14,660 - INFO - evaluating now!
2024-03-09 00:41:26,292 - INFO - Epoch [127/300] (85632) train_loss: 25.4658, val_loss: 26.3166, lr: 0.000653, 148.12s
2024-03-09 00:43:43,537 - INFO - epoch complete!
2024-03-09 00:43:43,538 - INFO - evaluating now!
2024-03-09 00:43:55,271 - INFO - Epoch [128/300] (86301) train_loss: 25.2938, val_loss: 26.1142, lr: 0.000648, 148.98s
2024-03-09 00:46:11,821 - INFO - epoch complete!
2024-03-09 00:46:11,822 - INFO - evaluating now!
2024-03-09 00:46:23,440 - INFO - Epoch [129/300] (86970) train_loss: 25.4939, val_loss: 26.2793, lr: 0.000644, 148.17s
2024-03-09 00:48:39,653 - INFO - epoch complete!
2024-03-09 00:48:39,654 - INFO - evaluating now!
2024-03-09 00:48:51,297 - INFO - Epoch [130/300] (87639) train_loss: 25.3778, val_loss: 26.4516, lr: 0.000639, 147.86s
2024-03-09 00:51:07,827 - INFO - epoch complete!
2024-03-09 00:51:07,828 - INFO - evaluating now!
2024-03-09 00:51:19,519 - INFO - Epoch [131/300] (88308) train_loss: 25.3056, val_loss: 26.2093, lr: 0.000634, 148.22s
2024-03-09 00:53:36,115 - INFO - epoch complete!
2024-03-09 00:53:36,116 - INFO - evaluating now!
2024-03-09 00:53:47,775 - INFO - Epoch [132/300] (88977) train_loss: 25.2919, val_loss: 25.8071, lr: 0.000630, 148.25s
2024-03-09 00:56:04,425 - INFO - epoch complete!
2024-03-09 00:56:04,426 - INFO - evaluating now!
2024-03-09 00:56:16,056 - INFO - Epoch [133/300] (89646) train_loss: 25.3189, val_loss: 26.9370, lr: 0.000625, 148.28s
2024-03-09 00:58:33,153 - INFO - epoch complete!
2024-03-09 00:58:33,154 - INFO - evaluating now!
2024-03-09 00:58:44,840 - INFO - Epoch [134/300] (90315) train_loss: 25.3380, val_loss: 26.0827, lr: 0.000620, 148.78s
2024-03-09 01:01:02,275 - INFO - epoch complete!
2024-03-09 01:01:02,277 - INFO - evaluating now!
2024-03-09 01:01:14,012 - INFO - Epoch [135/300] (90984) train_loss: 25.1882, val_loss: 25.9730, lr: 0.000616, 149.17s
2024-03-09 01:03:31,492 - INFO - epoch complete!
2024-03-09 01:03:31,493 - INFO - evaluating now!
2024-03-09 01:03:43,228 - INFO - Epoch [136/300] (91653) train_loss: 25.2137, val_loss: 26.0708, lr: 0.000611, 149.22s
2024-03-09 01:06:00,375 - INFO - epoch complete!
2024-03-09 01:06:00,376 - INFO - evaluating now!
2024-03-09 01:06:12,100 - INFO - Epoch [137/300] (92322) train_loss: 25.0924, val_loss: 26.0722, lr: 0.000606, 148.87s
2024-03-09 01:08:29,779 - INFO - epoch complete!
2024-03-09 01:08:29,780 - INFO - evaluating now!
2024-03-09 01:08:41,526 - INFO - Epoch [138/300] (92991) train_loss: 25.0443, val_loss: 26.2123, lr: 0.000602, 149.43s
2024-03-09 01:10:58,811 - INFO - epoch complete!
2024-03-09 01:10:58,812 - INFO - evaluating now!
2024-03-09 01:11:10,533 - INFO - Epoch [139/300] (93660) train_loss: 25.1512, val_loss: 26.2465, lr: 0.000597, 149.01s
2024-03-09 01:13:27,296 - INFO - epoch complete!
2024-03-09 01:13:27,297 - INFO - evaluating now!
2024-03-09 01:13:38,887 - INFO - Epoch [140/300] (94329) train_loss: 25.0350, val_loss: 25.8755, lr: 0.000592, 148.35s
2024-03-09 01:15:55,066 - INFO - epoch complete!
2024-03-09 01:15:55,067 - INFO - evaluating now!
2024-03-09 01:16:06,633 - INFO - Epoch [141/300] (94998) train_loss: 25.0739, val_loss: 26.6154, lr: 0.000588, 147.75s
2024-03-09 01:18:23,023 - INFO - epoch complete!
2024-03-09 01:18:23,024 - INFO - evaluating now!
2024-03-09 01:18:34,667 - INFO - Epoch [142/300] (95667) train_loss: 25.0169, val_loss: 25.9193, lr: 0.000583, 148.03s
2024-03-09 01:20:52,415 - INFO - epoch complete!
2024-03-09 01:20:52,416 - INFO - evaluating now!
2024-03-09 01:21:04,146 - INFO - Epoch [143/300] (96336) train_loss: 24.8876, val_loss: 26.1486, lr: 0.000578, 149.48s
2024-03-09 01:23:20,764 - INFO - epoch complete!
2024-03-09 01:23:20,765 - INFO - evaluating now!
2024-03-09 01:23:32,253 - INFO - Epoch [144/300] (97005) train_loss: 24.9678, val_loss: 26.0363, lr: 0.000574, 148.11s
2024-03-09 01:25:47,824 - INFO - epoch complete!
2024-03-09 01:25:47,826 - INFO - evaluating now!
2024-03-09 01:25:59,363 - INFO - Epoch [145/300] (97674) train_loss: 24.8088, val_loss: 26.5675, lr: 0.000569, 147.11s
2024-03-09 01:28:14,417 - INFO - epoch complete!
2024-03-09 01:28:14,418 - INFO - evaluating now!
2024-03-09 01:28:25,968 - INFO - Epoch [146/300] (98343) train_loss: 24.9034, val_loss: 25.8606, lr: 0.000564, 146.60s
2024-03-09 01:30:41,104 - INFO - epoch complete!
2024-03-09 01:30:41,105 - INFO - evaluating now!
2024-03-09 01:30:52,658 - INFO - Epoch [147/300] (99012) train_loss: 24.8465, val_loss: 26.3066, lr: 0.000559, 146.69s
2024-03-09 01:33:08,422 - INFO - epoch complete!
2024-03-09 01:33:08,423 - INFO - evaluating now!
2024-03-09 01:33:20,029 - INFO - Epoch [148/300] (99681) train_loss: 24.7771, val_loss: 26.3598, lr: 0.000555, 147.37s
2024-03-09 01:35:36,178 - INFO - epoch complete!
2024-03-09 01:35:36,179 - INFO - evaluating now!
2024-03-09 01:35:47,839 - INFO - Epoch [149/300] (100350) train_loss: 24.7326, val_loss: 25.9474, lr: 0.000550, 147.81s
2024-03-09 01:38:04,150 - INFO - epoch complete!
2024-03-09 01:38:04,151 - INFO - evaluating now!
2024-03-09 01:38:15,717 - INFO - Epoch [150/300] (101019) train_loss: 24.6542, val_loss: 25.9896, lr: 0.000545, 147.88s
2024-03-09 01:40:31,992 - INFO - epoch complete!
2024-03-09 01:40:31,993 - INFO - evaluating now!
2024-03-09 01:40:43,630 - INFO - Epoch [151/300] (101688) train_loss: 24.6093, val_loss: 26.2425, lr: 0.000541, 147.91s
2024-03-09 01:43:00,058 - INFO - epoch complete!
2024-03-09 01:43:00,059 - INFO - evaluating now!
2024-03-09 01:43:11,623 - INFO - Epoch [152/300] (102357) train_loss: 24.6159, val_loss: 25.9196, lr: 0.000536, 147.99s
2024-03-09 01:45:27,808 - INFO - epoch complete!
2024-03-09 01:45:27,809 - INFO - evaluating now!
2024-03-09 01:45:39,406 - INFO - Epoch [153/300] (103026) train_loss: 24.5905, val_loss: 25.7638, lr: 0.000531, 147.78s
2024-03-09 01:45:39,520 - INFO - Saved model at 153
2024-03-09 01:45:39,521 - INFO - Val loss decrease from 25.7962 to 25.7638, saving to ./libcity/cache/60140/model_cache/PDFormer_PeMS08_epoch153.tar
2024-03-09 01:47:56,317 - INFO - epoch complete!
2024-03-09 01:47:56,318 - INFO - evaluating now!
2024-03-09 01:48:08,070 - INFO - Epoch [154/300] (103695) train_loss: 24.5390, val_loss: 26.6473, lr: 0.000526, 148.55s
2024-03-09 01:50:25,392 - INFO - epoch complete!
2024-03-09 01:50:25,393 - INFO - evaluating now!
2024-03-09 01:50:37,127 - INFO - Epoch [155/300] (104364) train_loss: 24.5590, val_loss: 26.1955, lr: 0.000522, 149.06s
2024-03-09 01:52:53,803 - INFO - epoch complete!
2024-03-09 01:52:53,804 - INFO - evaluating now!
2024-03-09 01:53:05,468 - INFO - Epoch [156/300] (105033) train_loss: 24.4505, val_loss: 26.1380, lr: 0.000517, 148.34s
2024-03-09 01:55:21,610 - INFO - epoch complete!
2024-03-09 01:55:21,611 - INFO - evaluating now!
2024-03-09 01:55:33,217 - INFO - Epoch [157/300] (105702) train_loss: 24.4183, val_loss: 26.3489, lr: 0.000512, 147.75s
2024-03-09 01:57:50,257 - INFO - epoch complete!
2024-03-09 01:57:50,258 - INFO - evaluating now!
2024-03-09 01:58:01,922 - INFO - Epoch [158/300] (106371) train_loss: 24.3692, val_loss: 26.1531, lr: 0.000508, 148.70s
2024-03-09 02:00:18,489 - INFO - epoch complete!
2024-03-09 02:00:18,490 - INFO - evaluating now!
2024-03-09 02:00:30,125 - INFO - Epoch [159/300] (107040) train_loss: 24.3865, val_loss: 26.4463, lr: 0.000503, 148.20s
2024-03-09 02:02:46,881 - INFO - epoch complete!
2024-03-09 02:02:46,882 - INFO - evaluating now!
2024-03-09 02:02:58,546 - INFO - Epoch [160/300] (107709) train_loss: 24.2833, val_loss: 26.4339, lr: 0.000498, 148.42s
2024-03-09 02:05:15,351 - INFO - epoch complete!
2024-03-09 02:05:15,352 - INFO - evaluating now!
2024-03-09 02:05:26,915 - INFO - Epoch [161/300] (108378) train_loss: 24.3506, val_loss: 26.3644, lr: 0.000494, 148.37s
2024-03-09 02:07:43,254 - INFO - epoch complete!
2024-03-09 02:07:43,255 - INFO - evaluating now!
2024-03-09 02:07:54,864 - INFO - Epoch [162/300] (109047) train_loss: 24.2622, val_loss: 26.1857, lr: 0.000489, 147.95s
2024-03-09 02:10:11,362 - INFO - epoch complete!
2024-03-09 02:10:11,363 - INFO - evaluating now!
2024-03-09 02:10:23,024 - INFO - Epoch [163/300] (109716) train_loss: 24.1772, val_loss: 26.3246, lr: 0.000484, 148.16s
2024-03-09 02:12:39,567 - INFO - epoch complete!
2024-03-09 02:12:39,568 - INFO - evaluating now!
2024-03-09 02:12:51,237 - INFO - Epoch [164/300] (110385) train_loss: 24.2603, val_loss: 26.4074, lr: 0.000480, 148.21s
2024-03-09 02:15:08,465 - INFO - epoch complete!
2024-03-09 02:15:08,466 - INFO - evaluating now!
2024-03-09 02:15:20,165 - INFO - Epoch [165/300] (111054) train_loss: 24.2009, val_loss: 26.2353, lr: 0.000475, 148.93s
2024-03-09 02:17:37,277 - INFO - epoch complete!
2024-03-09 02:17:37,278 - INFO - evaluating now!
2024-03-09 02:17:49,056 - INFO - Epoch [166/300] (111723) train_loss: 24.0951, val_loss: 26.1084, lr: 0.000470, 148.89s
2024-03-09 02:20:06,788 - INFO - epoch complete!
2024-03-09 02:20:06,789 - INFO - evaluating now!
2024-03-09 02:20:18,562 - INFO - Epoch [167/300] (112392) train_loss: 24.0766, val_loss: 26.3886, lr: 0.000466, 149.51s
2024-03-09 02:22:37,292 - INFO - epoch complete!
2024-03-09 02:22:37,293 - INFO - evaluating now!
2024-03-09 02:22:49,121 - INFO - Epoch [168/300] (113061) train_loss: 24.0669, val_loss: 26.4736, lr: 0.000461, 150.56s
2024-03-09 02:25:07,291 - INFO - epoch complete!
2024-03-09 02:25:07,292 - INFO - evaluating now!
2024-03-09 02:25:19,039 - INFO - Epoch [169/300] (113730) train_loss: 24.0389, val_loss: 26.3866, lr: 0.000456, 149.92s
2024-03-09 02:27:36,393 - INFO - epoch complete!
2024-03-09 02:27:36,394 - INFO - evaluating now!
2024-03-09 02:27:48,158 - INFO - Epoch [170/300] (114399) train_loss: 23.9678, val_loss: 26.9561, lr: 0.000452, 149.12s
2024-03-09 02:30:05,620 - INFO - epoch complete!
2024-03-09 02:30:05,621 - INFO - evaluating now!
2024-03-09 02:30:17,365 - INFO - Epoch [171/300] (115068) train_loss: 23.9779, val_loss: 26.4795, lr: 0.000447, 149.21s
2024-03-09 02:32:34,186 - INFO - epoch complete!
2024-03-09 02:32:34,187 - INFO - evaluating now!
2024-03-09 02:32:45,893 - INFO - Epoch [172/300] (115737) train_loss: 23.8770, val_loss: 26.6105, lr: 0.000443, 148.53s
2024-03-09 02:35:02,596 - INFO - epoch complete!
2024-03-09 02:35:02,597 - INFO - evaluating now!
2024-03-09 02:35:14,223 - INFO - Epoch [173/300] (116406) train_loss: 23.9181, val_loss: 26.6416, lr: 0.000438, 148.33s
2024-03-09 02:37:31,019 - INFO - epoch complete!
2024-03-09 02:37:31,020 - INFO - evaluating now!
2024-03-09 02:37:42,704 - INFO - Epoch [174/300] (117075) train_loss: 23.8600, val_loss: 26.5737, lr: 0.000434, 148.48s
2024-03-09 02:39:59,790 - INFO - epoch complete!
2024-03-09 02:39:59,791 - INFO - evaluating now!
2024-03-09 02:40:11,497 - INFO - Epoch [175/300] (117744) train_loss: 23.7593, val_loss: 26.5468, lr: 0.000429, 148.79s
2024-03-09 02:42:28,095 - INFO - epoch complete!
2024-03-09 02:42:28,096 - INFO - evaluating now!
2024-03-09 02:42:39,709 - INFO - Epoch [176/300] (118413) train_loss: 23.8477, val_loss: 26.5563, lr: 0.000424, 148.21s
2024-03-09 02:44:55,939 - INFO - epoch complete!
2024-03-09 02:44:55,940 - INFO - evaluating now!
2024-03-09 02:45:07,601 - INFO - Epoch [177/300] (119082) train_loss: 23.7325, val_loss: 26.9379, lr: 0.000420, 147.89s
2024-03-09 02:47:23,911 - INFO - epoch complete!
2024-03-09 02:47:23,912 - INFO - evaluating now!
2024-03-09 02:47:35,472 - INFO - Epoch [178/300] (119751) train_loss: 23.6805, val_loss: 27.0126, lr: 0.000415, 147.87s
2024-03-09 02:49:51,666 - INFO - epoch complete!
2024-03-09 02:49:51,667 - INFO - evaluating now!
2024-03-09 02:50:03,217 - INFO - Epoch [179/300] (120420) train_loss: 23.7048, val_loss: 27.6839, lr: 0.000411, 147.74s
2024-03-09 02:52:19,468 - INFO - epoch complete!
2024-03-09 02:52:19,469 - INFO - evaluating now!
2024-03-09 02:52:31,019 - INFO - Epoch [180/300] (121089) train_loss: 23.6415, val_loss: 26.5789, lr: 0.000406, 147.80s
2024-03-09 02:54:46,765 - INFO - epoch complete!
2024-03-09 02:54:46,766 - INFO - evaluating now!
2024-03-09 02:54:58,304 - INFO - Epoch [181/300] (121758) train_loss: 23.6187, val_loss: 27.0651, lr: 0.000402, 147.28s
2024-03-09 02:57:14,987 - INFO - epoch complete!
2024-03-09 02:57:14,988 - INFO - evaluating now!
2024-03-09 02:57:26,578 - INFO - Epoch [182/300] (122427) train_loss: 23.5783, val_loss: 26.9663, lr: 0.000398, 148.27s
2024-03-09 02:59:43,912 - INFO - epoch complete!
2024-03-09 02:59:43,913 - INFO - evaluating now!
2024-03-09 02:59:55,673 - INFO - Epoch [183/300] (123096) train_loss: 23.5568, val_loss: 26.8679, lr: 0.000393, 149.09s
2024-03-09 03:02:12,873 - INFO - epoch complete!
2024-03-09 03:02:12,874 - INFO - evaluating now!
2024-03-09 03:02:24,638 - INFO - Epoch [184/300] (123765) train_loss: 23.5575, val_loss: 26.8436, lr: 0.000389, 148.96s
2024-03-09 03:04:41,675 - INFO - epoch complete!
2024-03-09 03:04:41,676 - INFO - evaluating now!
2024-03-09 03:04:53,462 - INFO - Epoch [185/300] (124434) train_loss: 23.4950, val_loss: 26.7878, lr: 0.000384, 148.82s
2024-03-09 03:07:10,530 - INFO - epoch complete!
2024-03-09 03:07:10,531 - INFO - evaluating now!
2024-03-09 03:07:22,234 - INFO - Epoch [186/300] (125103) train_loss: 23.4845, val_loss: 26.9167, lr: 0.000380, 148.77s
2024-03-09 03:09:40,093 - INFO - epoch complete!
2024-03-09 03:09:40,094 - INFO - evaluating now!
2024-03-09 03:09:51,865 - INFO - Epoch [187/300] (125772) train_loss: 23.4159, val_loss: 26.7338, lr: 0.000376, 149.63s
2024-03-09 03:12:08,589 - INFO - epoch complete!
2024-03-09 03:12:08,590 - INFO - evaluating now!
2024-03-09 03:12:20,164 - INFO - Epoch [188/300] (126441) train_loss: 23.4209, val_loss: 27.2846, lr: 0.000371, 148.30s
2024-03-09 03:14:36,949 - INFO - epoch complete!
2024-03-09 03:14:36,950 - INFO - evaluating now!
2024-03-09 03:14:48,533 - INFO - Epoch [189/300] (127110) train_loss: 23.4057, val_loss: 27.4574, lr: 0.000367, 148.37s
2024-03-09 03:17:05,552 - INFO - epoch complete!
2024-03-09 03:17:05,553 - INFO - evaluating now!
2024-03-09 03:17:17,122 - INFO - Epoch [190/300] (127779) train_loss: 23.3314, val_loss: 27.5360, lr: 0.000363, 148.59s
2024-03-09 03:19:33,871 - INFO - epoch complete!
2024-03-09 03:19:33,872 - INFO - evaluating now!
2024-03-09 03:19:45,526 - INFO - Epoch [191/300] (128448) train_loss: 23.3330, val_loss: 27.2601, lr: 0.000358, 148.40s
2024-03-09 03:22:03,219 - INFO - epoch complete!
2024-03-09 03:22:03,220 - INFO - evaluating now!
2024-03-09 03:22:14,935 - INFO - Epoch [192/300] (129117) train_loss: 23.3344, val_loss: 27.2048, lr: 0.000354, 149.41s
2024-03-09 03:24:31,956 - INFO - epoch complete!
2024-03-09 03:24:31,957 - INFO - evaluating now!
2024-03-09 03:24:43,533 - INFO - Epoch [193/300] (129786) train_loss: 23.2612, val_loss: 27.3240, lr: 0.000350, 148.60s
2024-03-09 03:27:00,361 - INFO - epoch complete!
2024-03-09 03:27:00,362 - INFO - evaluating now!
2024-03-09 03:27:11,980 - INFO - Epoch [194/300] (130455) train_loss: 23.2461, val_loss: 27.4823, lr: 0.000346, 148.45s
2024-03-09 03:29:28,884 - INFO - epoch complete!
2024-03-09 03:29:28,885 - INFO - evaluating now!
2024-03-09 03:29:40,436 - INFO - Epoch [195/300] (131124) train_loss: 23.2429, val_loss: 27.5652, lr: 0.000342, 148.46s
2024-03-09 03:31:56,877 - INFO - epoch complete!
2024-03-09 03:31:56,878 - INFO - evaluating now!
2024-03-09 03:32:08,425 - INFO - Epoch [196/300] (131793) train_loss: 23.2041, val_loss: 27.2106, lr: 0.000337, 147.99s
2024-03-09 03:34:25,080 - INFO - epoch complete!
2024-03-09 03:34:25,082 - INFO - evaluating now!
2024-03-09 03:34:36,616 - INFO - Epoch [197/300] (132462) train_loss: 23.1648, val_loss: 27.3084, lr: 0.000333, 148.19s
2024-03-09 03:36:52,989 - INFO - epoch complete!
2024-03-09 03:36:52,990 - INFO - evaluating now!
2024-03-09 03:37:04,534 - INFO - Epoch [198/300] (133131) train_loss: 23.1246, val_loss: 27.6903, lr: 0.000329, 147.92s
2024-03-09 03:39:20,910 - INFO - epoch complete!
2024-03-09 03:39:20,911 - INFO - evaluating now!
2024-03-09 03:39:32,487 - INFO - Epoch [199/300] (133800) train_loss: 23.0820, val_loss: 27.2866, lr: 0.000325, 147.95s
2024-03-09 03:41:48,932 - INFO - epoch complete!
2024-03-09 03:41:48,933 - INFO - evaluating now!
2024-03-09 03:42:00,476 - INFO - Epoch [200/300] (134469) train_loss: 23.1329, val_loss: 28.0873, lr: 0.000321, 147.99s
2024-03-09 03:44:16,423 - INFO - epoch complete!
2024-03-09 03:44:16,424 - INFO - evaluating now!
2024-03-09 03:44:27,978 - INFO - Epoch [201/300] (135138) train_loss: 23.0745, val_loss: 27.6624, lr: 0.000317, 147.50s
2024-03-09 03:46:44,048 - INFO - epoch complete!
2024-03-09 03:46:44,049 - INFO - evaluating now!
2024-03-09 03:46:55,621 - INFO - Epoch [202/300] (135807) train_loss: 23.0170, val_loss: 27.6900, lr: 0.000313, 147.64s
2024-03-09 03:49:11,549 - INFO - epoch complete!
2024-03-09 03:49:11,550 - INFO - evaluating now!
2024-03-09 03:49:23,092 - INFO - Epoch [203/300] (136476) train_loss: 23.0329, val_loss: 27.5581, lr: 0.000309, 147.47s
2024-03-09 03:49:23,093 - WARNING - Early stopping at epoch: 203
2024-03-09 03:49:23,093 - INFO - Trained totally 204 epochs, average train time is 136.900s, average eval time is 11.654s
2024-03-09 03:49:23,225 - INFO - Loaded model at 153
2024-03-09 03:49:23,228 - INFO - Saved model at ./libcity/cache/60140/model_cache/PDFormer_PeMS08.m
2024-03-09 03:49:23,346 - INFO - Start evaluating ...
2024-03-09 03:49:53,904 - INFO - Note that you select the average mode to evaluate!
2024-03-09 03:49:53,916 - INFO - Evaluate result is saved at ./libcity/cache/60140/evaluate_cache/2024_03_09_03_49_53_PDFormer_PeMS08_average.csv
2024-03-09 03:49:53,937 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   11.771497   inf  19.698074   11.787045     0.078117    19.599600
2   11.983819   inf  20.220581   12.000102     0.079170    20.121279
3   12.179120   inf  20.658867   12.196026     0.080274    20.559706
4   12.359928   inf  21.055693   12.377493     0.081393    20.960032
5   12.526725   inf  21.410706   12.544800     0.082456    21.316898
6   12.677849   inf  21.730120   12.696322     0.083479    21.637802
7   12.819958   inf  22.026907   12.838669     0.084486    21.934832
8   12.961417   inf  22.307886   12.980311     0.085560    22.216141
9   13.105720   inf  22.573195   13.124879     0.086532    22.481617
10  13.290516   inf  22.835035   13.310126     0.087678    22.744123
11  13.454629   inf  23.075897   13.474607     0.088732    22.985325
12  13.613893   inf  23.320385   13.634137     0.089860    23.230139
