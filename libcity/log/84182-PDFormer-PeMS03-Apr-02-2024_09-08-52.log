2024-04-02 09:08:52,870 - INFO - Log directory: ./libcity/log
2024-04-02 09:08:52,871 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS03, exp_id=84182
2024-04-02 09:08:52,871 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS03', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 1964, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 14, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 4, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 2, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS03'], 'geo_file': 'PeMS03', 'rel_file': 'PeMS03', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=2), 'exp_id': 84182}
2024-04-02 09:08:53,155 - INFO - Loaded file PeMS03.geo, num_nodes=358
2024-04-02 09:08:53,157 - INFO - set_weight_link_or_dist: link
2024-04-02 09:08:53,157 - INFO - init_weight_inf_or_zero: zero
2024-04-02 09:08:53,159 - INFO - Loaded file PeMS03.rel, shape=(358, 358)
2024-04-02 09:08:53,160 - INFO - Max adj_mx value = 1.0
2024-04-02 09:10:24,813 - INFO - Loading file PeMS03.dyna
2024-04-02 09:10:29,151 - INFO - Loaded file PeMS03.dyna, shape=(26208, 358, 1)
2024-04-02 09:10:29,220 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS03.npy
2024-04-02 09:10:29,220 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS03_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-04-02 09:10:49,619 - INFO - train	x: (15711, 12, 358, 9), y: (15711, 12, 358, 9), ind: (15711,)
2024-04-02 09:10:49,619 - INFO - eval	x: (5237, 12, 358, 9), y: (5237, 12, 358, 9), ind: (5237,)
2024-04-02 09:10:49,619 - INFO - test	x: (5237, 12, 358, 9), y: (5237, 12, 358, 9), ind: (5237,)
2024-04-02 09:10:50,938 - INFO - StandardScaler mean: 181.37526799238148, std: 144.4083626200602
2024-04-02 09:10:50,938 - INFO - NoneScaler
2024-04-02 09:10:54,743 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS03_14_3_16_5.npy
2024-04-02 09:10:54,746 - INFO - Use use_curriculum_learning!
2024-04-02 09:10:58,297 - INFO - Number of isolated points: 0
2024-04-02 09:10:58,327 - INFO - Number of isolated points: 0
2024-04-02 09:10:58,396 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-04-02 09:10:58,398 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,398 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,399 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,400 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:2	True
2024-04-02 09:10:58,401 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,402 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:2	True
2024-04-02 09:10:58,403 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,404 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:2	True
2024-04-02 09:10:58,405 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - end_conv1.bias	torch.Size([12])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - end_conv2.bias	torch.Size([1])	cuda:2	True
2024-04-02 09:10:58,406 - INFO - Total parameter numbers: 827741
2024-04-02 09:10:58,408 - INFO - You select `adamw` optimizer.
2024-04-02 09:10:58,409 - INFO - You select `cosinelr` lr_scheduler.
2024-04-02 09:10:58,409 - WARNING - Received none train loss func and will use the loss func defined in the model.module.
2024-04-02 09:10:58,410 - INFO - Number of isolated points: 0
2024-04-02 09:10:58,450 - INFO - Start training ...
2024-04-02 09:10:58,450 - INFO - num_batches:982
2024-04-02 09:10:58,530 - INFO - Training: task_level increase from 0 to 1
2024-04-02 09:10:58,530 - INFO - Current batches_seen is 0
2024-04-02 09:14:07,810 - INFO - epoch complete!
2024-04-02 09:14:07,811 - INFO - evaluating now!
2024-04-02 09:14:22,467 - INFO - Epoch [0/300] (982) train_loss: 238.0352, val_loss: 226.0444, lr: 0.000201, 204.02s
2024-04-02 09:14:22,508 - INFO - Saved model at 0
2024-04-02 09:14:22,508 - INFO - Val loss decrease from inf to 226.0444, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch0.tar
2024-04-02 09:17:32,480 - INFO - epoch complete!
2024-04-02 09:17:32,481 - INFO - evaluating now!
2024-04-02 09:17:47,138 - INFO - Epoch [1/300] (1964) train_loss: 50.8959, val_loss: 201.5804, lr: 0.000401, 204.63s
2024-04-02 09:17:47,179 - INFO - Saved model at 1
2024-04-02 09:17:47,179 - INFO - Val loss decrease from 226.0444 to 201.5804, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch1.tar
2024-04-02 09:17:47,221 - INFO - Training: task_level increase from 1 to 2
2024-04-02 09:17:47,221 - INFO - Current batches_seen is 1964
2024-04-02 09:20:57,112 - INFO - epoch complete!
2024-04-02 09:20:57,113 - INFO - evaluating now!
2024-04-02 09:21:11,768 - INFO - Epoch [2/300] (2946) train_loss: 38.9624, val_loss: 163.4253, lr: 0.000600, 204.59s
2024-04-02 09:21:11,807 - INFO - Saved model at 2
2024-04-02 09:21:11,808 - INFO - Val loss decrease from 201.5804 to 163.4253, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch2.tar
2024-04-02 09:24:21,506 - INFO - epoch complete!
2024-04-02 09:24:21,506 - INFO - evaluating now!
2024-04-02 09:24:36,152 - INFO - Epoch [3/300] (3928) train_loss: 30.5816, val_loss: 161.1826, lr: 0.000800, 204.34s
2024-04-02 09:24:36,191 - INFO - Saved model at 3
2024-04-02 09:24:36,191 - INFO - Val loss decrease from 163.4253 to 161.1826, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch3.tar
2024-04-02 09:24:36,232 - INFO - Training: task_level increase from 2 to 3
2024-04-02 09:24:36,232 - INFO - Current batches_seen is 3928
2024-04-02 09:27:46,117 - INFO - epoch complete!
2024-04-02 09:27:46,117 - INFO - evaluating now!
2024-04-02 09:28:00,760 - INFO - Epoch [4/300] (4910) train_loss: 31.6604, val_loss: 136.0323, lr: 0.000999, 204.57s
2024-04-02 09:28:00,799 - INFO - Saved model at 4
2024-04-02 09:28:00,799 - INFO - Val loss decrease from 161.1826 to 136.0323, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch4.tar
2024-04-02 09:31:10,766 - INFO - epoch complete!
2024-04-02 09:31:10,767 - INFO - evaluating now!
2024-04-02 09:31:25,413 - INFO - Epoch [5/300] (5892) train_loss: 29.4151, val_loss: 137.0417, lr: 0.000999, 204.61s
2024-04-02 09:31:25,455 - INFO - Training: task_level increase from 3 to 4
2024-04-02 09:31:25,455 - INFO - Current batches_seen is 5892
2024-04-02 09:34:35,369 - INFO - epoch complete!
2024-04-02 09:34:35,370 - INFO - evaluating now!
2024-04-02 09:34:50,013 - INFO - Epoch [6/300] (6874) train_loss: 30.0816, val_loss: 127.2438, lr: 0.000999, 204.60s
2024-04-02 09:34:50,052 - INFO - Saved model at 6
2024-04-02 09:34:50,052 - INFO - Val loss decrease from 136.0323 to 127.2438, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch6.tar
2024-04-02 09:37:59,993 - INFO - epoch complete!
2024-04-02 09:37:59,993 - INFO - evaluating now!
2024-04-02 09:38:14,648 - INFO - Epoch [7/300] (7856) train_loss: 29.0434, val_loss: 126.7334, lr: 0.000998, 204.60s
2024-04-02 09:38:14,687 - INFO - Saved model at 7
2024-04-02 09:38:14,688 - INFO - Val loss decrease from 127.2438 to 126.7334, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch7.tar
2024-04-02 09:38:14,729 - INFO - Training: task_level increase from 4 to 5
2024-04-02 09:38:14,729 - INFO - Current batches_seen is 7856
2024-04-02 09:41:24,738 - INFO - epoch complete!
2024-04-02 09:41:24,739 - INFO - evaluating now!
2024-04-02 09:41:39,383 - INFO - Epoch [8/300] (8838) train_loss: 30.0333, val_loss: 125.4801, lr: 0.000998, 204.69s
2024-04-02 09:41:39,422 - INFO - Saved model at 8
2024-04-02 09:41:39,422 - INFO - Val loss decrease from 126.7334 to 125.4801, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch8.tar
2024-04-02 09:44:49,765 - INFO - epoch complete!
2024-04-02 09:44:49,766 - INFO - evaluating now!
2024-04-02 09:45:04,415 - INFO - Epoch [9/300] (9820) train_loss: 29.0687, val_loss: 124.2617, lr: 0.000998, 204.99s
2024-04-02 09:45:04,454 - INFO - Saved model at 9
2024-04-02 09:45:04,454 - INFO - Val loss decrease from 125.4801 to 124.2617, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch9.tar
2024-04-02 09:45:04,496 - INFO - Training: task_level increase from 5 to 6
2024-04-02 09:45:04,496 - INFO - Current batches_seen is 9820
2024-04-02 09:48:14,627 - INFO - epoch complete!
2024-04-02 09:48:14,628 - INFO - evaluating now!
2024-04-02 09:48:29,269 - INFO - Epoch [10/300] (10802) train_loss: 29.7825, val_loss: 117.0491, lr: 0.000997, 204.81s
2024-04-02 09:48:29,308 - INFO - Saved model at 10
2024-04-02 09:48:29,308 - INFO - Val loss decrease from 124.2617 to 117.0491, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch10.tar
2024-04-02 09:51:39,402 - INFO - epoch complete!
2024-04-02 09:51:39,403 - INFO - evaluating now!
2024-04-02 09:51:54,044 - INFO - Epoch [11/300] (11784) train_loss: 29.1351, val_loss: 117.3869, lr: 0.000996, 204.74s
2024-04-02 09:51:54,085 - INFO - Training: task_level increase from 6 to 7
2024-04-02 09:51:54,085 - INFO - Current batches_seen is 11784
2024-04-02 09:55:04,101 - INFO - epoch complete!
2024-04-02 09:55:04,102 - INFO - evaluating now!
2024-04-02 09:55:18,747 - INFO - Epoch [12/300] (12766) train_loss: 30.0485, val_loss: 100.9335, lr: 0.000996, 204.70s
2024-04-02 09:55:18,786 - INFO - Saved model at 12
2024-04-02 09:55:18,786 - INFO - Val loss decrease from 117.0491 to 100.9335, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch12.tar
2024-04-02 09:58:28,659 - INFO - epoch complete!
2024-04-02 09:58:28,660 - INFO - evaluating now!
2024-04-02 09:58:43,311 - INFO - Epoch [13/300] (13748) train_loss: 29.2208, val_loss: 100.5214, lr: 0.000995, 204.52s
2024-04-02 09:58:43,350 - INFO - Saved model at 13
2024-04-02 09:58:43,350 - INFO - Val loss decrease from 100.9335 to 100.5214, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch13.tar
2024-04-02 09:58:43,391 - INFO - Training: task_level increase from 7 to 8
2024-04-02 09:58:43,391 - INFO - Current batches_seen is 13748
2024-04-02 10:01:53,528 - INFO - epoch complete!
2024-04-02 10:01:53,529 - INFO - evaluating now!
2024-04-02 10:02:08,191 - INFO - Epoch [14/300] (14730) train_loss: 30.0810, val_loss: 83.8725, lr: 0.000994, 204.84s
2024-04-02 10:02:08,230 - INFO - Saved model at 14
2024-04-02 10:02:08,230 - INFO - Val loss decrease from 100.5214 to 83.8725, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch14.tar
2024-04-02 10:05:18,449 - INFO - epoch complete!
2024-04-02 10:05:18,449 - INFO - evaluating now!
2024-04-02 10:05:33,101 - INFO - Epoch [15/300] (15712) train_loss: 29.4783, val_loss: 84.1380, lr: 0.000994, 204.87s
2024-04-02 10:05:33,143 - INFO - Training: task_level increase from 8 to 9
2024-04-02 10:05:33,143 - INFO - Current batches_seen is 15712
2024-04-02 10:08:43,066 - INFO - epoch complete!
2024-04-02 10:08:43,066 - INFO - evaluating now!
2024-04-02 10:08:57,716 - INFO - Epoch [16/300] (16694) train_loss: 30.3765, val_loss: 68.5669, lr: 0.000993, 204.61s
2024-04-02 10:08:57,755 - INFO - Saved model at 16
2024-04-02 10:08:57,755 - INFO - Val loss decrease from 83.8725 to 68.5669, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch16.tar
2024-04-02 10:12:07,948 - INFO - epoch complete!
2024-04-02 10:12:07,949 - INFO - evaluating now!
2024-04-02 10:12:22,597 - INFO - Epoch [17/300] (17676) train_loss: 29.6652, val_loss: 68.2643, lr: 0.000992, 204.84s
2024-04-02 10:12:22,636 - INFO - Saved model at 17
2024-04-02 10:12:22,636 - INFO - Val loss decrease from 68.5669 to 68.2643, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch17.tar
2024-04-02 10:12:22,678 - INFO - Training: task_level increase from 9 to 10
2024-04-02 10:12:22,678 - INFO - Current batches_seen is 17676
2024-04-02 10:15:32,719 - INFO - epoch complete!
2024-04-02 10:15:32,720 - INFO - evaluating now!
2024-04-02 10:15:47,364 - INFO - Epoch [18/300] (18658) train_loss: 30.3115, val_loss: 52.2464, lr: 0.000991, 204.73s
2024-04-02 10:15:47,403 - INFO - Saved model at 18
2024-04-02 10:15:47,404 - INFO - Val loss decrease from 68.2643 to 52.2464, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch18.tar
2024-04-02 10:18:57,544 - INFO - epoch complete!
2024-04-02 10:18:57,545 - INFO - evaluating now!
2024-04-02 10:19:12,203 - INFO - Epoch [19/300] (19640) train_loss: 29.7989, val_loss: 51.8239, lr: 0.000990, 204.80s
2024-04-02 10:19:12,242 - INFO - Saved model at 19
2024-04-02 10:19:12,242 - INFO - Val loss decrease from 52.2464 to 51.8239, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch19.tar
2024-04-02 10:19:12,283 - INFO - Training: task_level increase from 10 to 11
2024-04-02 10:19:12,284 - INFO - Current batches_seen is 19640
2024-04-02 10:22:26,319 - INFO - epoch complete!
2024-04-02 10:22:26,320 - INFO - evaluating now!
2024-04-02 10:22:40,850 - INFO - Epoch [20/300] (20622) train_loss: 30.3654, val_loss: 35.8936, lr: 0.000989, 208.61s
2024-04-02 10:22:40,884 - INFO - Saved model at 20
2024-04-02 10:22:40,884 - INFO - Val loss decrease from 51.8239 to 35.8936, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch20.tar
2024-04-02 10:25:42,354 - INFO - epoch complete!
2024-04-02 10:25:42,355 - INFO - evaluating now!
2024-04-02 10:25:56,875 - INFO - Epoch [21/300] (21604) train_loss: 29.7616, val_loss: 35.9388, lr: 0.000988, 195.99s
2024-04-02 10:25:56,915 - INFO - Training: task_level increase from 11 to 12
2024-04-02 10:25:56,915 - INFO - Current batches_seen is 21604
2024-04-02 10:29:07,102 - INFO - epoch complete!
2024-04-02 10:29:07,103 - INFO - evaluating now!
2024-04-02 10:29:21,624 - INFO - Epoch [22/300] (22586) train_loss: 30.1909, val_loss: 29.1596, lr: 0.000987, 204.75s
2024-04-02 10:29:21,658 - INFO - Saved model at 22
2024-04-02 10:29:21,658 - INFO - Val loss decrease from 35.8936 to 29.1596, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch22.tar
2024-04-02 10:32:23,079 - INFO - epoch complete!
2024-04-02 10:32:23,079 - INFO - evaluating now!
2024-04-02 10:32:37,595 - INFO - Epoch [23/300] (23568) train_loss: 29.7823, val_loss: 29.0977, lr: 0.000986, 195.94s
2024-04-02 10:32:37,629 - INFO - Saved model at 23
2024-04-02 10:32:37,630 - INFO - Val loss decrease from 29.1596 to 29.0977, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch23.tar
2024-04-02 10:35:38,865 - INFO - epoch complete!
2024-04-02 10:35:38,866 - INFO - evaluating now!
2024-04-02 10:35:53,378 - INFO - Epoch [24/300] (24550) train_loss: 29.5500, val_loss: 28.6861, lr: 0.000985, 195.75s
2024-04-02 10:35:53,412 - INFO - Saved model at 24
2024-04-02 10:35:53,412 - INFO - Val loss decrease from 29.0977 to 28.6861, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch24.tar
2024-04-02 10:38:54,730 - INFO - epoch complete!
2024-04-02 10:38:54,730 - INFO - evaluating now!
2024-04-02 10:39:09,246 - INFO - Epoch [25/300] (25532) train_loss: 29.3033, val_loss: 29.4648, lr: 0.000983, 195.83s
2024-04-02 10:42:10,634 - INFO - epoch complete!
2024-04-02 10:42:10,635 - INFO - evaluating now!
2024-04-02 10:42:25,147 - INFO - Epoch [26/300] (26514) train_loss: 29.1670, val_loss: 28.2908, lr: 0.000982, 195.90s
2024-04-02 10:42:25,181 - INFO - Saved model at 26
2024-04-02 10:42:25,181 - INFO - Val loss decrease from 28.6861 to 28.2908, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch26.tar
2024-04-02 10:45:26,434 - INFO - epoch complete!
2024-04-02 10:45:26,434 - INFO - evaluating now!
2024-04-02 10:45:40,944 - INFO - Epoch [27/300] (27496) train_loss: 28.9737, val_loss: 28.5823, lr: 0.000981, 195.76s
2024-04-02 10:48:42,243 - INFO - epoch complete!
2024-04-02 10:48:42,244 - INFO - evaluating now!
2024-04-02 10:48:56,761 - INFO - Epoch [28/300] (28478) train_loss: 28.8874, val_loss: 28.4040, lr: 0.000979, 195.82s
2024-04-02 10:51:58,084 - INFO - epoch complete!
2024-04-02 10:51:58,085 - INFO - evaluating now!
2024-04-02 10:52:12,607 - INFO - Epoch [29/300] (29460) train_loss: 28.7966, val_loss: 28.0840, lr: 0.000978, 195.85s
2024-04-02 10:52:12,641 - INFO - Saved model at 29
2024-04-02 10:52:12,641 - INFO - Val loss decrease from 28.2908 to 28.0840, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch29.tar
2024-04-02 10:55:14,308 - INFO - epoch complete!
2024-04-02 10:55:14,308 - INFO - evaluating now!
2024-04-02 10:55:28,824 - INFO - Epoch [30/300] (30442) train_loss: 28.6578, val_loss: 28.3461, lr: 0.000976, 196.18s
2024-04-02 10:58:37,367 - INFO - epoch complete!
2024-04-02 10:58:37,368 - INFO - evaluating now!
2024-04-02 10:58:51,896 - INFO - Epoch [31/300] (31424) train_loss: 28.6879, val_loss: 28.1996, lr: 0.000975, 203.07s
2024-04-02 11:01:58,048 - INFO - epoch complete!
2024-04-02 11:01:58,049 - INFO - evaluating now!
2024-04-02 11:02:12,645 - INFO - Epoch [32/300] (32406) train_loss: 28.3101, val_loss: 27.8006, lr: 0.000973, 200.75s
2024-04-02 11:02:12,680 - INFO - Saved model at 32
2024-04-02 11:02:12,680 - INFO - Val loss decrease from 28.0840 to 27.8006, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch32.tar
2024-04-02 11:05:14,267 - INFO - epoch complete!
2024-04-02 11:05:14,268 - INFO - evaluating now!
2024-04-02 11:05:28,811 - INFO - Epoch [33/300] (33388) train_loss: 28.4469, val_loss: 28.2723, lr: 0.000972, 196.13s
2024-04-02 11:08:30,340 - INFO - epoch complete!
2024-04-02 11:08:30,341 - INFO - evaluating now!
2024-04-02 11:08:44,870 - INFO - Epoch [34/300] (34370) train_loss: 28.2755, val_loss: 27.5496, lr: 0.000970, 196.06s
2024-04-02 11:08:44,904 - INFO - Saved model at 34
2024-04-02 11:08:44,904 - INFO - Val loss decrease from 27.8006 to 27.5496, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch34.tar
2024-04-02 11:11:47,475 - INFO - epoch complete!
2024-04-02 11:11:47,475 - INFO - evaluating now!
2024-04-02 11:12:02,134 - INFO - Epoch [35/300] (35352) train_loss: 28.1610, val_loss: 27.5270, lr: 0.000968, 197.23s
2024-04-02 11:12:02,172 - INFO - Saved model at 35
2024-04-02 11:12:02,173 - INFO - Val loss decrease from 27.5496 to 27.5270, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch35.tar
2024-04-02 11:15:12,159 - INFO - epoch complete!
2024-04-02 11:15:12,160 - INFO - evaluating now!
2024-04-02 11:15:26,810 - INFO - Epoch [36/300] (36334) train_loss: 28.0669, val_loss: 28.0385, lr: 0.000967, 204.64s
2024-04-02 11:18:36,910 - INFO - epoch complete!
2024-04-02 11:18:36,911 - INFO - evaluating now!
2024-04-02 11:18:51,568 - INFO - Epoch [37/300] (37316) train_loss: 28.1174, val_loss: 27.1769, lr: 0.000965, 204.76s
2024-04-02 11:18:51,607 - INFO - Saved model at 37
2024-04-02 11:18:51,608 - INFO - Val loss decrease from 27.5270 to 27.1769, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch37.tar
2024-04-02 11:22:01,916 - INFO - epoch complete!
2024-04-02 11:22:01,916 - INFO - evaluating now!
2024-04-02 11:22:16,574 - INFO - Epoch [38/300] (38298) train_loss: 27.9786, val_loss: 27.6739, lr: 0.000963, 204.97s
2024-04-02 11:25:26,834 - INFO - epoch complete!
2024-04-02 11:25:26,835 - INFO - evaluating now!
2024-04-02 11:25:41,491 - INFO - Epoch [39/300] (39280) train_loss: 27.9736, val_loss: 28.0705, lr: 0.000961, 204.92s
2024-04-02 11:28:51,685 - INFO - epoch complete!
2024-04-02 11:28:51,685 - INFO - evaluating now!
2024-04-02 11:29:06,349 - INFO - Epoch [40/300] (40262) train_loss: 27.9734, val_loss: 27.5803, lr: 0.000959, 204.86s
2024-04-02 11:32:16,430 - INFO - epoch complete!
2024-04-02 11:32:16,431 - INFO - evaluating now!
2024-04-02 11:32:31,087 - INFO - Epoch [41/300] (41244) train_loss: 27.9604, val_loss: 28.0776, lr: 0.000957, 204.74s
2024-04-02 11:35:41,125 - INFO - epoch complete!
2024-04-02 11:35:41,126 - INFO - evaluating now!
2024-04-02 11:35:55,772 - INFO - Epoch [42/300] (42226) train_loss: 27.9726, val_loss: 27.8184, lr: 0.000955, 204.68s
2024-04-02 11:39:05,881 - INFO - epoch complete!
2024-04-02 11:39:05,882 - INFO - evaluating now!
2024-04-02 11:39:20,530 - INFO - Epoch [43/300] (43208) train_loss: 27.8643, val_loss: 27.3549, lr: 0.000953, 204.76s
2024-04-02 11:42:31,714 - INFO - epoch complete!
2024-04-02 11:42:31,715 - INFO - evaluating now!
2024-04-02 11:42:46,252 - INFO - Epoch [44/300] (44190) train_loss: 27.7965, val_loss: 27.1968, lr: 0.000951, 205.72s
2024-04-02 11:45:47,759 - INFO - epoch complete!
2024-04-02 11:45:47,760 - INFO - evaluating now!
2024-04-02 11:46:02,289 - INFO - Epoch [45/300] (45172) train_loss: 27.7726, val_loss: 27.6032, lr: 0.000949, 196.04s
2024-04-02 11:49:03,866 - INFO - epoch complete!
2024-04-02 11:49:03,867 - INFO - evaluating now!
2024-04-02 11:49:18,391 - INFO - Epoch [46/300] (46154) train_loss: 27.6941, val_loss: 28.1732, lr: 0.000947, 196.10s
2024-04-02 11:52:19,887 - INFO - epoch complete!
2024-04-02 11:52:19,887 - INFO - evaluating now!
2024-04-02 11:52:34,412 - INFO - Epoch [47/300] (47136) train_loss: 27.7670, val_loss: 27.1880, lr: 0.000944, 196.02s
2024-04-02 11:55:35,895 - INFO - epoch complete!
2024-04-02 11:55:35,896 - INFO - evaluating now!
2024-04-02 11:55:50,404 - INFO - Epoch [48/300] (48118) train_loss: 27.6612, val_loss: 27.5232, lr: 0.000942, 195.99s
2024-04-02 11:58:51,700 - INFO - epoch complete!
2024-04-02 11:58:51,701 - INFO - evaluating now!
2024-04-02 11:59:06,220 - INFO - Epoch [49/300] (49100) train_loss: 27.6423, val_loss: 27.6570, lr: 0.000940, 195.82s
2024-04-02 12:02:07,651 - INFO - epoch complete!
2024-04-02 12:02:07,651 - INFO - evaluating now!
2024-04-02 12:02:22,163 - INFO - Epoch [50/300] (50082) train_loss: 27.4833, val_loss: 27.0521, lr: 0.000937, 195.94s
2024-04-02 12:02:22,197 - INFO - Saved model at 50
2024-04-02 12:02:22,197 - INFO - Val loss decrease from 27.1769 to 27.0521, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch50.tar
2024-04-02 12:05:23,487 - INFO - epoch complete!
2024-04-02 12:05:23,487 - INFO - evaluating now!
2024-04-02 12:05:38,000 - INFO - Epoch [51/300] (51064) train_loss: 27.5274, val_loss: 27.2981, lr: 0.000935, 195.80s
2024-04-02 12:08:39,445 - INFO - epoch complete!
2024-04-02 12:08:39,446 - INFO - evaluating now!
2024-04-02 12:08:53,969 - INFO - Epoch [52/300] (52046) train_loss: 27.5673, val_loss: 27.1777, lr: 0.000932, 195.97s
2024-04-02 12:12:05,368 - INFO - epoch complete!
2024-04-02 12:12:05,369 - INFO - evaluating now!
2024-04-02 12:12:19,927 - INFO - Epoch [53/300] (53028) train_loss: 27.5060, val_loss: 27.2155, lr: 0.000930, 205.96s
2024-04-02 12:15:21,555 - INFO - epoch complete!
2024-04-02 12:15:21,555 - INFO - evaluating now!
2024-04-02 12:15:36,101 - INFO - Epoch [54/300] (54010) train_loss: 27.4664, val_loss: 27.1834, lr: 0.000927, 196.17s
2024-04-02 12:18:37,631 - INFO - epoch complete!
2024-04-02 12:18:37,632 - INFO - evaluating now!
2024-04-02 12:18:52,180 - INFO - Epoch [55/300] (54992) train_loss: 27.4223, val_loss: 27.0728, lr: 0.000925, 196.08s
2024-04-02 12:21:53,723 - INFO - epoch complete!
2024-04-02 12:21:53,723 - INFO - evaluating now!
2024-04-02 12:22:08,426 - INFO - Epoch [56/300] (55974) train_loss: 27.4196, val_loss: 27.5888, lr: 0.000922, 196.25s
2024-04-02 12:25:18,220 - INFO - epoch complete!
2024-04-02 12:25:18,221 - INFO - evaluating now!
2024-04-02 12:25:32,881 - INFO - Epoch [57/300] (56956) train_loss: 27.3292, val_loss: 27.4158, lr: 0.000920, 204.45s
2024-04-02 12:28:42,727 - INFO - epoch complete!
2024-04-02 12:28:42,728 - INFO - evaluating now!
2024-04-02 12:28:57,385 - INFO - Epoch [58/300] (57938) train_loss: 27.4178, val_loss: 27.5171, lr: 0.000917, 204.50s
2024-04-02 12:32:07,525 - INFO - epoch complete!
2024-04-02 12:32:07,526 - INFO - evaluating now!
2024-04-02 12:32:22,188 - INFO - Epoch [59/300] (58920) train_loss: 27.3673, val_loss: 27.3887, lr: 0.000914, 204.80s
2024-04-02 12:35:32,429 - INFO - epoch complete!
2024-04-02 12:35:32,430 - INFO - evaluating now!
2024-04-02 12:35:47,086 - INFO - Epoch [60/300] (59902) train_loss: 27.2919, val_loss: 27.7000, lr: 0.000911, 204.90s
2024-04-02 12:39:00,236 - INFO - epoch complete!
2024-04-02 12:39:00,237 - INFO - evaluating now!
2024-04-02 12:39:14,785 - INFO - Epoch [61/300] (60884) train_loss: 27.3308, val_loss: 26.9231, lr: 0.000908, 207.70s
2024-04-02 12:39:14,819 - INFO - Saved model at 61
2024-04-02 12:39:14,819 - INFO - Val loss decrease from 27.0521 to 26.9231, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch61.tar
2024-04-02 12:42:16,235 - INFO - epoch complete!
2024-04-02 12:42:16,236 - INFO - evaluating now!
2024-04-02 12:42:30,762 - INFO - Epoch [62/300] (61866) train_loss: 27.2240, val_loss: 27.3815, lr: 0.000906, 195.94s
2024-04-02 12:45:32,285 - INFO - epoch complete!
2024-04-02 12:45:32,286 - INFO - evaluating now!
2024-04-02 12:45:46,812 - INFO - Epoch [63/300] (62848) train_loss: 27.2640, val_loss: 27.2617, lr: 0.000903, 196.05s
2024-04-02 12:48:48,403 - INFO - epoch complete!
2024-04-02 12:48:48,404 - INFO - evaluating now!
2024-04-02 12:49:02,944 - INFO - Epoch [64/300] (63830) train_loss: 27.1675, val_loss: 26.9819, lr: 0.000900, 196.13s
2024-04-02 12:52:04,526 - INFO - epoch complete!
2024-04-02 12:52:04,527 - INFO - evaluating now!
2024-04-02 12:52:19,055 - INFO - Epoch [65/300] (64812) train_loss: 27.1571, val_loss: 27.3435, lr: 0.000897, 196.11s
2024-04-02 12:55:20,788 - INFO - epoch complete!
2024-04-02 12:55:20,789 - INFO - evaluating now!
2024-04-02 12:55:35,314 - INFO - Epoch [66/300] (65794) train_loss: 27.2748, val_loss: 26.6852, lr: 0.000894, 196.26s
2024-04-02 12:55:35,348 - INFO - Saved model at 66
2024-04-02 12:55:35,348 - INFO - Val loss decrease from 26.9231 to 26.6852, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch66.tar
2024-04-02 12:58:36,973 - INFO - epoch complete!
2024-04-02 12:58:36,973 - INFO - evaluating now!
2024-04-02 12:58:51,494 - INFO - Epoch [67/300] (66776) train_loss: 27.0453, val_loss: 27.1824, lr: 0.000891, 196.15s
2024-04-02 13:01:53,053 - INFO - epoch complete!
2024-04-02 13:01:53,053 - INFO - evaluating now!
2024-04-02 13:02:07,584 - INFO - Epoch [68/300] (67758) train_loss: 27.0798, val_loss: 26.8762, lr: 0.000888, 196.09s
2024-04-02 13:05:09,267 - INFO - epoch complete!
2024-04-02 13:05:09,268 - INFO - evaluating now!
2024-04-02 13:05:23,795 - INFO - Epoch [69/300] (68740) train_loss: 27.0060, val_loss: 27.8435, lr: 0.000884, 196.21s
2024-04-02 13:08:25,290 - INFO - epoch complete!
2024-04-02 13:08:25,291 - INFO - evaluating now!
2024-04-02 13:08:39,820 - INFO - Epoch [70/300] (69722) train_loss: 27.0119, val_loss: 26.8197, lr: 0.000881, 196.02s
2024-04-02 13:11:41,600 - INFO - epoch complete!
2024-04-02 13:11:41,600 - INFO - evaluating now!
2024-04-02 13:11:56,126 - INFO - Epoch [71/300] (70704) train_loss: 26.9334, val_loss: 27.0463, lr: 0.000878, 196.31s
2024-04-02 13:14:57,899 - INFO - epoch complete!
2024-04-02 13:14:57,900 - INFO - evaluating now!
2024-04-02 13:15:12,432 - INFO - Epoch [72/300] (71686) train_loss: 27.0136, val_loss: 28.1437, lr: 0.000875, 196.31s
2024-04-02 13:18:14,288 - INFO - epoch complete!
2024-04-02 13:18:14,289 - INFO - evaluating now!
2024-04-02 13:18:28,815 - INFO - Epoch [73/300] (72668) train_loss: 26.8843, val_loss: 26.7732, lr: 0.000872, 196.38s
2024-04-02 13:21:30,906 - INFO - epoch complete!
2024-04-02 13:21:30,907 - INFO - evaluating now!
2024-04-02 13:21:45,436 - INFO - Epoch [74/300] (73650) train_loss: 26.9334, val_loss: 26.4877, lr: 0.000868, 196.62s
2024-04-02 13:21:45,470 - INFO - Saved model at 74
2024-04-02 13:21:45,471 - INFO - Val loss decrease from 26.6852 to 26.4877, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch74.tar
2024-04-02 13:24:47,630 - INFO - epoch complete!
2024-04-02 13:24:47,630 - INFO - evaluating now!
2024-04-02 13:25:02,164 - INFO - Epoch [75/300] (74632) train_loss: 26.8504, val_loss: 26.7875, lr: 0.000865, 196.69s
2024-04-02 13:28:04,371 - INFO - epoch complete!
2024-04-02 13:28:04,372 - INFO - evaluating now!
2024-04-02 13:28:18,902 - INFO - Epoch [76/300] (75614) train_loss: 26.8544, val_loss: 27.5557, lr: 0.000861, 196.74s
2024-04-02 13:31:21,023 - INFO - epoch complete!
2024-04-02 13:31:21,024 - INFO - evaluating now!
2024-04-02 13:31:35,545 - INFO - Epoch [77/300] (76596) train_loss: 26.8490, val_loss: 26.7855, lr: 0.000858, 196.64s
2024-04-02 13:34:37,650 - INFO - epoch complete!
2024-04-02 13:34:37,651 - INFO - evaluating now!
2024-04-02 13:34:52,177 - INFO - Epoch [78/300] (77578) train_loss: 26.8872, val_loss: 27.1910, lr: 0.000855, 196.63s
2024-04-02 13:37:54,284 - INFO - epoch complete!
2024-04-02 13:37:54,285 - INFO - evaluating now!
2024-04-02 13:38:08,818 - INFO - Epoch [79/300] (78560) train_loss: 26.7310, val_loss: 27.2555, lr: 0.000851, 196.64s
2024-04-02 13:41:11,002 - INFO - epoch complete!
2024-04-02 13:41:11,002 - INFO - evaluating now!
2024-04-02 13:41:25,527 - INFO - Epoch [80/300] (79542) train_loss: 26.7498, val_loss: 26.5308, lr: 0.000848, 196.71s
2024-04-02 13:44:33,562 - INFO - epoch complete!
2024-04-02 13:44:33,563 - INFO - evaluating now!
2024-04-02 13:44:48,087 - INFO - Epoch [81/300] (80524) train_loss: 26.7121, val_loss: 26.5370, lr: 0.000844, 202.56s
2024-04-02 13:47:49,881 - INFO - epoch complete!
2024-04-02 13:47:49,881 - INFO - evaluating now!
2024-04-02 13:48:04,414 - INFO - Epoch [82/300] (81506) train_loss: 26.6048, val_loss: 27.0253, lr: 0.000840, 196.33s
2024-04-02 13:51:05,996 - INFO - epoch complete!
2024-04-02 13:51:05,997 - INFO - evaluating now!
2024-04-02 13:51:20,519 - INFO - Epoch [83/300] (82488) train_loss: 26.6275, val_loss: 27.4158, lr: 0.000837, 196.10s
2024-04-02 13:54:22,122 - INFO - epoch complete!
2024-04-02 13:54:22,123 - INFO - evaluating now!
2024-04-02 13:54:36,641 - INFO - Epoch [84/300] (83470) train_loss: 26.5890, val_loss: 26.8618, lr: 0.000833, 196.12s
2024-04-02 13:57:38,115 - INFO - epoch complete!
2024-04-02 13:57:38,116 - INFO - evaluating now!
2024-04-02 13:57:52,629 - INFO - Epoch [85/300] (84452) train_loss: 26.5445, val_loss: 26.7081, lr: 0.000830, 195.99s
2024-04-02 14:01:04,328 - INFO - epoch complete!
2024-04-02 14:01:04,328 - INFO - evaluating now!
2024-04-02 14:01:18,872 - INFO - Epoch [86/300] (85434) train_loss: 26.4742, val_loss: 26.8632, lr: 0.000826, 206.24s
2024-04-02 14:04:20,410 - INFO - epoch complete!
2024-04-02 14:04:20,411 - INFO - evaluating now!
2024-04-02 14:04:34,941 - INFO - Epoch [87/300] (86416) train_loss: 26.4381, val_loss: 26.8422, lr: 0.000822, 196.07s
2024-04-02 14:07:36,432 - INFO - epoch complete!
2024-04-02 14:07:36,433 - INFO - evaluating now!
2024-04-02 14:07:50,961 - INFO - Epoch [88/300] (87398) train_loss: 26.4268, val_loss: 26.5568, lr: 0.000818, 196.02s
2024-04-02 14:10:52,288 - INFO - epoch complete!
2024-04-02 14:10:52,289 - INFO - evaluating now!
2024-04-02 14:11:06,830 - INFO - Epoch [89/300] (88380) train_loss: 26.4002, val_loss: 26.5505, lr: 0.000815, 195.87s
2024-04-02 14:14:08,159 - INFO - epoch complete!
2024-04-02 14:14:08,160 - INFO - evaluating now!
2024-04-02 14:14:22,695 - INFO - Epoch [90/300] (89362) train_loss: 26.3437, val_loss: 26.6972, lr: 0.000811, 195.86s
2024-04-02 14:17:24,212 - INFO - epoch complete!
2024-04-02 14:17:24,213 - INFO - evaluating now!
2024-04-02 14:17:38,750 - INFO - Epoch [91/300] (90344) train_loss: 26.2745, val_loss: 26.4097, lr: 0.000807, 196.05s
2024-04-02 14:17:38,785 - INFO - Saved model at 91
2024-04-02 14:17:38,786 - INFO - Val loss decrease from 26.4877 to 26.4097, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch91.tar
2024-04-02 14:20:40,315 - INFO - epoch complete!
2024-04-02 14:20:40,315 - INFO - evaluating now!
2024-04-02 14:20:54,854 - INFO - Epoch [92/300] (91326) train_loss: 26.2824, val_loss: 26.7073, lr: 0.000803, 196.07s
2024-04-02 14:23:56,554 - INFO - epoch complete!
2024-04-02 14:23:56,555 - INFO - evaluating now!
2024-04-02 14:24:11,106 - INFO - Epoch [93/300] (92308) train_loss: 26.1920, val_loss: 26.1124, lr: 0.000799, 196.25s
2024-04-02 14:24:11,140 - INFO - Saved model at 93
2024-04-02 14:24:11,141 - INFO - Val loss decrease from 26.4097 to 26.1124, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch93.tar
2024-04-02 14:27:13,647 - INFO - epoch complete!
2024-04-02 14:27:13,648 - INFO - evaluating now!
2024-04-02 14:27:28,180 - INFO - Epoch [94/300] (93290) train_loss: 26.1740, val_loss: 26.7372, lr: 0.000795, 197.04s
2024-04-02 14:30:48,023 - INFO - epoch complete!
2024-04-02 14:30:48,024 - INFO - evaluating now!
2024-04-02 14:31:02,579 - INFO - Epoch [95/300] (94272) train_loss: 26.1392, val_loss: 26.3419, lr: 0.000791, 214.40s
2024-04-02 14:34:05,042 - INFO - epoch complete!
2024-04-02 14:34:05,042 - INFO - evaluating now!
2024-04-02 14:34:19,647 - INFO - Epoch [96/300] (95254) train_loss: 26.0962, val_loss: 26.6147, lr: 0.000787, 197.07s
2024-04-02 14:37:29,419 - INFO - epoch complete!
2024-04-02 14:37:29,420 - INFO - evaluating now!
2024-04-02 14:37:45,306 - INFO - Epoch [97/300] (96236) train_loss: 26.1205, val_loss: 26.9743, lr: 0.000783, 205.66s
2024-04-02 14:41:04,733 - INFO - epoch complete!
2024-04-02 14:41:04,734 - INFO - evaluating now!
2024-04-02 14:41:19,676 - INFO - Epoch [98/300] (97218) train_loss: 26.0117, val_loss: 26.0911, lr: 0.000779, 214.37s
2024-04-02 14:41:19,714 - INFO - Saved model at 98
2024-04-02 14:41:19,714 - INFO - Val loss decrease from 26.1124 to 26.0911, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch98.tar
2024-04-02 14:44:26,871 - INFO - epoch complete!
2024-04-02 14:44:26,872 - INFO - evaluating now!
2024-04-02 14:44:41,476 - INFO - Epoch [99/300] (98200) train_loss: 25.8758, val_loss: 26.0369, lr: 0.000775, 201.76s
2024-04-02 14:44:41,512 - INFO - Saved model at 99
2024-04-02 14:44:41,512 - INFO - Val loss decrease from 26.0911 to 26.0369, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch99.tar
2024-04-02 14:47:56,265 - INFO - epoch complete!
2024-04-02 14:47:56,265 - INFO - evaluating now!
2024-04-02 14:48:10,906 - INFO - Epoch [100/300] (99182) train_loss: 25.9334, val_loss: 26.1946, lr: 0.000771, 209.39s
2024-04-02 14:51:26,271 - INFO - epoch complete!
2024-04-02 14:51:26,272 - INFO - evaluating now!
2024-04-02 14:51:40,959 - INFO - Epoch [101/300] (100164) train_loss: 25.9770, val_loss: 26.1668, lr: 0.000767, 210.05s
2024-04-02 14:54:59,582 - INFO - epoch complete!
2024-04-02 14:54:59,582 - INFO - evaluating now!
2024-04-02 14:55:14,145 - INFO - Epoch [102/300] (101146) train_loss: 25.8970, val_loss: 25.9827, lr: 0.000763, 213.18s
2024-04-02 14:55:14,181 - INFO - Saved model at 102
2024-04-02 14:55:14,181 - INFO - Val loss decrease from 26.0369 to 25.9827, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch102.tar
2024-04-02 14:58:23,801 - INFO - epoch complete!
2024-04-02 14:58:23,802 - INFO - evaluating now!
2024-04-02 14:58:38,538 - INFO - Epoch [103/300] (102128) train_loss: 25.8451, val_loss: 26.3339, lr: 0.000758, 204.36s
2024-04-02 15:01:45,899 - INFO - epoch complete!
2024-04-02 15:01:45,900 - INFO - evaluating now!
2024-04-02 15:02:00,721 - INFO - Epoch [104/300] (103110) train_loss: 25.8117, val_loss: 25.8858, lr: 0.000754, 202.18s
2024-04-02 15:02:00,759 - INFO - Saved model at 104
2024-04-02 15:02:00,759 - INFO - Val loss decrease from 25.9827 to 25.8858, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch104.tar
2024-04-02 15:05:09,029 - INFO - epoch complete!
2024-04-02 15:05:09,030 - INFO - evaluating now!
2024-04-02 15:05:23,807 - INFO - Epoch [105/300] (104092) train_loss: 25.7553, val_loss: 26.2303, lr: 0.000750, 203.05s
2024-04-02 15:08:38,774 - INFO - epoch complete!
2024-04-02 15:08:38,775 - INFO - evaluating now!
2024-04-02 15:08:53,549 - INFO - Epoch [106/300] (105074) train_loss: 25.7170, val_loss: 25.8862, lr: 0.000746, 209.74s
2024-04-02 15:12:08,783 - INFO - epoch complete!
2024-04-02 15:12:08,784 - INFO - evaluating now!
2024-04-02 15:12:23,597 - INFO - Epoch [107/300] (106056) train_loss: 25.6942, val_loss: 26.1057, lr: 0.000742, 210.05s
2024-04-02 15:15:30,677 - INFO - epoch complete!
2024-04-02 15:15:30,678 - INFO - evaluating now!
2024-04-02 15:15:45,459 - INFO - Epoch [108/300] (107038) train_loss: 25.6846, val_loss: 26.2784, lr: 0.000737, 201.86s
2024-04-02 15:18:52,853 - INFO - epoch complete!
2024-04-02 15:18:52,854 - INFO - evaluating now!
2024-04-02 15:19:07,650 - INFO - Epoch [109/300] (108020) train_loss: 25.6818, val_loss: 26.0493, lr: 0.000733, 202.19s
2024-04-02 15:22:15,664 - INFO - epoch complete!
2024-04-02 15:22:15,665 - INFO - evaluating now!
2024-04-02 15:22:30,470 - INFO - Epoch [110/300] (109002) train_loss: 25.6652, val_loss: 26.2170, lr: 0.000729, 202.82s
2024-04-02 15:25:38,018 - INFO - epoch complete!
2024-04-02 15:25:38,018 - INFO - evaluating now!
2024-04-02 15:25:52,783 - INFO - Epoch [111/300] (109984) train_loss: 25.6102, val_loss: 26.4239, lr: 0.000724, 202.31s
2024-04-02 15:29:06,689 - INFO - epoch complete!
2024-04-02 15:29:06,690 - INFO - evaluating now!
2024-04-02 15:29:21,435 - INFO - Epoch [112/300] (110966) train_loss: 25.6288, val_loss: 26.2268, lr: 0.000720, 208.65s
2024-04-02 15:32:31,125 - INFO - epoch complete!
2024-04-02 15:32:31,126 - INFO - evaluating now!
2024-04-02 15:32:45,764 - INFO - Epoch [113/300] (111948) train_loss: 25.4985, val_loss: 26.0913, lr: 0.000716, 204.33s
2024-04-02 15:36:01,882 - INFO - epoch complete!
2024-04-02 15:36:01,883 - INFO - evaluating now!
2024-04-02 15:36:16,643 - INFO - Epoch [114/300] (112930) train_loss: 25.5163, val_loss: 25.8270, lr: 0.000711, 210.88s
2024-04-02 15:36:16,680 - INFO - Saved model at 114
2024-04-02 15:36:16,681 - INFO - Val loss decrease from 25.8858 to 25.8270, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch114.tar
2024-04-02 15:39:31,561 - INFO - epoch complete!
2024-04-02 15:39:31,561 - INFO - evaluating now!
2024-04-02 15:39:46,358 - INFO - Epoch [115/300] (113912) train_loss: 25.4454, val_loss: 25.8854, lr: 0.000707, 209.68s
2024-04-02 15:42:56,216 - INFO - epoch complete!
2024-04-02 15:42:56,217 - INFO - evaluating now!
2024-04-02 15:43:11,299 - INFO - Epoch [116/300] (114894) train_loss: 25.4693, val_loss: 25.9543, lr: 0.000702, 204.94s
2024-04-02 15:46:24,032 - INFO - epoch complete!
2024-04-02 15:46:24,033 - INFO - evaluating now!
2024-04-02 15:46:38,871 - INFO - Epoch [117/300] (115876) train_loss: 25.3712, val_loss: 26.0114, lr: 0.000698, 207.57s
2024-04-02 15:49:53,029 - INFO - epoch complete!
2024-04-02 15:49:53,029 - INFO - evaluating now!
2024-04-02 15:50:07,891 - INFO - Epoch [118/300] (116858) train_loss: 25.3603, val_loss: 26.4742, lr: 0.000694, 209.02s
2024-04-02 15:53:20,337 - INFO - epoch complete!
2024-04-02 15:53:20,338 - INFO - evaluating now!
2024-04-02 15:53:34,989 - INFO - Epoch [119/300] (117840) train_loss: 25.3372, val_loss: 25.9131, lr: 0.000689, 207.10s
2024-04-02 15:56:44,023 - INFO - epoch complete!
2024-04-02 15:56:44,024 - INFO - evaluating now!
2024-04-02 15:56:58,787 - INFO - Epoch [120/300] (118822) train_loss: 25.3235, val_loss: 25.9182, lr: 0.000685, 203.80s
2024-04-02 16:00:04,304 - INFO - epoch complete!
2024-04-02 16:00:04,304 - INFO - evaluating now!
2024-04-02 16:00:19,026 - INFO - Epoch [121/300] (119804) train_loss: 25.3100, val_loss: 26.2278, lr: 0.000680, 200.24s
2024-04-02 16:03:27,872 - INFO - epoch complete!
2024-04-02 16:03:27,872 - INFO - evaluating now!
2024-04-02 16:03:42,615 - INFO - Epoch [122/300] (120786) train_loss: 25.2736, val_loss: 26.0275, lr: 0.000676, 203.59s
2024-04-02 16:06:56,062 - INFO - epoch complete!
2024-04-02 16:06:56,063 - INFO - evaluating now!
2024-04-02 16:07:10,825 - INFO - Epoch [123/300] (121768) train_loss: 25.2844, val_loss: 26.5018, lr: 0.000671, 208.21s
2024-04-02 16:10:25,549 - INFO - epoch complete!
2024-04-02 16:10:25,550 - INFO - evaluating now!
2024-04-02 16:10:40,193 - INFO - Epoch [124/300] (122750) train_loss: 25.2421, val_loss: 26.3992, lr: 0.000666, 209.37s
2024-04-02 16:13:48,835 - INFO - epoch complete!
2024-04-02 16:13:48,836 - INFO - evaluating now!
2024-04-02 16:14:03,575 - INFO - Epoch [125/300] (123732) train_loss: 25.3210, val_loss: 26.1249, lr: 0.000662, 203.38s
2024-04-02 16:17:09,774 - INFO - epoch complete!
2024-04-02 16:17:09,775 - INFO - evaluating now!
2024-04-02 16:17:24,586 - INFO - Epoch [126/300] (124714) train_loss: 25.1871, val_loss: 26.1184, lr: 0.000657, 201.01s
2024-04-02 16:20:34,816 - INFO - epoch complete!
2024-04-02 16:20:34,817 - INFO - evaluating now!
2024-04-02 16:20:49,433 - INFO - Epoch [127/300] (125696) train_loss: 25.1687, val_loss: 26.3360, lr: 0.000653, 204.85s
2024-04-02 16:24:00,896 - INFO - epoch complete!
2024-04-02 16:24:00,897 - INFO - evaluating now!
2024-04-02 16:24:15,913 - INFO - Epoch [128/300] (126678) train_loss: 25.1438, val_loss: 25.7654, lr: 0.000648, 206.48s
2024-04-02 16:24:15,950 - INFO - Saved model at 128
2024-04-02 16:24:15,950 - INFO - Val loss decrease from 25.8270 to 25.7654, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch128.tar
2024-04-02 16:27:25,030 - INFO - epoch complete!
2024-04-02 16:27:25,031 - INFO - evaluating now!
2024-04-02 16:27:39,635 - INFO - Epoch [129/300] (127660) train_loss: 25.0451, val_loss: 26.5585, lr: 0.000644, 203.68s
2024-04-02 16:30:43,822 - INFO - epoch complete!
2024-04-02 16:30:43,822 - INFO - evaluating now!
2024-04-02 16:30:58,307 - INFO - Epoch [130/300] (128642) train_loss: 25.0392, val_loss: 25.6564, lr: 0.000639, 198.67s
2024-04-02 16:30:58,342 - INFO - Saved model at 130
2024-04-02 16:30:58,342 - INFO - Val loss decrease from 25.7654 to 25.6564, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch130.tar
2024-04-02 16:34:16,336 - INFO - epoch complete!
2024-04-02 16:34:16,337 - INFO - evaluating now!
2024-04-02 16:34:32,496 - INFO - Epoch [131/300] (129624) train_loss: 25.0582, val_loss: 25.6976, lr: 0.000634, 214.15s
2024-04-02 16:37:49,638 - INFO - epoch complete!
2024-04-02 16:37:49,639 - INFO - evaluating now!
2024-04-02 16:38:05,384 - INFO - Epoch [132/300] (130606) train_loss: 24.9522, val_loss: 25.8357, lr: 0.000630, 212.89s
2024-04-02 16:41:28,065 - INFO - epoch complete!
2024-04-02 16:41:28,066 - INFO - evaluating now!
2024-04-02 16:41:44,097 - INFO - Epoch [133/300] (131588) train_loss: 24.9654, val_loss: 26.1085, lr: 0.000625, 218.71s
2024-04-02 16:45:01,516 - INFO - epoch complete!
2024-04-02 16:45:01,517 - INFO - evaluating now!
2024-04-02 16:45:16,180 - INFO - Epoch [134/300] (132570) train_loss: 24.9831, val_loss: 25.8655, lr: 0.000620, 212.08s
2024-04-02 16:48:38,439 - INFO - epoch complete!
2024-04-02 16:48:38,440 - INFO - evaluating now!
2024-04-02 16:48:53,700 - INFO - Epoch [135/300] (133552) train_loss: 24.9086, val_loss: 26.2700, lr: 0.000616, 217.52s
2024-04-02 16:52:06,293 - INFO - epoch complete!
2024-04-02 16:52:06,294 - INFO - evaluating now!
2024-04-02 16:52:21,177 - INFO - Epoch [136/300] (134534) train_loss: 24.9360, val_loss: 25.9401, lr: 0.000611, 207.48s
2024-04-02 16:55:25,556 - INFO - epoch complete!
2024-04-02 16:55:25,557 - INFO - evaluating now!
2024-04-02 16:55:40,114 - INFO - Epoch [137/300] (135516) train_loss: 24.8603, val_loss: 25.8706, lr: 0.000606, 198.94s
2024-04-02 16:58:43,502 - INFO - epoch complete!
2024-04-02 16:58:43,502 - INFO - evaluating now!
2024-04-02 16:58:58,016 - INFO - Epoch [138/300] (136498) train_loss: 24.9344, val_loss: 25.8398, lr: 0.000602, 197.90s
2024-04-02 17:02:01,253 - INFO - epoch complete!
2024-04-02 17:02:01,254 - INFO - evaluating now!
2024-04-02 17:02:15,784 - INFO - Epoch [139/300] (137480) train_loss: 24.8486, val_loss: 26.3184, lr: 0.000597, 197.77s
2024-04-02 17:05:25,520 - INFO - epoch complete!
2024-04-02 17:05:25,521 - INFO - evaluating now!
2024-04-02 17:05:40,035 - INFO - Epoch [140/300] (138462) train_loss: 24.8373, val_loss: 25.8100, lr: 0.000592, 204.25s
2024-04-02 17:08:43,602 - INFO - epoch complete!
2024-04-02 17:08:43,603 - INFO - evaluating now!
2024-04-02 17:08:58,142 - INFO - Epoch [141/300] (139444) train_loss: 24.8126, val_loss: 25.9261, lr: 0.000588, 198.11s
2024-04-02 17:12:01,601 - INFO - epoch complete!
2024-04-02 17:12:01,601 - INFO - evaluating now!
2024-04-02 17:12:16,144 - INFO - Epoch [142/300] (140426) train_loss: 24.7700, val_loss: 26.2506, lr: 0.000583, 198.00s
2024-04-02 17:15:19,740 - INFO - epoch complete!
2024-04-02 17:15:19,740 - INFO - evaluating now!
2024-04-02 17:15:34,277 - INFO - Epoch [143/300] (141408) train_loss: 24.7341, val_loss: 25.7469, lr: 0.000578, 198.13s
2024-04-02 17:18:44,793 - INFO - epoch complete!
2024-04-02 17:18:44,794 - INFO - evaluating now!
2024-04-02 17:18:59,384 - INFO - Epoch [144/300] (142390) train_loss: 24.6930, val_loss: 26.4354, lr: 0.000574, 205.11s
2024-04-02 17:22:11,625 - INFO - epoch complete!
2024-04-02 17:22:11,626 - INFO - evaluating now!
2024-04-02 17:22:26,210 - INFO - Epoch [145/300] (143372) train_loss: 24.6639, val_loss: 25.7659, lr: 0.000569, 206.83s
2024-04-02 17:25:30,501 - INFO - epoch complete!
2024-04-02 17:25:30,501 - INFO - evaluating now!
2024-04-02 17:25:45,138 - INFO - Epoch [146/300] (144354) train_loss: 24.6946, val_loss: 25.9093, lr: 0.000564, 198.93s
2024-04-02 17:29:00,626 - INFO - epoch complete!
2024-04-02 17:29:00,626 - INFO - evaluating now!
2024-04-02 17:29:15,236 - INFO - Epoch [147/300] (145336) train_loss: 24.6438, val_loss: 25.8261, lr: 0.000559, 210.10s
2024-04-02 17:32:19,376 - INFO - epoch complete!
2024-04-02 17:32:19,376 - INFO - evaluating now!
2024-04-02 17:32:34,265 - INFO - Epoch [148/300] (146318) train_loss: 24.6184, val_loss: 25.5761, lr: 0.000555, 199.03s
2024-04-02 17:32:34,302 - INFO - Saved model at 148
2024-04-02 17:32:34,302 - INFO - Val loss decrease from 25.6564 to 25.5761, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch148.tar
2024-04-02 17:35:50,186 - INFO - epoch complete!
2024-04-02 17:35:50,187 - INFO - evaluating now!
2024-04-02 17:36:04,784 - INFO - Epoch [149/300] (147300) train_loss: 24.6012, val_loss: 26.2548, lr: 0.000550, 210.48s
2024-04-02 17:39:08,436 - INFO - epoch complete!
2024-04-02 17:39:08,436 - INFO - evaluating now!
2024-04-02 17:39:22,981 - INFO - Epoch [150/300] (148282) train_loss: 24.6119, val_loss: 25.7036, lr: 0.000545, 198.20s
2024-04-02 17:42:26,898 - INFO - epoch complete!
2024-04-02 17:42:26,899 - INFO - evaluating now!
2024-04-02 17:42:41,454 - INFO - Epoch [151/300] (149264) train_loss: 24.5165, val_loss: 25.6574, lr: 0.000541, 198.47s
2024-04-02 17:45:45,589 - INFO - epoch complete!
2024-04-02 17:45:45,589 - INFO - evaluating now!
2024-04-02 17:46:00,155 - INFO - Epoch [152/300] (150246) train_loss: 24.6001, val_loss: 25.7418, lr: 0.000536, 198.70s
2024-04-02 17:49:04,314 - INFO - epoch complete!
2024-04-02 17:49:04,315 - INFO - evaluating now!
2024-04-02 17:49:18,870 - INFO - Epoch [153/300] (151228) train_loss: 24.4750, val_loss: 25.9111, lr: 0.000531, 198.72s
2024-04-02 17:52:23,222 - INFO - epoch complete!
2024-04-02 17:52:23,223 - INFO - evaluating now!
2024-04-02 17:52:37,774 - INFO - Epoch [154/300] (152210) train_loss: 24.4938, val_loss: 26.1563, lr: 0.000526, 198.90s
2024-04-02 17:55:41,566 - INFO - epoch complete!
2024-04-02 17:55:41,567 - INFO - evaluating now!
2024-04-02 17:55:56,128 - INFO - Epoch [155/300] (153192) train_loss: 24.5297, val_loss: 25.7392, lr: 0.000522, 198.35s
2024-04-02 17:58:59,963 - INFO - epoch complete!
2024-04-02 17:58:59,964 - INFO - evaluating now!
2024-04-02 17:59:14,535 - INFO - Epoch [156/300] (154174) train_loss: 24.4527, val_loss: 25.5824, lr: 0.000517, 198.41s
2024-04-02 18:02:18,641 - INFO - epoch complete!
2024-04-02 18:02:18,641 - INFO - evaluating now!
2024-04-02 18:02:33,179 - INFO - Epoch [157/300] (155156) train_loss: 24.3988, val_loss: 25.9806, lr: 0.000512, 198.64s
2024-04-02 18:05:37,971 - INFO - epoch complete!
2024-04-02 18:05:37,972 - INFO - evaluating now!
2024-04-02 18:05:52,516 - INFO - Epoch [158/300] (156138) train_loss: 24.4459, val_loss: 25.7321, lr: 0.000508, 199.34s
2024-04-02 18:08:57,563 - INFO - epoch complete!
2024-04-02 18:08:57,564 - INFO - evaluating now!
2024-04-02 18:09:12,111 - INFO - Epoch [159/300] (157120) train_loss: 24.3842, val_loss: 25.6766, lr: 0.000503, 199.59s
2024-04-02 18:12:18,470 - INFO - epoch complete!
2024-04-02 18:12:18,471 - INFO - evaluating now!
2024-04-02 18:12:32,992 - INFO - Epoch [160/300] (158102) train_loss: 24.3853, val_loss: 25.8075, lr: 0.000498, 200.88s
2024-04-02 18:15:47,473 - INFO - epoch complete!
2024-04-02 18:15:47,474 - INFO - evaluating now!
2024-04-02 18:16:02,013 - INFO - Epoch [161/300] (159084) train_loss: 24.2943, val_loss: 25.7557, lr: 0.000494, 209.02s
2024-04-02 18:19:05,481 - INFO - epoch complete!
2024-04-02 18:19:05,481 - INFO - evaluating now!
2024-04-02 18:19:20,006 - INFO - Epoch [162/300] (160066) train_loss: 24.3487, val_loss: 25.5354, lr: 0.000489, 197.99s
2024-04-02 18:19:20,043 - INFO - Saved model at 162
2024-04-02 18:19:20,043 - INFO - Val loss decrease from 25.5761 to 25.5354, saving to ./libcity/cache/84182/model_cache/PDFormer_PeMS03_epoch162.tar
2024-04-02 18:22:23,402 - INFO - epoch complete!
2024-04-02 18:22:23,403 - INFO - evaluating now!
2024-04-02 18:22:37,923 - INFO - Epoch [163/300] (161048) train_loss: 24.2907, val_loss: 25.8677, lr: 0.000484, 197.88s
2024-04-02 18:25:41,201 - INFO - epoch complete!
2024-04-02 18:25:41,201 - INFO - evaluating now!
2024-04-02 18:25:55,661 - INFO - Epoch [164/300] (162030) train_loss: 24.3058, val_loss: 25.9336, lr: 0.000480, 197.74s
2024-04-02 18:28:58,763 - INFO - epoch complete!
2024-04-02 18:28:58,764 - INFO - evaluating now!
2024-04-02 18:29:13,219 - INFO - Epoch [165/300] (163012) train_loss: 24.2412, val_loss: 25.8189, lr: 0.000475, 197.56s
2024-04-02 18:32:17,324 - INFO - epoch complete!
2024-04-02 18:32:17,325 - INFO - evaluating now!
2024-04-02 18:32:31,771 - INFO - Epoch [166/300] (163994) train_loss: 24.2359, val_loss: 25.7473, lr: 0.000470, 198.55s
2024-04-02 18:35:37,852 - INFO - epoch complete!
2024-04-02 18:35:37,853 - INFO - evaluating now!
2024-04-02 18:35:52,316 - INFO - Epoch [167/300] (164976) train_loss: 24.2155, val_loss: 25.6446, lr: 0.000466, 200.54s
2024-04-02 18:39:04,924 - INFO - epoch complete!
2024-04-02 18:39:04,925 - INFO - evaluating now!
2024-04-02 18:39:19,425 - INFO - Epoch [168/300] (165958) train_loss: 24.2326, val_loss: 25.8300, lr: 0.000461, 207.11s
2024-04-02 18:42:22,516 - INFO - epoch complete!
2024-04-02 18:42:22,517 - INFO - evaluating now!
2024-04-02 18:42:37,014 - INFO - Epoch [169/300] (166940) train_loss: 24.1341, val_loss: 25.7520, lr: 0.000456, 197.59s
2024-04-02 18:45:41,030 - INFO - epoch complete!
2024-04-02 18:45:41,031 - INFO - evaluating now!
2024-04-02 18:45:55,524 - INFO - Epoch [170/300] (167922) train_loss: 24.1436, val_loss: 25.6426, lr: 0.000452, 198.51s
2024-04-02 18:48:59,954 - INFO - epoch complete!
2024-04-02 18:48:59,954 - INFO - evaluating now!
2024-04-02 18:49:14,450 - INFO - Epoch [171/300] (168904) train_loss: 24.1426, val_loss: 25.7445, lr: 0.000447, 198.93s
2024-04-02 18:52:18,392 - INFO - epoch complete!
2024-04-02 18:52:18,392 - INFO - evaluating now!
2024-04-02 18:52:32,883 - INFO - Epoch [172/300] (169886) train_loss: 24.1462, val_loss: 25.5777, lr: 0.000443, 198.43s
2024-04-02 18:55:36,288 - INFO - epoch complete!
2024-04-02 18:55:36,288 - INFO - evaluating now!
2024-04-02 18:55:50,781 - INFO - Epoch [173/300] (170868) train_loss: 24.0859, val_loss: 25.7756, lr: 0.000438, 197.90s
2024-04-02 18:58:53,959 - INFO - epoch complete!
2024-04-02 18:58:53,960 - INFO - evaluating now!
2024-04-02 18:59:08,462 - INFO - Epoch [174/300] (171850) train_loss: 24.0794, val_loss: 25.6572, lr: 0.000434, 197.68s
2024-04-02 19:02:11,592 - INFO - epoch complete!
2024-04-02 19:02:11,592 - INFO - evaluating now!
2024-04-02 19:02:26,087 - INFO - Epoch [175/300] (172832) train_loss: 24.0506, val_loss: 25.6164, lr: 0.000429, 197.63s
2024-04-02 19:05:29,350 - INFO - epoch complete!
2024-04-02 19:05:29,351 - INFO - evaluating now!
2024-04-02 19:05:43,848 - INFO - Epoch [176/300] (173814) train_loss: 24.0433, val_loss: 25.8749, lr: 0.000424, 197.76s
2024-04-02 19:08:47,227 - INFO - epoch complete!
2024-04-02 19:08:47,228 - INFO - evaluating now!
2024-04-02 19:09:01,732 - INFO - Epoch [177/300] (174796) train_loss: 24.0237, val_loss: 25.7992, lr: 0.000420, 197.88s
2024-04-02 19:12:05,234 - INFO - epoch complete!
2024-04-02 19:12:05,235 - INFO - evaluating now!
2024-04-02 19:12:19,743 - INFO - Epoch [178/300] (175778) train_loss: 24.0529, val_loss: 25.7967, lr: 0.000415, 198.01s
2024-04-02 19:15:23,105 - INFO - epoch complete!
2024-04-02 19:15:23,106 - INFO - evaluating now!
2024-04-02 19:15:37,603 - INFO - Epoch [179/300] (176760) train_loss: 23.9912, val_loss: 26.2804, lr: 0.000411, 197.86s
2024-04-02 19:18:40,907 - INFO - epoch complete!
2024-04-02 19:18:40,907 - INFO - evaluating now!
2024-04-02 19:18:55,412 - INFO - Epoch [180/300] (177742) train_loss: 23.9576, val_loss: 25.7663, lr: 0.000406, 197.81s
2024-04-02 19:21:58,701 - INFO - epoch complete!
2024-04-02 19:21:58,702 - INFO - evaluating now!
2024-04-02 19:22:13,210 - INFO - Epoch [181/300] (178724) train_loss: 23.9366, val_loss: 25.5814, lr: 0.000402, 197.80s
2024-04-02 19:25:16,580 - INFO - epoch complete!
2024-04-02 19:25:16,581 - INFO - evaluating now!
2024-04-02 19:25:31,080 - INFO - Epoch [182/300] (179706) train_loss: 23.9354, val_loss: 25.7993, lr: 0.000398, 197.87s
2024-04-02 19:28:34,403 - INFO - epoch complete!
2024-04-02 19:28:34,404 - INFO - evaluating now!
2024-04-02 19:28:48,907 - INFO - Epoch [183/300] (180688) train_loss: 23.9213, val_loss: 26.0808, lr: 0.000393, 197.83s
2024-04-02 19:31:52,295 - INFO - epoch complete!
2024-04-02 19:31:52,295 - INFO - evaluating now!
2024-04-02 19:32:06,800 - INFO - Epoch [184/300] (181670) train_loss: 23.9227, val_loss: 25.6219, lr: 0.000389, 197.89s
2024-04-02 19:35:10,295 - INFO - epoch complete!
2024-04-02 19:35:10,296 - INFO - evaluating now!
2024-04-02 19:35:24,797 - INFO - Epoch [185/300] (182652) train_loss: 23.8614, val_loss: 25.6950, lr: 0.000384, 198.00s
2024-04-02 19:38:28,197 - INFO - epoch complete!
2024-04-02 19:38:28,198 - INFO - evaluating now!
2024-04-02 19:38:42,695 - INFO - Epoch [186/300] (183634) train_loss: 23.8874, val_loss: 25.7058, lr: 0.000380, 197.90s
2024-04-02 19:41:46,099 - INFO - epoch complete!
2024-04-02 19:41:46,100 - INFO - evaluating now!
2024-04-02 19:42:00,601 - INFO - Epoch [187/300] (184616) train_loss: 23.8931, val_loss: 25.7232, lr: 0.000376, 197.91s
2024-04-02 19:45:05,563 - INFO - epoch complete!
2024-04-02 19:45:05,564 - INFO - evaluating now!
2024-04-02 19:45:20,079 - INFO - Epoch [188/300] (185598) train_loss: 23.8223, val_loss: 25.7291, lr: 0.000371, 199.48s
2024-04-02 19:48:23,314 - INFO - epoch complete!
2024-04-02 19:48:23,315 - INFO - evaluating now!
2024-04-02 19:48:37,822 - INFO - Epoch [189/300] (186580) train_loss: 23.7907, val_loss: 25.5759, lr: 0.000367, 197.74s
2024-04-02 19:51:41,127 - INFO - epoch complete!
2024-04-02 19:51:41,128 - INFO - evaluating now!
2024-04-02 19:51:55,638 - INFO - Epoch [190/300] (187562) train_loss: 23.7464, val_loss: 25.5922, lr: 0.000363, 197.82s
2024-04-02 19:55:05,801 - INFO - epoch complete!
2024-04-02 19:55:05,802 - INFO - evaluating now!
2024-04-02 19:55:20,268 - INFO - Epoch [191/300] (188544) train_loss: 23.7943, val_loss: 25.7024, lr: 0.000358, 204.63s
2024-04-02 19:58:23,510 - INFO - epoch complete!
2024-04-02 19:58:23,510 - INFO - evaluating now!
2024-04-02 19:58:37,972 - INFO - Epoch [192/300] (189526) train_loss: 23.7484, val_loss: 25.9636, lr: 0.000354, 197.70s
2024-04-02 20:01:41,081 - INFO - epoch complete!
2024-04-02 20:01:41,082 - INFO - evaluating now!
2024-04-02 20:01:55,552 - INFO - Epoch [193/300] (190508) train_loss: 23.7482, val_loss: 25.6240, lr: 0.000350, 197.58s
2024-04-02 20:04:58,834 - INFO - epoch complete!
2024-04-02 20:04:58,835 - INFO - evaluating now!
2024-04-02 20:05:13,304 - INFO - Epoch [194/300] (191490) train_loss: 23.7300, val_loss: 25.9508, lr: 0.000346, 197.75s
2024-04-02 20:08:16,462 - INFO - epoch complete!
2024-04-02 20:08:16,462 - INFO - evaluating now!
2024-04-02 20:08:30,940 - INFO - Epoch [195/300] (192472) train_loss: 23.7270, val_loss: 25.9307, lr: 0.000342, 197.64s
2024-04-02 20:11:34,190 - INFO - epoch complete!
2024-04-02 20:11:34,190 - INFO - evaluating now!
2024-04-02 20:11:48,660 - INFO - Epoch [196/300] (193454) train_loss: 23.7004, val_loss: 25.8939, lr: 0.000337, 197.72s
2024-04-02 20:14:55,161 - INFO - epoch complete!
2024-04-02 20:14:55,161 - INFO - evaluating now!
2024-04-02 20:15:09,710 - INFO - Epoch [197/300] (194436) train_loss: 23.6690, val_loss: 25.7121, lr: 0.000333, 201.05s
2024-04-02 20:18:13,104 - INFO - epoch complete!
2024-04-02 20:18:13,104 - INFO - evaluating now!
2024-04-02 20:18:27,661 - INFO - Epoch [198/300] (195418) train_loss: 23.7413, val_loss: 25.6154, lr: 0.000329, 197.95s
2024-04-02 20:21:31,217 - INFO - epoch complete!
2024-04-02 20:21:31,218 - INFO - evaluating now!
2024-04-02 20:21:45,745 - INFO - Epoch [199/300] (196400) train_loss: 23.6686, val_loss: 25.7858, lr: 0.000325, 198.08s
2024-04-02 20:24:48,987 - INFO - epoch complete!
2024-04-02 20:24:48,988 - INFO - evaluating now!
2024-04-02 20:25:03,507 - INFO - Epoch [200/300] (197382) train_loss: 23.6640, val_loss: 25.7722, lr: 0.000321, 197.76s
2024-04-02 20:28:06,712 - INFO - epoch complete!
2024-04-02 20:28:06,713 - INFO - evaluating now!
2024-04-02 20:28:21,225 - INFO - Epoch [201/300] (198364) train_loss: 23.5928, val_loss: 25.6431, lr: 0.000317, 197.72s
2024-04-02 20:31:24,484 - INFO - epoch complete!
2024-04-02 20:31:24,485 - INFO - evaluating now!
2024-04-02 20:31:38,998 - INFO - Epoch [202/300] (199346) train_loss: 23.6155, val_loss: 25.8045, lr: 0.000313, 197.77s
2024-04-02 20:34:52,058 - INFO - epoch complete!
2024-04-02 20:34:52,058 - INFO - evaluating now!
2024-04-02 20:35:06,532 - INFO - Epoch [203/300] (200328) train_loss: 23.5984, val_loss: 25.7089, lr: 0.000309, 207.53s
2024-04-02 20:38:22,610 - INFO - epoch complete!
2024-04-02 20:38:22,611 - INFO - evaluating now!
2024-04-02 20:38:37,079 - INFO - Epoch [204/300] (201310) train_loss: 23.5613, val_loss: 25.7654, lr: 0.000305, 210.55s
2024-04-02 20:41:40,286 - INFO - epoch complete!
2024-04-02 20:41:40,287 - INFO - evaluating now!
2024-04-02 20:41:54,756 - INFO - Epoch [205/300] (202292) train_loss: 23.5587, val_loss: 25.5902, lr: 0.000301, 197.68s
2024-04-02 20:44:58,002 - INFO - epoch complete!
2024-04-02 20:44:58,002 - INFO - evaluating now!
2024-04-02 20:45:12,462 - INFO - Epoch [206/300] (203274) train_loss: 23.5450, val_loss: 25.6349, lr: 0.000297, 197.71s
2024-04-02 20:48:15,519 - INFO - epoch complete!
2024-04-02 20:48:15,519 - INFO - evaluating now!
2024-04-02 20:48:29,974 - INFO - Epoch [207/300] (204256) train_loss: 23.5293, val_loss: 25.9194, lr: 0.000293, 197.51s
2024-04-02 20:51:33,024 - INFO - epoch complete!
2024-04-02 20:51:33,025 - INFO - evaluating now!
2024-04-02 20:51:47,486 - INFO - Epoch [208/300] (205238) train_loss: 23.5425, val_loss: 25.7097, lr: 0.000289, 197.51s
2024-04-02 20:54:50,706 - INFO - epoch complete!
2024-04-02 20:54:50,707 - INFO - evaluating now!
2024-04-02 20:55:05,152 - INFO - Epoch [209/300] (206220) train_loss: 23.5568, val_loss: 25.6124, lr: 0.000285, 197.67s
2024-04-02 20:58:08,326 - INFO - epoch complete!
2024-04-02 20:58:08,327 - INFO - evaluating now!
2024-04-02 20:58:22,771 - INFO - Epoch [210/300] (207202) train_loss: 23.4669, val_loss: 25.7661, lr: 0.000282, 197.62s
2024-04-02 21:01:26,093 - INFO - epoch complete!
2024-04-02 21:01:26,093 - INFO - evaluating now!
2024-04-02 21:01:40,596 - INFO - Epoch [211/300] (208184) train_loss: 23.4986, val_loss: 25.5720, lr: 0.000278, 197.82s
2024-04-02 21:04:46,017 - INFO - epoch complete!
2024-04-02 21:04:46,018 - INFO - evaluating now!
2024-04-02 21:05:00,604 - INFO - Epoch [212/300] (209166) train_loss: 23.4549, val_loss: 25.8102, lr: 0.000274, 200.01s
2024-04-02 21:05:00,604 - WARNING - Early stopping at epoch: 212
2024-04-02 21:05:00,604 - INFO - Trained totally 213 epochs, average train time is 186.515s, average eval time is 14.612s
2024-04-02 21:05:00,640 - INFO - Loaded model at 162
2024-04-02 21:05:00,641 - INFO - Saved model at ./libcity/cache/84182/model_cache/PDFormer_PeMS03.m
2024-04-02 21:05:00,679 - INFO - Start evaluating ...
2024-04-02 21:05:46,262 - INFO - Note that you select the average mode to evaluate!
2024-04-02 21:05:46,267 - INFO - Evaluate result is saved at ./libcity/cache/84182/evaluate_cache/2024_04_02_21_05_46_PDFormer_PeMS03_average.csv
2024-04-02 21:05:46,274 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   12.581283   inf  22.741388   12.624536     0.135842    22.768101
2   12.907617   inf  23.396982   12.950818     0.138800    23.420668
3   13.205152   inf  23.964136   13.248343     0.141638    23.985100
4   13.474366   inf  24.461386   13.517329     0.144179    24.479839
5   13.716230   inf  24.895414   13.758766     0.146851    24.911572
6   13.935716   inf  25.282896   13.977994     0.149149    25.297081
7   14.136742   inf  25.633198   14.178985     0.150990    25.645721
8   14.322453   inf  25.949991   14.364594     0.152594    25.960987
9   14.499217   inf  26.242342   14.541054     0.154319    26.251945
10  14.665595   inf  26.508158   14.707056     0.156102    26.516424
11  14.828669   inf  26.762695   14.869878     0.157977    26.769804
12  14.998907   inf  27.020565   15.039810     0.160289    27.026632
