2024-03-01 13:08:12,990 - INFO - Log directory: ./libcity/log
2024-03-01 13:08:12,991 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS08, exp_id=7058
2024-03-01 13:08:12,991 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS08', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 2776, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 21, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 6, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 1, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS08'], 'geo_file': 'PeMS08', 'rel_file': 'PeMS08', 'adp_file': 'PeMS08', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 7058}
2024-03-01 13:08:13,281 - INFO - Loaded file PeMS08.geo, num_nodes=170
2024-03-01 13:08:13,282 - INFO - set_weight_link_or_dist: link
2024-03-01 13:08:13,282 - INFO - init_weight_inf_or_zero: zero
2024-03-01 13:08:13,284 - INFO - Loaded file PeMS08.rel, shape=(170, 170)
2024-03-01 13:08:13,284 - INFO - Max adj_mx value = 1.0
2024-03-01 13:08:23,190 - INFO - Loading file PeMS08.dyna
2024-03-01 13:08:24,917 - INFO - Loaded file PeMS08.dyna, shape=(17856, 170, 1)
2024-03-01 13:08:24,938 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS08.npy
2024-03-01 13:08:24,938 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS08_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-03-01 13:08:31,855 - INFO - train	x: (10700, 12, 170, 9), y: (10700, 12, 170, 9), ind: (10700,)
2024-03-01 13:08:31,856 - INFO - eval	x: (3566, 12, 170, 9), y: (3566, 12, 170, 9), ind: (3566,)
2024-03-01 13:08:31,856 - INFO - test	x: (3567, 12, 170, 9), y: (3567, 12, 170, 9), ind: (3567,)
2024-03-01 13:08:32,305 - INFO - StandardScaler mean: 229.8431355598314, std: 145.62553066568907
2024-03-01 13:08:32,306 - INFO - NoneScaler
2024-03-01 13:08:33,555 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS08_21_3_16_5.npy
2024-03-01 13:08:33,562 - INFO - Use use_curriculum_learning!
2024-03-01 13:08:37,105 - INFO - Number of isolated points: 0
2024-03-01 13:08:37,116 - INFO - Number of isolated points: 0
2024-03-01 13:08:37,164 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (stag_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-03-01 13:08:37,167 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.stag_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - enc_embed_layer.stag_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,167 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,168 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,169 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 13:08:37,170 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,171 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,172 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,173 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,174 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,175 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,176 - INFO - encoder_blocks.5.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-01 13:08:37,177 - INFO - encoder_blocks.5.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.5.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-03-01 13:08:37,178 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-03-01 13:08:37,179 - INFO - Total parameter numbers: 1104669
2024-03-01 13:08:37,181 - INFO - You select `adamw` optimizer.
2024-03-01 13:08:37,181 - INFO - You select `cosinelr` lr_scheduler.
2024-03-01 13:08:37,182 - WARNING - Received none train loss func and will use the loss func defined in the model.
2024-03-01 13:08:37,182 - INFO - Number of isolated points: 0
2024-03-01 13:08:37,194 - INFO - Start training ...
2024-03-01 13:08:37,194 - INFO - num_batches:669
2024-03-01 13:08:37,263 - INFO - Training: task_level increase from 0 to 1
2024-03-01 13:08:37,263 - INFO - Current batches_seen is 0
2024-03-01 13:10:21,723 - INFO - epoch complete!
2024-03-01 13:10:21,724 - INFO - evaluating now!
2024-03-01 13:10:28,034 - INFO - Epoch [0/300] (669) train_loss: 240.0191, val_loss: 256.3605, lr: 0.000201, 110.84s
2024-03-01 13:10:28,085 - INFO - Saved model at 0
2024-03-01 13:10:28,085 - INFO - Val loss decrease from inf to 256.3605, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch0.tar
2024-03-01 13:12:33,492 - INFO - epoch complete!
2024-03-01 13:12:33,492 - INFO - evaluating now!
2024-03-01 13:12:45,405 - INFO - Epoch [1/300] (1338) train_loss: 76.0206, val_loss: 165.0381, lr: 0.000401, 137.32s
2024-03-01 13:12:45,455 - INFO - Saved model at 1
2024-03-01 13:12:45,455 - INFO - Val loss decrease from 256.3605 to 165.0381, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch1.tar
2024-03-01 13:15:13,548 - INFO - epoch complete!
2024-03-01 13:15:13,550 - INFO - evaluating now!
2024-03-01 13:15:24,820 - INFO - Epoch [2/300] (2007) train_loss: 40.9426, val_loss: 172.4002, lr: 0.000600, 159.37s
2024-03-01 13:17:49,051 - INFO - epoch complete!
2024-03-01 13:17:49,052 - INFO - evaluating now!
2024-03-01 13:18:00,354 - INFO - Epoch [3/300] (2676) train_loss: 36.4732, val_loss: 183.6498, lr: 0.000800, 155.53s
2024-03-01 13:18:22,406 - INFO - Training: task_level increase from 1 to 2
2024-03-01 13:18:22,407 - INFO - Current batches_seen is 2776
2024-03-01 13:20:28,421 - INFO - epoch complete!
2024-03-01 13:20:28,422 - INFO - evaluating now!
2024-03-01 13:20:40,341 - INFO - Epoch [4/300] (3345) train_loss: 38.1856, val_loss: 131.7820, lr: 0.000999, 159.99s
2024-03-01 13:20:40,392 - INFO - Saved model at 4
2024-03-01 13:20:40,392 - INFO - Val loss decrease from 165.0381 to 131.7820, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch4.tar
2024-03-01 13:23:07,151 - INFO - epoch complete!
2024-03-01 13:23:07,151 - INFO - evaluating now!
2024-03-01 13:23:18,432 - INFO - Epoch [5/300] (4014) train_loss: 32.7459, val_loss: 135.1641, lr: 0.000999, 158.04s
2024-03-01 13:25:43,751 - INFO - epoch complete!
2024-03-01 13:25:43,752 - INFO - evaluating now!
2024-03-01 13:25:55,718 - INFO - Epoch [6/300] (4683) train_loss: 31.0748, val_loss: 136.8261, lr: 0.000999, 157.29s
2024-03-01 13:28:22,029 - INFO - epoch complete!
2024-03-01 13:28:22,030 - INFO - evaluating now!
2024-03-01 13:28:33,978 - INFO - Epoch [7/300] (5352) train_loss: 30.0783, val_loss: 139.2067, lr: 0.000998, 158.26s
2024-03-01 13:29:18,136 - INFO - Training: task_level increase from 2 to 3
2024-03-01 13:29:18,136 - INFO - Current batches_seen is 5552
2024-03-01 13:30:59,559 - INFO - epoch complete!
2024-03-01 13:30:59,560 - INFO - evaluating now!
2024-03-01 13:31:10,914 - INFO - Epoch [8/300] (6021) train_loss: 30.4445, val_loss: 140.2179, lr: 0.000998, 156.94s
2024-03-01 13:33:36,036 - INFO - epoch complete!
2024-03-01 13:33:36,036 - INFO - evaluating now!
2024-03-01 13:33:47,969 - INFO - Epoch [9/300] (6690) train_loss: 30.0227, val_loss: 142.6540, lr: 0.000998, 157.05s
2024-03-01 13:36:17,386 - INFO - epoch complete!
2024-03-01 13:36:17,387 - INFO - evaluating now!
2024-03-01 13:36:29,329 - INFO - Epoch [10/300] (7359) train_loss: 29.5909, val_loss: 145.9401, lr: 0.000997, 161.36s
2024-03-01 13:39:05,111 - INFO - epoch complete!
2024-03-01 13:39:05,112 - INFO - evaluating now!
2024-03-01 13:39:16,809 - INFO - Epoch [11/300] (8028) train_loss: 29.1925, val_loss: 149.2938, lr: 0.000996, 167.48s
2024-03-01 13:40:21,684 - INFO - Training: task_level increase from 3 to 4
2024-03-01 13:40:21,684 - INFO - Current batches_seen is 8328
2024-03-01 13:41:41,602 - INFO - epoch complete!
2024-03-01 13:41:41,602 - INFO - evaluating now!
2024-03-01 13:41:53,264 - INFO - Epoch [12/300] (8697) train_loss: 29.7866, val_loss: 144.3044, lr: 0.000996, 156.45s
2024-03-01 13:44:21,729 - INFO - epoch complete!
2024-03-01 13:44:21,729 - INFO - evaluating now!
2024-03-01 13:44:33,041 - INFO - Epoch [13/300] (9366) train_loss: 29.9177, val_loss: 143.6869, lr: 0.000995, 159.78s
2024-03-01 13:46:59,019 - INFO - epoch complete!
2024-03-01 13:46:59,020 - INFO - evaluating now!
2024-03-01 13:47:11,061 - INFO - Epoch [14/300] (10035) train_loss: 29.5272, val_loss: 144.5442, lr: 0.000994, 158.02s
2024-03-01 13:49:39,996 - INFO - epoch complete!
2024-03-01 13:49:39,997 - INFO - evaluating now!
2024-03-01 13:49:52,010 - INFO - Epoch [15/300] (10704) train_loss: 29.1231, val_loss: 145.6890, lr: 0.000994, 160.95s
2024-03-01 13:51:19,651 - INFO - Training: task_level increase from 4 to 5
2024-03-01 13:51:19,651 - INFO - Current batches_seen is 11104
2024-03-01 13:52:18,713 - INFO - epoch complete!
2024-03-01 13:52:18,713 - INFO - evaluating now!
2024-03-01 13:52:30,686 - INFO - Epoch [16/300] (11373) train_loss: 30.4992, val_loss: 131.4436, lr: 0.000993, 158.68s
2024-03-01 13:52:30,737 - INFO - Saved model at 16
2024-03-01 13:52:30,738 - INFO - Val loss decrease from 131.7820 to 131.4436, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch16.tar
2024-03-01 13:54:56,584 - INFO - epoch complete!
2024-03-01 13:54:56,585 - INFO - evaluating now!
2024-03-01 13:55:08,656 - INFO - Epoch [17/300] (12042) train_loss: 29.8938, val_loss: 130.5675, lr: 0.000992, 157.92s
2024-03-01 13:55:08,707 - INFO - Saved model at 17
2024-03-01 13:55:08,707 - INFO - Val loss decrease from 131.4436 to 130.5675, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch17.tar
2024-03-01 13:57:41,225 - INFO - epoch complete!
2024-03-01 13:57:41,225 - INFO - evaluating now!
2024-03-01 13:57:53,228 - INFO - Epoch [18/300] (12711) train_loss: 29.5540, val_loss: 131.8359, lr: 0.000991, 164.52s
2024-03-01 14:00:24,751 - INFO - epoch complete!
2024-03-01 14:00:24,751 - INFO - evaluating now!
2024-03-01 14:00:36,721 - INFO - Epoch [19/300] (13380) train_loss: 29.1408, val_loss: 132.1173, lr: 0.000990, 163.49s
2024-03-01 14:02:31,595 - INFO - Training: task_level increase from 5 to 6
2024-03-01 14:02:31,595 - INFO - Current batches_seen is 13880
2024-03-01 14:03:10,170 - INFO - epoch complete!
2024-03-01 14:03:10,171 - INFO - evaluating now!
2024-03-01 14:03:22,118 - INFO - Epoch [20/300] (14049) train_loss: 29.4375, val_loss: 117.2298, lr: 0.000989, 165.40s
2024-03-01 14:03:22,167 - INFO - Saved model at 20
2024-03-01 14:03:22,167 - INFO - Val loss decrease from 130.5675 to 117.2298, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch20.tar
2024-03-01 14:05:48,071 - INFO - epoch complete!
2024-03-01 14:05:48,072 - INFO - evaluating now!
2024-03-01 14:06:00,080 - INFO - Epoch [21/300] (14718) train_loss: 29.3806, val_loss: 117.1688, lr: 0.000988, 157.91s
2024-03-01 14:06:00,131 - INFO - Saved model at 21
2024-03-01 14:06:00,131 - INFO - Val loss decrease from 117.2298 to 117.1688, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch21.tar
2024-03-01 14:08:26,158 - INFO - epoch complete!
2024-03-01 14:08:26,159 - INFO - evaluating now!
2024-03-01 14:08:38,196 - INFO - Epoch [22/300] (15387) train_loss: 28.9901, val_loss: 117.7392, lr: 0.000987, 158.06s
2024-03-01 14:11:04,398 - INFO - epoch complete!
2024-03-01 14:11:04,399 - INFO - evaluating now!
2024-03-01 14:11:16,434 - INFO - Epoch [23/300] (16056) train_loss: 28.9623, val_loss: 118.2869, lr: 0.000986, 158.24s
2024-03-01 14:13:27,733 - INFO - Training: task_level increase from 6 to 7
2024-03-01 14:13:27,733 - INFO - Current batches_seen is 16656
2024-03-01 14:13:42,772 - INFO - epoch complete!
2024-03-01 14:13:42,772 - INFO - evaluating now!
2024-03-01 14:13:54,044 - INFO - Epoch [24/300] (16725) train_loss: 28.8663, val_loss: 104.1610, lr: 0.000985, 157.61s
2024-03-01 14:13:54,095 - INFO - Saved model at 24
2024-03-01 14:13:54,095 - INFO - Val loss decrease from 117.1688 to 104.1610, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch24.tar
2024-03-01 14:16:21,960 - INFO - epoch complete!
2024-03-01 14:16:21,961 - INFO - evaluating now!
2024-03-01 14:16:33,280 - INFO - Epoch [25/300] (17394) train_loss: 29.0968, val_loss: 104.0578, lr: 0.000983, 159.18s
2024-03-01 14:16:33,331 - INFO - Saved model at 25
2024-03-01 14:16:33,332 - INFO - Val loss decrease from 104.1610 to 104.0578, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch25.tar
2024-03-01 14:19:00,496 - INFO - epoch complete!
2024-03-01 14:19:00,496 - INFO - evaluating now!
2024-03-01 14:19:12,439 - INFO - Epoch [26/300] (18063) train_loss: 28.7576, val_loss: 104.3359, lr: 0.000982, 159.11s
2024-03-01 14:21:39,084 - INFO - epoch complete!
2024-03-01 14:21:39,084 - INFO - evaluating now!
2024-03-01 14:21:50,986 - INFO - Epoch [27/300] (18732) train_loss: 28.6344, val_loss: 104.4755, lr: 0.000981, 158.55s
2024-03-01 14:24:19,845 - INFO - epoch complete!
2024-03-01 14:24:19,845 - INFO - evaluating now!
2024-03-01 14:24:31,749 - INFO - Epoch [28/300] (19401) train_loss: 28.5313, val_loss: 104.8037, lr: 0.000979, 160.76s
2024-03-01 14:24:38,565 - INFO - Training: task_level increase from 7 to 8
2024-03-01 14:24:38,565 - INFO - Current batches_seen is 19432
2024-03-01 14:26:56,967 - INFO - epoch complete!
2024-03-01 14:26:56,968 - INFO - evaluating now!
2024-03-01 14:27:08,483 - INFO - Epoch [29/300] (20070) train_loss: 28.9639, val_loss: 89.9297, lr: 0.000978, 156.73s
2024-03-01 14:27:08,533 - INFO - Saved model at 29
2024-03-01 14:27:08,534 - INFO - Val loss decrease from 104.0578 to 89.9297, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch29.tar
2024-03-01 14:29:33,426 - INFO - epoch complete!
2024-03-01 14:29:33,427 - INFO - evaluating now!
2024-03-01 14:29:44,733 - INFO - Epoch [30/300] (20739) train_loss: 28.8322, val_loss: 90.9546, lr: 0.000976, 156.20s
2024-03-01 14:32:11,468 - INFO - epoch complete!
2024-03-01 14:32:11,469 - INFO - evaluating now!
2024-03-01 14:32:23,388 - INFO - Epoch [31/300] (21408) train_loss: 28.4305, val_loss: 90.4872, lr: 0.000975, 158.65s
2024-03-01 14:34:50,312 - INFO - epoch complete!
2024-03-01 14:34:50,312 - INFO - evaluating now!
2024-03-01 14:35:02,356 - INFO - Epoch [32/300] (22077) train_loss: 28.3520, val_loss: 90.1212, lr: 0.000973, 158.97s
2024-03-01 14:35:33,419 - INFO - Training: task_level increase from 8 to 9
2024-03-01 14:35:33,419 - INFO - Current batches_seen is 22208
2024-03-01 14:37:33,233 - INFO - epoch complete!
2024-03-01 14:37:33,233 - INFO - evaluating now!
2024-03-01 14:37:45,285 - INFO - Epoch [33/300] (22746) train_loss: 28.6405, val_loss: 80.6195, lr: 0.000972, 162.93s
2024-03-01 14:37:45,335 - INFO - Saved model at 33
2024-03-01 14:37:45,335 - INFO - Val loss decrease from 89.9297 to 80.6195, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch33.tar
2024-03-01 14:40:17,259 - INFO - epoch complete!
2024-03-01 14:40:17,260 - INFO - evaluating now!
2024-03-01 14:40:28,854 - INFO - Epoch [34/300] (23415) train_loss: 28.5542, val_loss: 80.5538, lr: 0.000970, 163.52s
2024-03-01 14:40:28,905 - INFO - Saved model at 34
2024-03-01 14:40:28,905 - INFO - Val loss decrease from 80.6195 to 80.5538, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch34.tar
2024-03-01 14:42:58,995 - INFO - epoch complete!
2024-03-01 14:42:58,996 - INFO - evaluating now!
2024-03-01 14:43:10,105 - INFO - Epoch [35/300] (24084) train_loss: 28.3351, val_loss: 80.3975, lr: 0.000968, 161.20s
2024-03-01 14:43:10,156 - INFO - Saved model at 35
2024-03-01 14:43:10,156 - INFO - Val loss decrease from 80.5538 to 80.3975, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch35.tar
2024-03-01 14:45:37,534 - INFO - epoch complete!
2024-03-01 14:45:37,535 - INFO - evaluating now!
2024-03-01 14:45:48,813 - INFO - Epoch [36/300] (24753) train_loss: 28.1433, val_loss: 80.6411, lr: 0.000967, 158.66s
2024-03-01 14:46:38,025 - INFO - Training: task_level increase from 9 to 10
2024-03-01 14:46:38,025 - INFO - Current batches_seen is 24984
2024-03-01 14:48:13,443 - INFO - epoch complete!
2024-03-01 14:48:13,443 - INFO - evaluating now!
2024-03-01 14:48:25,331 - INFO - Epoch [37/300] (25422) train_loss: 28.7873, val_loss: 63.4353, lr: 0.000965, 156.52s
2024-03-01 14:48:25,383 - INFO - Saved model at 37
2024-03-01 14:48:25,383 - INFO - Val loss decrease from 80.3975 to 63.4353, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch37.tar
2024-03-01 14:50:51,462 - INFO - epoch complete!
2024-03-01 14:50:51,463 - INFO - evaluating now!
2024-03-01 14:51:03,271 - INFO - Epoch [38/300] (26091) train_loss: 28.5260, val_loss: 63.0935, lr: 0.000963, 157.89s
2024-03-01 14:51:03,321 - INFO - Saved model at 38
2024-03-01 14:51:03,322 - INFO - Val loss decrease from 63.4353 to 63.0935, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch38.tar
2024-03-01 14:53:31,421 - INFO - epoch complete!
2024-03-01 14:53:31,422 - INFO - evaluating now!
2024-03-01 14:53:43,341 - INFO - Epoch [39/300] (26760) train_loss: 28.3871, val_loss: 63.7324, lr: 0.000961, 160.02s
2024-03-01 14:56:07,804 - INFO - epoch complete!
2024-03-01 14:56:07,804 - INFO - evaluating now!
2024-03-01 14:56:19,207 - INFO - Epoch [40/300] (27429) train_loss: 28.2224, val_loss: 63.1468, lr: 0.000959, 155.86s
2024-03-01 14:57:30,287 - INFO - Training: task_level increase from 10 to 11
2024-03-01 14:57:30,288 - INFO - Current batches_seen is 27760
2024-03-01 14:58:43,446 - INFO - epoch complete!
2024-03-01 14:58:43,446 - INFO - evaluating now!
2024-03-01 14:58:54,722 - INFO - Epoch [41/300] (28098) train_loss: 28.8678, val_loss: 45.6661, lr: 0.000957, 155.51s
2024-03-01 14:58:54,771 - INFO - Saved model at 41
2024-03-01 14:58:54,771 - INFO - Val loss decrease from 63.0935 to 45.6661, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch41.tar
2024-03-01 15:01:19,661 - INFO - epoch complete!
2024-03-01 15:01:19,661 - INFO - evaluating now!
2024-03-01 15:01:30,919 - INFO - Epoch [42/300] (28767) train_loss: 28.5042, val_loss: 46.1721, lr: 0.000955, 156.15s
2024-03-01 15:03:56,022 - INFO - epoch complete!
2024-03-01 15:03:56,023 - INFO - evaluating now!
2024-03-01 15:04:07,823 - INFO - Epoch [43/300] (29436) train_loss: 28.4198, val_loss: 45.5943, lr: 0.000953, 156.90s
2024-03-01 15:04:07,874 - INFO - Saved model at 43
2024-03-01 15:04:07,874 - INFO - Val loss decrease from 45.6661 to 45.5943, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch43.tar
2024-03-01 15:06:33,944 - INFO - epoch complete!
2024-03-01 15:06:33,945 - INFO - evaluating now!
2024-03-01 15:06:45,938 - INFO - Epoch [44/300] (30105) train_loss: 28.1649, val_loss: 45.4759, lr: 0.000951, 158.06s
2024-03-01 15:06:45,987 - INFO - Saved model at 44
2024-03-01 15:06:45,987 - INFO - Val loss decrease from 45.5943 to 45.4759, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch44.tar
2024-03-01 15:08:19,551 - INFO - Training: task_level increase from 11 to 12
2024-03-01 15:08:19,552 - INFO - Current batches_seen is 30536
2024-03-01 15:09:12,328 - INFO - epoch complete!
2024-03-01 15:09:12,328 - INFO - evaluating now!
2024-03-01 15:09:24,282 - INFO - Epoch [45/300] (30774) train_loss: 28.5406, val_loss: 28.3772, lr: 0.000949, 158.29s
2024-03-01 15:09:24,332 - INFO - Saved model at 45
2024-03-01 15:09:24,332 - INFO - Val loss decrease from 45.4759 to 28.3772, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch45.tar
2024-03-01 15:11:48,952 - INFO - epoch complete!
2024-03-01 15:11:48,953 - INFO - evaluating now!
2024-03-01 15:12:00,851 - INFO - Epoch [46/300] (31443) train_loss: 28.7437, val_loss: 28.8228, lr: 0.000947, 156.52s
2024-03-01 15:14:26,623 - INFO - epoch complete!
2024-03-01 15:14:26,624 - INFO - evaluating now!
2024-03-01 15:14:38,515 - INFO - Epoch [47/300] (32112) train_loss: 28.4232, val_loss: 28.0280, lr: 0.000944, 157.66s
2024-03-01 15:14:38,566 - INFO - Saved model at 47
2024-03-01 15:14:38,566 - INFO - Val loss decrease from 28.3772 to 28.0280, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch47.tar
2024-03-01 15:17:03,323 - INFO - epoch complete!
2024-03-01 15:17:03,324 - INFO - evaluating now!
2024-03-01 15:17:15,156 - INFO - Epoch [48/300] (32781) train_loss: 28.2907, val_loss: 28.6356, lr: 0.000942, 156.59s
2024-03-01 15:19:41,426 - INFO - epoch complete!
2024-03-01 15:19:41,427 - INFO - evaluating now!
2024-03-01 15:19:53,313 - INFO - Epoch [49/300] (33450) train_loss: 28.3090, val_loss: 28.2782, lr: 0.000940, 158.16s
2024-03-01 15:22:18,402 - INFO - epoch complete!
2024-03-01 15:22:18,403 - INFO - evaluating now!
2024-03-01 15:22:30,120 - INFO - Epoch [50/300] (34119) train_loss: 28.1986, val_loss: 27.9279, lr: 0.000937, 156.81s
2024-03-01 15:22:30,170 - INFO - Saved model at 50
2024-03-01 15:22:30,171 - INFO - Val loss decrease from 28.0280 to 27.9279, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch50.tar
2024-03-01 15:24:55,278 - INFO - epoch complete!
2024-03-01 15:24:55,279 - INFO - evaluating now!
2024-03-01 15:25:07,090 - INFO - Epoch [51/300] (34788) train_loss: 27.9867, val_loss: 27.9259, lr: 0.000935, 156.92s
2024-03-01 15:25:07,142 - INFO - Saved model at 51
2024-03-01 15:25:07,143 - INFO - Val loss decrease from 27.9279 to 27.9259, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch51.tar
2024-03-01 15:27:31,616 - INFO - epoch complete!
2024-03-01 15:27:31,616 - INFO - evaluating now!
2024-03-01 15:27:42,925 - INFO - Epoch [52/300] (35457) train_loss: 27.8801, val_loss: 27.6484, lr: 0.000932, 155.78s
2024-03-01 15:27:42,975 - INFO - Saved model at 52
2024-03-01 15:27:42,975 - INFO - Val loss decrease from 27.9259 to 27.6484, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch52.tar
2024-03-01 15:30:08,802 - INFO - epoch complete!
2024-03-01 15:30:08,802 - INFO - evaluating now!
2024-03-01 15:30:20,808 - INFO - Epoch [53/300] (36126) train_loss: 27.8590, val_loss: 28.0657, lr: 0.000930, 157.83s
2024-03-01 15:32:50,613 - INFO - epoch complete!
2024-03-01 15:32:50,613 - INFO - evaluating now!
2024-03-01 15:33:02,684 - INFO - Epoch [54/300] (36795) train_loss: 27.8264, val_loss: 27.5453, lr: 0.000927, 161.88s
2024-03-01 15:33:02,736 - INFO - Saved model at 54
2024-03-01 15:33:02,737 - INFO - Val loss decrease from 27.6484 to 27.5453, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch54.tar
2024-03-01 15:35:41,846 - INFO - epoch complete!
2024-03-01 15:35:41,846 - INFO - evaluating now!
2024-03-01 15:35:53,955 - INFO - Epoch [55/300] (37464) train_loss: 27.6739, val_loss: 27.5441, lr: 0.000925, 171.22s
2024-03-01 15:35:54,005 - INFO - Saved model at 55
2024-03-01 15:35:54,006 - INFO - Val loss decrease from 27.5453 to 27.5441, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch55.tar
2024-03-01 15:38:20,534 - INFO - epoch complete!
2024-03-01 15:38:20,535 - INFO - evaluating now!
2024-03-01 15:38:31,890 - INFO - Epoch [56/300] (38133) train_loss: 27.7756, val_loss: 27.3423, lr: 0.000922, 157.88s
2024-03-01 15:38:32,066 - INFO - Saved model at 56
2024-03-01 15:38:32,066 - INFO - Val loss decrease from 27.5441 to 27.3423, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch56.tar
2024-03-01 15:40:58,202 - INFO - epoch complete!
2024-03-01 15:40:58,203 - INFO - evaluating now!
2024-03-01 15:41:10,118 - INFO - Epoch [57/300] (38802) train_loss: 27.6000, val_loss: 27.6885, lr: 0.000920, 158.05s
2024-03-01 15:43:34,874 - INFO - epoch complete!
2024-03-01 15:43:34,874 - INFO - evaluating now!
2024-03-01 15:43:46,680 - INFO - Epoch [58/300] (39471) train_loss: 27.5858, val_loss: 27.5948, lr: 0.000917, 156.56s
2024-03-01 15:46:12,675 - INFO - epoch complete!
2024-03-01 15:46:12,675 - INFO - evaluating now!
2024-03-01 15:46:24,736 - INFO - Epoch [59/300] (40140) train_loss: 27.4896, val_loss: 27.3725, lr: 0.000914, 158.06s
2024-03-01 15:48:49,907 - INFO - epoch complete!
2024-03-01 15:48:49,907 - INFO - evaluating now!
2024-03-01 15:49:00,983 - INFO - Epoch [60/300] (40809) train_loss: 27.4523, val_loss: 27.9237, lr: 0.000911, 156.25s
2024-03-01 15:51:25,732 - INFO - epoch complete!
2024-03-01 15:51:25,733 - INFO - evaluating now!
2024-03-01 15:51:37,020 - INFO - Epoch [61/300] (41478) train_loss: 27.4897, val_loss: 28.1434, lr: 0.000908, 156.04s
2024-03-01 15:54:02,897 - INFO - epoch complete!
2024-03-01 15:54:02,898 - INFO - evaluating now!
2024-03-01 15:54:14,755 - INFO - Epoch [62/300] (42147) train_loss: 27.4251, val_loss: 28.0340, lr: 0.000906, 157.73s
2024-03-01 15:56:40,519 - INFO - epoch complete!
2024-03-01 15:56:40,520 - INFO - evaluating now!
2024-03-01 15:56:52,436 - INFO - Epoch [63/300] (42816) train_loss: 27.3694, val_loss: 27.0189, lr: 0.000903, 157.68s
2024-03-01 15:56:52,487 - INFO - Saved model at 63
2024-03-01 15:56:52,488 - INFO - Val loss decrease from 27.3423 to 27.0189, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch63.tar
2024-03-01 15:59:19,320 - INFO - epoch complete!
2024-03-01 15:59:19,320 - INFO - evaluating now!
2024-03-01 15:59:31,303 - INFO - Epoch [64/300] (43485) train_loss: 27.3110, val_loss: 28.5222, lr: 0.000900, 158.82s
2024-03-01 16:01:58,664 - INFO - epoch complete!
2024-03-01 16:01:58,665 - INFO - evaluating now!
2024-03-01 16:02:10,176 - INFO - Epoch [65/300] (44154) train_loss: 27.4496, val_loss: 27.3749, lr: 0.000897, 158.87s
2024-03-01 16:04:36,150 - INFO - epoch complete!
2024-03-01 16:04:36,151 - INFO - evaluating now!
2024-03-01 16:04:48,085 - INFO - Epoch [66/300] (44823) train_loss: 27.1731, val_loss: 27.1932, lr: 0.000894, 157.91s
2024-03-01 16:07:13,759 - INFO - epoch complete!
2024-03-01 16:07:13,759 - INFO - evaluating now!
2024-03-01 16:07:25,655 - INFO - Epoch [67/300] (45492) train_loss: 27.1732, val_loss: 27.2797, lr: 0.000891, 157.57s
2024-03-01 16:09:52,191 - INFO - epoch complete!
2024-03-01 16:09:52,192 - INFO - evaluating now!
2024-03-01 16:10:04,099 - INFO - Epoch [68/300] (46161) train_loss: 27.1468, val_loss: 27.0105, lr: 0.000888, 158.44s
2024-03-01 16:10:04,151 - INFO - Saved model at 68
2024-03-01 16:10:04,151 - INFO - Val loss decrease from 27.0189 to 27.0105, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch68.tar
2024-03-01 16:12:29,774 - INFO - epoch complete!
2024-03-01 16:12:29,775 - INFO - evaluating now!
2024-03-01 16:12:41,645 - INFO - Epoch [69/300] (46830) train_loss: 27.1089, val_loss: 27.1492, lr: 0.000884, 157.49s
2024-03-01 16:15:09,040 - INFO - epoch complete!
2024-03-01 16:15:09,041 - INFO - evaluating now!
2024-03-01 16:15:20,976 - INFO - Epoch [70/300] (47499) train_loss: 27.0462, val_loss: 26.9046, lr: 0.000881, 159.33s
2024-03-01 16:15:21,028 - INFO - Saved model at 70
2024-03-01 16:15:21,028 - INFO - Val loss decrease from 27.0105 to 26.9046, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch70.tar
2024-03-01 16:17:47,705 - INFO - epoch complete!
2024-03-01 16:17:47,706 - INFO - evaluating now!
2024-03-01 16:17:59,690 - INFO - Epoch [71/300] (48168) train_loss: 27.0093, val_loss: 26.7674, lr: 0.000878, 158.66s
2024-03-01 16:17:59,741 - INFO - Saved model at 71
2024-03-01 16:17:59,741 - INFO - Val loss decrease from 26.9046 to 26.7674, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch71.tar
2024-03-01 16:20:25,322 - INFO - epoch complete!
2024-03-01 16:20:25,322 - INFO - evaluating now!
2024-03-01 16:20:36,845 - INFO - Epoch [72/300] (48837) train_loss: 26.9127, val_loss: 29.0625, lr: 0.000875, 157.10s
2024-03-01 16:23:02,242 - INFO - epoch complete!
2024-03-01 16:23:02,242 - INFO - evaluating now!
2024-03-01 16:23:13,654 - INFO - Epoch [73/300] (49506) train_loss: 26.9558, val_loss: 26.9900, lr: 0.000872, 156.81s
2024-03-01 16:25:37,974 - INFO - epoch complete!
2024-03-01 16:25:37,975 - INFO - evaluating now!
2024-03-01 16:25:49,298 - INFO - Epoch [74/300] (50175) train_loss: 26.8908, val_loss: 27.1112, lr: 0.000868, 155.64s
2024-03-01 16:28:15,701 - INFO - epoch complete!
2024-03-01 16:28:15,702 - INFO - evaluating now!
2024-03-01 16:28:27,813 - INFO - Epoch [75/300] (50844) train_loss: 26.8505, val_loss: 26.8022, lr: 0.000865, 158.52s
2024-03-01 16:30:59,125 - INFO - epoch complete!
2024-03-01 16:30:59,126 - INFO - evaluating now!
2024-03-01 16:31:10,608 - INFO - Epoch [76/300] (51513) train_loss: 26.7524, val_loss: 26.6355, lr: 0.000861, 162.79s
2024-03-01 16:31:10,662 - INFO - Saved model at 76
2024-03-01 16:31:10,662 - INFO - Val loss decrease from 26.7674 to 26.6355, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch76.tar
2024-03-01 16:33:37,081 - INFO - epoch complete!
2024-03-01 16:33:37,081 - INFO - evaluating now!
2024-03-01 16:33:49,009 - INFO - Epoch [77/300] (52182) train_loss: 26.7992, val_loss: 26.7230, lr: 0.000858, 158.35s
2024-03-01 16:36:16,687 - INFO - epoch complete!
2024-03-01 16:36:16,688 - INFO - evaluating now!
2024-03-01 16:36:28,656 - INFO - Epoch [78/300] (52851) train_loss: 26.8043, val_loss: 27.1365, lr: 0.000855, 159.65s
2024-03-01 16:38:55,514 - INFO - epoch complete!
2024-03-01 16:38:55,515 - INFO - evaluating now!
2024-03-01 16:39:06,753 - INFO - Epoch [79/300] (53520) train_loss: 26.8917, val_loss: 26.5057, lr: 0.000851, 158.10s
2024-03-01 16:39:06,804 - INFO - Saved model at 79
2024-03-01 16:39:06,804 - INFO - Val loss decrease from 26.6355 to 26.5057, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch79.tar
2024-03-01 16:41:31,641 - INFO - epoch complete!
2024-03-01 16:41:31,642 - INFO - evaluating now!
2024-03-01 16:41:43,615 - INFO - Epoch [80/300] (54189) train_loss: 26.6947, val_loss: 26.8289, lr: 0.000848, 156.81s
2024-03-01 16:44:16,847 - INFO - epoch complete!
2024-03-01 16:44:16,848 - INFO - evaluating now!
2024-03-01 16:44:29,167 - INFO - Epoch [81/300] (54858) train_loss: 26.6588, val_loss: 27.3425, lr: 0.000844, 165.55s
2024-03-01 16:46:56,116 - INFO - epoch complete!
2024-03-01 16:46:56,116 - INFO - evaluating now!
2024-03-01 16:47:08,019 - INFO - Epoch [82/300] (55527) train_loss: 26.5573, val_loss: 26.8803, lr: 0.000840, 158.85s
2024-03-01 16:49:40,956 - INFO - epoch complete!
2024-03-01 16:49:40,957 - INFO - evaluating now!
2024-03-01 16:49:53,253 - INFO - Epoch [83/300] (56196) train_loss: 26.5828, val_loss: 26.5757, lr: 0.000837, 165.23s
2024-03-01 16:52:23,218 - INFO - epoch complete!
2024-03-01 16:52:23,219 - INFO - evaluating now!
2024-03-01 16:52:35,412 - INFO - Epoch [84/300] (56865) train_loss: 26.6256, val_loss: 27.0768, lr: 0.000833, 162.16s
2024-03-01 16:55:07,948 - INFO - epoch complete!
2024-03-01 16:55:07,948 - INFO - evaluating now!
2024-03-01 16:55:20,133 - INFO - Epoch [85/300] (57534) train_loss: 26.5766, val_loss: 27.0143, lr: 0.000830, 164.72s
2024-03-01 16:57:52,720 - INFO - epoch complete!
2024-03-01 16:57:52,721 - INFO - evaluating now!
2024-03-01 16:58:04,836 - INFO - Epoch [86/300] (58203) train_loss: 26.4366, val_loss: 26.6107, lr: 0.000826, 164.70s
2024-03-01 17:00:42,264 - INFO - epoch complete!
2024-03-01 17:00:42,265 - INFO - evaluating now!
2024-03-01 17:00:53,377 - INFO - Epoch [87/300] (58872) train_loss: 26.5718, val_loss: 26.9798, lr: 0.000822, 168.54s
2024-03-01 17:03:21,798 - INFO - epoch complete!
2024-03-01 17:03:21,798 - INFO - evaluating now!
2024-03-01 17:03:32,921 - INFO - Epoch [88/300] (59541) train_loss: 26.4453, val_loss: 26.4661, lr: 0.000818, 159.54s
2024-03-01 17:03:32,973 - INFO - Saved model at 88
2024-03-01 17:03:32,973 - INFO - Val loss decrease from 26.5057 to 26.4661, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch88.tar
2024-03-01 17:05:57,541 - INFO - epoch complete!
2024-03-01 17:05:57,542 - INFO - evaluating now!
2024-03-01 17:06:08,870 - INFO - Epoch [89/300] (60210) train_loss: 26.2628, val_loss: 26.7622, lr: 0.000815, 155.90s
2024-03-01 17:08:34,066 - INFO - epoch complete!
2024-03-01 17:08:34,066 - INFO - evaluating now!
2024-03-01 17:08:45,405 - INFO - Epoch [90/300] (60879) train_loss: 26.2975, val_loss: 26.6465, lr: 0.000811, 156.53s
2024-03-01 17:11:13,973 - INFO - epoch complete!
2024-03-01 17:11:13,974 - INFO - evaluating now!
2024-03-01 17:11:25,913 - INFO - Epoch [91/300] (61548) train_loss: 26.3830, val_loss: 27.0021, lr: 0.000807, 160.51s
2024-03-01 17:13:55,509 - INFO - epoch complete!
2024-03-01 17:13:55,509 - INFO - evaluating now!
2024-03-01 17:14:07,262 - INFO - Epoch [92/300] (62217) train_loss: 26.4494, val_loss: 26.7846, lr: 0.000803, 161.35s
2024-03-01 17:16:40,939 - INFO - epoch complete!
2024-03-01 17:16:40,940 - INFO - evaluating now!
2024-03-01 17:16:51,989 - INFO - Epoch [93/300] (62886) train_loss: 26.3798, val_loss: 26.5158, lr: 0.000799, 164.73s
2024-03-01 17:19:27,299 - INFO - epoch complete!
2024-03-01 17:19:27,300 - INFO - evaluating now!
2024-03-01 17:19:40,074 - INFO - Epoch [94/300] (63555) train_loss: 26.1900, val_loss: 27.1334, lr: 0.000795, 168.08s
2024-03-01 17:22:11,922 - INFO - epoch complete!
2024-03-01 17:22:11,923 - INFO - evaluating now!
2024-03-01 17:22:24,278 - INFO - Epoch [95/300] (64224) train_loss: 26.1934, val_loss: 26.4153, lr: 0.000791, 164.20s
2024-03-01 17:22:24,329 - INFO - Saved model at 95
2024-03-01 17:22:24,330 - INFO - Val loss decrease from 26.4661 to 26.4153, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch95.tar
2024-03-01 17:24:52,933 - INFO - epoch complete!
2024-03-01 17:24:52,933 - INFO - evaluating now!
2024-03-01 17:25:04,726 - INFO - Epoch [96/300] (64893) train_loss: 26.0985, val_loss: 26.3326, lr: 0.000787, 160.40s
2024-03-01 17:25:04,779 - INFO - Saved model at 96
2024-03-01 17:25:04,779 - INFO - Val loss decrease from 26.4153 to 26.3326, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch96.tar
2024-03-01 17:27:31,274 - INFO - epoch complete!
2024-03-01 17:27:31,274 - INFO - evaluating now!
2024-03-01 17:27:43,388 - INFO - Epoch [97/300] (65562) train_loss: 26.1293, val_loss: 26.6348, lr: 0.000783, 158.61s
2024-03-01 17:30:10,182 - INFO - epoch complete!
2024-03-01 17:30:10,183 - INFO - evaluating now!
2024-03-01 17:30:21,917 - INFO - Epoch [98/300] (66231) train_loss: 26.1183, val_loss: 26.1530, lr: 0.000779, 158.53s
2024-03-01 17:30:21,968 - INFO - Saved model at 98
2024-03-01 17:30:21,968 - INFO - Val loss decrease from 26.3326 to 26.1530, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch98.tar
2024-03-01 17:32:48,040 - INFO - epoch complete!
2024-03-01 17:32:48,040 - INFO - evaluating now!
2024-03-01 17:32:59,797 - INFO - Epoch [99/300] (66900) train_loss: 26.0788, val_loss: 26.2542, lr: 0.000775, 157.83s
2024-03-01 17:35:25,883 - INFO - epoch complete!
2024-03-01 17:35:25,884 - INFO - evaluating now!
2024-03-01 17:35:37,693 - INFO - Epoch [100/300] (67569) train_loss: 26.0964, val_loss: 26.3227, lr: 0.000771, 157.90s
2024-03-01 17:38:04,337 - INFO - epoch complete!
2024-03-01 17:38:04,338 - INFO - evaluating now!
2024-03-01 17:38:16,513 - INFO - Epoch [101/300] (68238) train_loss: 25.9158, val_loss: 26.5176, lr: 0.000767, 158.82s
2024-03-01 17:40:47,893 - INFO - epoch complete!
2024-03-01 17:40:47,894 - INFO - evaluating now!
2024-03-01 17:40:59,957 - INFO - Epoch [102/300] (68907) train_loss: 26.0776, val_loss: 26.5155, lr: 0.000763, 163.44s
2024-03-01 17:43:25,073 - INFO - epoch complete!
2024-03-01 17:43:25,074 - INFO - evaluating now!
2024-03-01 17:43:37,079 - INFO - Epoch [103/300] (69576) train_loss: 26.0320, val_loss: 26.4231, lr: 0.000758, 157.12s
2024-03-01 17:46:02,329 - INFO - epoch complete!
2024-03-01 17:46:02,329 - INFO - evaluating now!
2024-03-01 17:46:13,724 - INFO - Epoch [104/300] (70245) train_loss: 25.9306, val_loss: 26.2602, lr: 0.000754, 156.64s
2024-03-01 17:48:38,424 - INFO - epoch complete!
2024-03-01 17:48:38,424 - INFO - evaluating now!
2024-03-01 17:48:50,457 - INFO - Epoch [105/300] (70914) train_loss: 25.9991, val_loss: 26.0059, lr: 0.000750, 156.73s
2024-03-01 17:48:50,507 - INFO - Saved model at 105
2024-03-01 17:48:50,507 - INFO - Val loss decrease from 26.1530 to 26.0059, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch105.tar
2024-03-01 17:51:19,159 - INFO - epoch complete!
2024-03-01 17:51:19,160 - INFO - evaluating now!
2024-03-01 17:51:30,625 - INFO - Epoch [106/300] (71583) train_loss: 25.8832, val_loss: 26.5542, lr: 0.000746, 160.12s
2024-03-01 17:53:59,033 - INFO - epoch complete!
2024-03-01 17:53:59,033 - INFO - evaluating now!
2024-03-01 17:54:11,195 - INFO - Epoch [107/300] (72252) train_loss: 25.9228, val_loss: 26.6961, lr: 0.000742, 160.57s
2024-03-01 17:56:39,166 - INFO - epoch complete!
2024-03-01 17:56:39,167 - INFO - evaluating now!
2024-03-01 17:56:51,279 - INFO - Epoch [108/300] (72921) train_loss: 25.8910, val_loss: 26.7927, lr: 0.000737, 160.08s
2024-03-01 17:59:18,709 - INFO - epoch complete!
2024-03-01 17:59:18,710 - INFO - evaluating now!
2024-03-01 17:59:30,169 - INFO - Epoch [109/300] (73590) train_loss: 25.8053, val_loss: 26.3864, lr: 0.000733, 158.89s
2024-03-01 18:01:56,858 - INFO - epoch complete!
2024-03-01 18:01:56,859 - INFO - evaluating now!
2024-03-01 18:02:08,650 - INFO - Epoch [110/300] (74259) train_loss: 25.7829, val_loss: 26.1081, lr: 0.000729, 158.48s
2024-03-01 18:04:36,506 - INFO - epoch complete!
2024-03-01 18:04:36,507 - INFO - evaluating now!
2024-03-01 18:04:48,571 - INFO - Epoch [111/300] (74928) train_loss: 25.7810, val_loss: 27.0627, lr: 0.000724, 159.92s
2024-03-01 18:07:14,438 - INFO - epoch complete!
2024-03-01 18:07:14,439 - INFO - evaluating now!
2024-03-01 18:07:26,471 - INFO - Epoch [112/300] (75597) train_loss: 25.7647, val_loss: 26.0641, lr: 0.000720, 157.90s
2024-03-01 18:09:54,031 - INFO - epoch complete!
2024-03-01 18:09:54,032 - INFO - evaluating now!
2024-03-01 18:10:06,051 - INFO - Epoch [113/300] (76266) train_loss: 25.6579, val_loss: 26.0265, lr: 0.000716, 159.58s
2024-03-01 18:12:33,593 - INFO - epoch complete!
2024-03-01 18:12:33,593 - INFO - evaluating now!
2024-03-01 18:12:45,698 - INFO - Epoch [114/300] (76935) train_loss: 25.6854, val_loss: 26.1199, lr: 0.000711, 159.65s
2024-03-01 18:15:13,060 - INFO - epoch complete!
2024-03-01 18:15:13,061 - INFO - evaluating now!
2024-03-01 18:15:25,204 - INFO - Epoch [115/300] (77604) train_loss: 25.7294, val_loss: 26.4400, lr: 0.000707, 159.51s
2024-03-01 18:17:52,547 - INFO - epoch complete!
2024-03-01 18:17:52,548 - INFO - evaluating now!
2024-03-01 18:18:04,651 - INFO - Epoch [116/300] (78273) train_loss: 25.6616, val_loss: 25.8975, lr: 0.000702, 159.45s
2024-03-01 18:18:04,703 - INFO - Saved model at 116
2024-03-01 18:18:04,703 - INFO - Val loss decrease from 26.0059 to 25.8975, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch116.tar
2024-03-01 18:20:31,193 - INFO - epoch complete!
2024-03-01 18:20:31,194 - INFO - evaluating now!
2024-03-01 18:20:43,274 - INFO - Epoch [117/300] (78942) train_loss: 25.6955, val_loss: 26.0936, lr: 0.000698, 158.57s
2024-03-01 18:23:09,584 - INFO - epoch complete!
2024-03-01 18:23:09,584 - INFO - evaluating now!
2024-03-01 18:23:21,706 - INFO - Epoch [118/300] (79611) train_loss: 25.6722, val_loss: 26.1779, lr: 0.000694, 158.43s
2024-03-01 18:25:50,688 - INFO - epoch complete!
2024-03-01 18:25:50,689 - INFO - evaluating now!
2024-03-01 18:26:02,947 - INFO - Epoch [119/300] (80280) train_loss: 25.5748, val_loss: 25.9168, lr: 0.000689, 161.24s
2024-03-01 18:28:37,417 - INFO - epoch complete!
2024-03-01 18:28:37,417 - INFO - evaluating now!
2024-03-01 18:28:49,578 - INFO - Epoch [120/300] (80949) train_loss: 25.5922, val_loss: 26.1218, lr: 0.000685, 166.63s
2024-03-01 18:31:24,435 - INFO - epoch complete!
2024-03-01 18:31:24,436 - INFO - evaluating now!
2024-03-01 18:31:36,579 - INFO - Epoch [121/300] (81618) train_loss: 25.5781, val_loss: 25.7562, lr: 0.000680, 167.00s
2024-03-01 18:31:36,632 - INFO - Saved model at 121
2024-03-01 18:31:36,632 - INFO - Val loss decrease from 25.8975 to 25.7562, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch121.tar
2024-03-01 18:34:08,914 - INFO - epoch complete!
2024-03-01 18:34:08,914 - INFO - evaluating now!
2024-03-01 18:34:20,462 - INFO - Epoch [122/300] (82287) train_loss: 25.4781, val_loss: 26.1208, lr: 0.000676, 163.83s
2024-03-01 18:36:47,787 - INFO - epoch complete!
2024-03-01 18:36:47,788 - INFO - evaluating now!
2024-03-01 18:36:59,461 - INFO - Epoch [123/300] (82956) train_loss: 25.4170, val_loss: 25.8325, lr: 0.000671, 159.00s
2024-03-01 18:39:25,009 - INFO - epoch complete!
2024-03-01 18:39:25,009 - INFO - evaluating now!
2024-03-01 18:39:36,393 - INFO - Epoch [124/300] (83625) train_loss: 25.5394, val_loss: 26.0540, lr: 0.000666, 156.93s
2024-03-01 18:42:10,241 - INFO - epoch complete!
2024-03-01 18:42:10,242 - INFO - evaluating now!
2024-03-01 18:42:22,209 - INFO - Epoch [125/300] (84294) train_loss: 25.3890, val_loss: 26.3517, lr: 0.000662, 165.82s
2024-03-01 18:44:48,926 - INFO - epoch complete!
2024-03-01 18:44:48,927 - INFO - evaluating now!
2024-03-01 18:45:00,335 - INFO - Epoch [126/300] (84963) train_loss: 25.4357, val_loss: 25.9154, lr: 0.000657, 158.13s
2024-03-01 18:47:28,232 - INFO - epoch complete!
2024-03-01 18:47:28,233 - INFO - evaluating now!
2024-03-01 18:47:39,934 - INFO - Epoch [127/300] (85632) train_loss: 25.3378, val_loss: 25.8847, lr: 0.000653, 159.60s
2024-03-01 18:50:12,221 - INFO - epoch complete!
2024-03-01 18:50:12,221 - INFO - evaluating now!
2024-03-01 18:50:23,870 - INFO - Epoch [128/300] (86301) train_loss: 25.3865, val_loss: 25.9077, lr: 0.000648, 163.93s
2024-03-01 18:52:49,847 - INFO - epoch complete!
2024-03-01 18:52:49,847 - INFO - evaluating now!
2024-03-01 18:53:01,516 - INFO - Epoch [129/300] (86970) train_loss: 25.3790, val_loss: 25.9373, lr: 0.000644, 157.65s
2024-03-01 18:55:28,522 - INFO - epoch complete!
2024-03-01 18:55:28,523 - INFO - evaluating now!
2024-03-01 18:55:40,206 - INFO - Epoch [130/300] (87639) train_loss: 25.2857, val_loss: 25.7815, lr: 0.000639, 158.69s
2024-03-01 18:58:11,639 - INFO - epoch complete!
2024-03-01 18:58:11,640 - INFO - evaluating now!
2024-03-01 18:58:23,624 - INFO - Epoch [131/300] (88308) train_loss: 25.3680, val_loss: 25.9781, lr: 0.000634, 163.42s
2024-03-01 19:00:54,667 - INFO - epoch complete!
2024-03-01 19:00:54,668 - INFO - evaluating now!
2024-03-01 19:01:06,675 - INFO - Epoch [132/300] (88977) train_loss: 25.2796, val_loss: 25.5495, lr: 0.000630, 163.05s
2024-03-01 19:01:06,732 - INFO - Saved model at 132
2024-03-01 19:01:06,733 - INFO - Val loss decrease from 25.7562 to 25.5495, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch132.tar
2024-03-01 19:03:36,220 - INFO - epoch complete!
2024-03-01 19:03:36,221 - INFO - evaluating now!
2024-03-01 19:03:48,342 - INFO - Epoch [133/300] (89646) train_loss: 25.2113, val_loss: 25.7923, lr: 0.000625, 161.61s
2024-03-01 19:06:24,420 - INFO - epoch complete!
2024-03-01 19:06:24,420 - INFO - evaluating now!
2024-03-01 19:06:36,622 - INFO - Epoch [134/300] (90315) train_loss: 25.2842, val_loss: 26.1144, lr: 0.000620, 168.28s
2024-03-01 19:09:06,302 - INFO - epoch complete!
2024-03-01 19:09:06,303 - INFO - evaluating now!
2024-03-01 19:09:18,468 - INFO - Epoch [135/300] (90984) train_loss: 25.1971, val_loss: 25.7527, lr: 0.000616, 161.85s
2024-03-01 19:11:49,109 - INFO - epoch complete!
2024-03-01 19:11:49,110 - INFO - evaluating now!
2024-03-01 19:12:01,312 - INFO - Epoch [136/300] (91653) train_loss: 25.1850, val_loss: 25.8648, lr: 0.000611, 162.84s
2024-03-01 19:14:31,249 - INFO - epoch complete!
2024-03-01 19:14:31,249 - INFO - evaluating now!
2024-03-01 19:14:43,305 - INFO - Epoch [137/300] (92322) train_loss: 25.2338, val_loss: 25.7854, lr: 0.000606, 161.99s
2024-03-01 19:17:16,553 - INFO - epoch complete!
2024-03-01 19:17:16,554 - INFO - evaluating now!
2024-03-01 19:17:28,478 - INFO - Epoch [138/300] (92991) train_loss: 25.1290, val_loss: 25.5467, lr: 0.000602, 165.17s
2024-03-01 19:17:28,530 - INFO - Saved model at 138
2024-03-01 19:17:28,531 - INFO - Val loss decrease from 25.5495 to 25.5467, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch138.tar
2024-03-01 19:19:56,391 - INFO - epoch complete!
2024-03-01 19:19:56,391 - INFO - evaluating now!
2024-03-01 19:20:08,497 - INFO - Epoch [139/300] (93660) train_loss: 25.0054, val_loss: 25.8380, lr: 0.000597, 159.97s
2024-03-01 19:22:37,371 - INFO - epoch complete!
2024-03-01 19:22:37,371 - INFO - evaluating now!
2024-03-01 19:22:49,492 - INFO - Epoch [140/300] (94329) train_loss: 25.0604, val_loss: 25.8237, lr: 0.000592, 160.99s
2024-03-01 19:25:26,328 - INFO - epoch complete!
2024-03-01 19:25:26,329 - INFO - evaluating now!
2024-03-01 19:25:38,436 - INFO - Epoch [141/300] (94998) train_loss: 25.0343, val_loss: 25.8335, lr: 0.000588, 168.94s
2024-03-01 19:28:07,399 - INFO - epoch complete!
2024-03-01 19:28:07,399 - INFO - evaluating now!
2024-03-01 19:28:19,248 - INFO - Epoch [142/300] (95667) train_loss: 25.0928, val_loss: 25.8053, lr: 0.000583, 160.81s
2024-03-01 19:30:53,877 - INFO - epoch complete!
2024-03-01 19:30:53,877 - INFO - evaluating now!
2024-03-01 19:31:05,206 - INFO - Epoch [143/300] (96336) train_loss: 25.1152, val_loss: 25.8268, lr: 0.000578, 165.96s
2024-03-01 19:33:35,070 - INFO - epoch complete!
2024-03-01 19:33:35,071 - INFO - evaluating now!
2024-03-01 19:33:46,934 - INFO - Epoch [144/300] (97005) train_loss: 25.0063, val_loss: 25.6503, lr: 0.000574, 161.73s
2024-03-01 19:36:16,611 - INFO - epoch complete!
2024-03-01 19:36:16,611 - INFO - evaluating now!
2024-03-01 19:36:28,733 - INFO - Epoch [145/300] (97674) train_loss: 24.8899, val_loss: 26.1059, lr: 0.000569, 161.80s
2024-03-01 19:39:01,411 - INFO - epoch complete!
2024-03-01 19:39:01,412 - INFO - evaluating now!
2024-03-01 19:39:13,593 - INFO - Epoch [146/300] (98343) train_loss: 24.9916, val_loss: 26.0710, lr: 0.000564, 164.86s
2024-03-01 19:41:40,329 - INFO - epoch complete!
2024-03-01 19:41:40,330 - INFO - evaluating now!
2024-03-01 19:41:52,501 - INFO - Epoch [147/300] (99012) train_loss: 25.0158, val_loss: 26.1660, lr: 0.000559, 158.91s
2024-03-01 19:44:22,656 - INFO - epoch complete!
2024-03-01 19:44:22,657 - INFO - evaluating now!
2024-03-01 19:44:34,862 - INFO - Epoch [148/300] (99681) train_loss: 24.9341, val_loss: 25.8363, lr: 0.000555, 162.36s
2024-03-01 19:47:06,335 - INFO - epoch complete!
2024-03-01 19:47:06,336 - INFO - evaluating now!
2024-03-01 19:47:18,423 - INFO - Epoch [149/300] (100350) train_loss: 24.8406, val_loss: 25.5474, lr: 0.000550, 163.56s
2024-03-01 19:49:51,317 - INFO - epoch complete!
2024-03-01 19:49:51,317 - INFO - evaluating now!
2024-03-01 19:50:03,516 - INFO - Epoch [150/300] (101019) train_loss: 24.8214, val_loss: 26.2450, lr: 0.000545, 165.09s
2024-03-01 19:52:34,241 - INFO - epoch complete!
2024-03-01 19:52:34,242 - INFO - evaluating now!
2024-03-01 19:52:46,497 - INFO - Epoch [151/300] (101688) train_loss: 24.8318, val_loss: 25.8200, lr: 0.000541, 162.98s
2024-03-01 19:55:16,475 - INFO - epoch complete!
2024-03-01 19:55:16,475 - INFO - evaluating now!
2024-03-01 19:55:28,600 - INFO - Epoch [152/300] (102357) train_loss: 24.8187, val_loss: 25.5980, lr: 0.000536, 162.10s
2024-03-01 19:57:58,458 - INFO - epoch complete!
2024-03-01 19:57:58,459 - INFO - evaluating now!
2024-03-01 19:58:10,806 - INFO - Epoch [153/300] (103026) train_loss: 24.8395, val_loss: 25.6059, lr: 0.000531, 162.20s
2024-03-01 20:00:47,346 - INFO - epoch complete!
2024-03-01 20:00:47,347 - INFO - evaluating now!
2024-03-01 20:00:59,699 - INFO - Epoch [154/300] (103695) train_loss: 24.7834, val_loss: 25.9506, lr: 0.000526, 168.89s
2024-03-01 20:03:28,977 - INFO - epoch complete!
2024-03-01 20:03:28,978 - INFO - evaluating now!
2024-03-01 20:03:40,596 - INFO - Epoch [155/300] (104364) train_loss: 24.8169, val_loss: 25.7367, lr: 0.000522, 160.90s
2024-03-01 20:06:08,317 - INFO - epoch complete!
2024-03-01 20:06:08,318 - INFO - evaluating now!
2024-03-01 20:06:20,470 - INFO - Epoch [156/300] (105033) train_loss: 24.7386, val_loss: 26.1247, lr: 0.000517, 159.87s
2024-03-01 20:08:55,490 - INFO - epoch complete!
2024-03-01 20:08:55,491 - INFO - evaluating now!
2024-03-01 20:09:07,905 - INFO - Epoch [157/300] (105702) train_loss: 24.7241, val_loss: 25.8433, lr: 0.000512, 167.43s
2024-03-01 20:11:46,977 - INFO - epoch complete!
2024-03-01 20:11:46,978 - INFO - evaluating now!
2024-03-01 20:11:59,140 - INFO - Epoch [158/300] (106371) train_loss: 24.7435, val_loss: 25.5180, lr: 0.000508, 171.23s
2024-03-01 20:11:59,191 - INFO - Saved model at 158
2024-03-01 20:11:59,191 - INFO - Val loss decrease from 25.5467 to 25.5180, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch158.tar
2024-03-01 20:14:30,950 - INFO - epoch complete!
2024-03-01 20:14:30,950 - INFO - evaluating now!
2024-03-01 20:14:43,033 - INFO - Epoch [159/300] (107040) train_loss: 24.7515, val_loss: 25.5119, lr: 0.000503, 163.84s
2024-03-01 20:14:43,087 - INFO - Saved model at 159
2024-03-01 20:14:43,087 - INFO - Val loss decrease from 25.5180 to 25.5119, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch159.tar
2024-03-01 20:17:12,998 - INFO - epoch complete!
2024-03-01 20:17:12,999 - INFO - evaluating now!
2024-03-01 20:17:25,185 - INFO - Epoch [160/300] (107709) train_loss: 24.6504, val_loss: 25.4812, lr: 0.000498, 162.10s
2024-03-01 20:17:25,237 - INFO - Saved model at 160
2024-03-01 20:17:25,237 - INFO - Val loss decrease from 25.5119 to 25.4812, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch160.tar
2024-03-01 20:19:56,182 - INFO - epoch complete!
2024-03-01 20:19:56,182 - INFO - evaluating now!
2024-03-01 20:20:08,047 - INFO - Epoch [161/300] (108378) train_loss: 24.6391, val_loss: 25.7071, lr: 0.000494, 162.81s
2024-03-01 20:22:39,052 - INFO - epoch complete!
2024-03-01 20:22:39,052 - INFO - evaluating now!
2024-03-01 20:22:50,105 - INFO - Epoch [162/300] (109047) train_loss: 24.6384, val_loss: 25.7268, lr: 0.000489, 162.06s
2024-03-01 20:25:28,627 - INFO - epoch complete!
2024-03-01 20:25:28,628 - INFO - evaluating now!
2024-03-01 20:25:40,507 - INFO - Epoch [163/300] (109716) train_loss: 24.6114, val_loss: 25.8503, lr: 0.000484, 170.40s
2024-03-01 20:28:12,747 - INFO - epoch complete!
2024-03-01 20:28:12,748 - INFO - evaluating now!
2024-03-01 20:28:24,911 - INFO - Epoch [164/300] (110385) train_loss: 24.5539, val_loss: 25.5983, lr: 0.000480, 164.40s
2024-03-01 20:30:54,752 - INFO - epoch complete!
2024-03-01 20:30:54,752 - INFO - evaluating now!
2024-03-01 20:31:07,028 - INFO - Epoch [165/300] (111054) train_loss: 24.5604, val_loss: 25.5550, lr: 0.000475, 162.12s
2024-03-01 20:33:37,547 - INFO - epoch complete!
2024-03-01 20:33:37,548 - INFO - evaluating now!
2024-03-01 20:33:49,569 - INFO - Epoch [166/300] (111723) train_loss: 24.5476, val_loss: 25.5676, lr: 0.000470, 162.54s
2024-03-01 20:36:18,634 - INFO - epoch complete!
2024-03-01 20:36:18,635 - INFO - evaluating now!
2024-03-01 20:36:30,858 - INFO - Epoch [167/300] (112392) train_loss: 24.5042, val_loss: 25.2807, lr: 0.000466, 161.29s
2024-03-01 20:36:30,908 - INFO - Saved model at 167
2024-03-01 20:36:30,908 - INFO - Val loss decrease from 25.4812 to 25.2807, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch167.tar
2024-03-01 20:39:05,273 - INFO - epoch complete!
2024-03-01 20:39:05,274 - INFO - evaluating now!
2024-03-01 20:39:17,325 - INFO - Epoch [168/300] (113061) train_loss: 24.4696, val_loss: 25.6294, lr: 0.000461, 166.42s
2024-03-01 20:41:47,984 - INFO - epoch complete!
2024-03-01 20:41:47,985 - INFO - evaluating now!
2024-03-01 20:42:00,207 - INFO - Epoch [169/300] (113730) train_loss: 24.4352, val_loss: 25.8182, lr: 0.000456, 162.88s
2024-03-01 20:44:35,487 - INFO - epoch complete!
2024-03-01 20:44:35,487 - INFO - evaluating now!
2024-03-01 20:44:47,508 - INFO - Epoch [170/300] (114399) train_loss: 24.4765, val_loss: 26.0398, lr: 0.000452, 167.30s
2024-03-01 20:47:18,819 - INFO - epoch complete!
2024-03-01 20:47:18,819 - INFO - evaluating now!
2024-03-01 20:47:31,015 - INFO - Epoch [171/300] (115068) train_loss: 24.4633, val_loss: 26.7974, lr: 0.000447, 163.51s
2024-03-01 20:50:05,593 - INFO - epoch complete!
2024-03-01 20:50:05,593 - INFO - evaluating now!
2024-03-01 20:50:17,202 - INFO - Epoch [172/300] (115737) train_loss: 24.4375, val_loss: 25.4595, lr: 0.000443, 166.19s
2024-03-01 20:52:50,769 - INFO - epoch complete!
2024-03-01 20:52:50,770 - INFO - evaluating now!
2024-03-01 20:53:03,005 - INFO - Epoch [173/300] (116406) train_loss: 24.3687, val_loss: 25.4365, lr: 0.000438, 165.80s
2024-03-01 20:55:31,685 - INFO - epoch complete!
2024-03-01 20:55:31,685 - INFO - evaluating now!
2024-03-01 20:55:43,382 - INFO - Epoch [174/300] (117075) train_loss: 24.3741, val_loss: 25.4759, lr: 0.000434, 160.38s
2024-03-01 20:58:16,588 - INFO - epoch complete!
2024-03-01 20:58:16,589 - INFO - evaluating now!
2024-03-01 20:58:28,784 - INFO - Epoch [175/300] (117744) train_loss: 24.3405, val_loss: 25.6203, lr: 0.000429, 165.40s
2024-03-01 21:01:00,696 - INFO - epoch complete!
2024-03-01 21:01:00,696 - INFO - evaluating now!
2024-03-01 21:01:13,139 - INFO - Epoch [176/300] (118413) train_loss: 24.3120, val_loss: 25.4000, lr: 0.000424, 164.35s
2024-03-01 21:03:42,889 - INFO - epoch complete!
2024-03-01 21:03:42,890 - INFO - evaluating now!
2024-03-01 21:03:54,945 - INFO - Epoch [177/300] (119082) train_loss: 24.3146, val_loss: 25.3675, lr: 0.000420, 161.80s
2024-03-01 21:06:26,246 - INFO - epoch complete!
2024-03-01 21:06:26,246 - INFO - evaluating now!
2024-03-01 21:06:37,811 - INFO - Epoch [178/300] (119751) train_loss: 24.2645, val_loss: 25.3466, lr: 0.000415, 162.87s
2024-03-01 21:09:08,111 - INFO - epoch complete!
2024-03-01 21:09:08,112 - INFO - evaluating now!
2024-03-01 21:09:19,158 - INFO - Epoch [179/300] (120420) train_loss: 24.2726, val_loss: 25.5009, lr: 0.000411, 161.35s
2024-03-01 21:11:50,660 - INFO - epoch complete!
2024-03-01 21:11:50,661 - INFO - evaluating now!
2024-03-01 21:12:02,458 - INFO - Epoch [180/300] (121089) train_loss: 24.2282, val_loss: 25.4643, lr: 0.000406, 163.30s
2024-03-01 21:14:34,454 - INFO - epoch complete!
2024-03-01 21:14:34,454 - INFO - evaluating now!
2024-03-01 21:14:46,716 - INFO - Epoch [181/300] (121758) train_loss: 24.1897, val_loss: 25.4380, lr: 0.000402, 164.26s
2024-03-01 21:17:18,568 - INFO - epoch complete!
2024-03-01 21:17:18,569 - INFO - evaluating now!
2024-03-01 21:17:30,887 - INFO - Epoch [182/300] (122427) train_loss: 24.1860, val_loss: 25.1597, lr: 0.000398, 164.17s
2024-03-01 21:17:30,939 - INFO - Saved model at 182
2024-03-01 21:17:30,940 - INFO - Val loss decrease from 25.2807 to 25.1597, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch182.tar
2024-03-01 21:20:08,200 - INFO - epoch complete!
2024-03-01 21:20:08,201 - INFO - evaluating now!
2024-03-01 21:20:20,500 - INFO - Epoch [183/300] (123096) train_loss: 24.1695, val_loss: 25.4605, lr: 0.000393, 169.56s
2024-03-01 21:22:54,201 - INFO - epoch complete!
2024-03-01 21:22:54,202 - INFO - evaluating now!
2024-03-01 21:23:06,021 - INFO - Epoch [184/300] (123765) train_loss: 24.1900, val_loss: 25.2924, lr: 0.000389, 165.52s
2024-03-01 21:25:38,553 - INFO - epoch complete!
2024-03-01 21:25:38,554 - INFO - evaluating now!
2024-03-01 21:25:50,826 - INFO - Epoch [185/300] (124434) train_loss: 24.1241, val_loss: 25.3095, lr: 0.000384, 164.81s
2024-03-01 21:28:28,771 - INFO - epoch complete!
2024-03-01 21:28:28,772 - INFO - evaluating now!
2024-03-01 21:28:40,847 - INFO - Epoch [186/300] (125103) train_loss: 24.1210, val_loss: 25.1113, lr: 0.000380, 170.02s
2024-03-01 21:28:40,898 - INFO - Saved model at 186
2024-03-01 21:28:40,899 - INFO - Val loss decrease from 25.1597 to 25.1113, saving to ./libcity/cache/7058/model_cache/PDFormer_PeMS08_epoch186.tar
2024-03-01 21:31:09,538 - INFO - epoch complete!
2024-03-01 21:31:09,539 - INFO - evaluating now!
2024-03-01 21:31:21,518 - INFO - Epoch [187/300] (125772) train_loss: 24.1053, val_loss: 25.5220, lr: 0.000376, 160.62s
2024-03-01 21:33:50,570 - INFO - epoch complete!
2024-03-01 21:33:50,571 - INFO - evaluating now!
2024-03-01 21:34:02,756 - INFO - Epoch [188/300] (126441) train_loss: 24.1065, val_loss: 25.4143, lr: 0.000371, 161.24s
2024-03-01 21:36:37,008 - INFO - epoch complete!
2024-03-01 21:36:37,009 - INFO - evaluating now!
2024-03-01 21:36:48,945 - INFO - Epoch [189/300] (127110) train_loss: 24.0513, val_loss: 25.3442, lr: 0.000367, 166.19s
2024-03-01 21:39:20,305 - INFO - epoch complete!
2024-03-01 21:39:20,306 - INFO - evaluating now!
2024-03-01 21:39:32,350 - INFO - Epoch [190/300] (127779) train_loss: 24.0512, val_loss: 25.3555, lr: 0.000363, 163.40s
2024-03-01 21:42:01,074 - INFO - epoch complete!
2024-03-01 21:42:01,074 - INFO - evaluating now!
2024-03-01 21:42:12,462 - INFO - Epoch [191/300] (128448) train_loss: 24.0325, val_loss: 25.4584, lr: 0.000358, 160.11s
2024-03-01 21:44:42,684 - INFO - epoch complete!
2024-03-01 21:44:42,684 - INFO - evaluating now!
2024-03-01 21:44:54,636 - INFO - Epoch [192/300] (129117) train_loss: 24.0139, val_loss: 25.1921, lr: 0.000354, 162.17s
2024-03-01 21:47:22,552 - INFO - epoch complete!
2024-03-01 21:47:22,553 - INFO - evaluating now!
2024-03-01 21:47:34,639 - INFO - Epoch [193/300] (129786) train_loss: 24.0414, val_loss: 25.3013, lr: 0.000350, 160.00s
2024-03-01 21:50:04,164 - INFO - epoch complete!
2024-03-01 21:50:04,165 - INFO - evaluating now!
2024-03-01 21:50:15,590 - INFO - Epoch [194/300] (130455) train_loss: 23.9965, val_loss: 25.2384, lr: 0.000346, 160.95s
2024-03-01 21:52:45,197 - INFO - epoch complete!
2024-03-01 21:52:45,197 - INFO - evaluating now!
2024-03-01 21:52:57,307 - INFO - Epoch [195/300] (131124) train_loss: 23.9775, val_loss: 25.2147, lr: 0.000342, 161.72s
2024-03-01 21:55:23,944 - INFO - epoch complete!
2024-03-01 21:55:23,945 - INFO - evaluating now!
2024-03-01 21:55:35,611 - INFO - Epoch [196/300] (131793) train_loss: 23.9107, val_loss: 25.3072, lr: 0.000337, 158.30s
2024-03-01 21:58:01,132 - INFO - epoch complete!
2024-03-01 21:58:01,132 - INFO - evaluating now!
2024-03-01 21:58:13,212 - INFO - Epoch [197/300] (132462) train_loss: 23.9388, val_loss: 25.3753, lr: 0.000333, 157.60s
2024-03-01 22:00:40,016 - INFO - epoch complete!
2024-03-01 22:00:40,016 - INFO - evaluating now!
2024-03-01 22:00:51,484 - INFO - Epoch [198/300] (133131) train_loss: 23.8810, val_loss: 25.4533, lr: 0.000329, 158.27s
2024-03-01 22:03:18,467 - INFO - epoch complete!
2024-03-01 22:03:18,468 - INFO - evaluating now!
2024-03-01 22:03:30,553 - INFO - Epoch [199/300] (133800) train_loss: 23.8847, val_loss: 25.2947, lr: 0.000325, 159.07s
2024-03-01 22:05:59,160 - INFO - epoch complete!
2024-03-01 22:05:59,161 - INFO - evaluating now!
2024-03-01 22:06:11,181 - INFO - Epoch [200/300] (134469) train_loss: 23.8342, val_loss: 25.3735, lr: 0.000321, 160.63s
2024-03-01 22:08:38,748 - INFO - epoch complete!
2024-03-01 22:08:38,749 - INFO - evaluating now!
2024-03-01 22:08:50,814 - INFO - Epoch [201/300] (135138) train_loss: 23.8480, val_loss: 25.3625, lr: 0.000317, 159.63s
2024-03-01 22:11:18,625 - INFO - epoch complete!
2024-03-01 22:11:18,626 - INFO - evaluating now!
2024-03-01 22:11:30,560 - INFO - Epoch [202/300] (135807) train_loss: 23.8040, val_loss: 25.6926, lr: 0.000313, 159.75s
2024-03-01 22:13:57,504 - INFO - epoch complete!
2024-03-01 22:13:57,505 - INFO - evaluating now!
2024-03-01 22:14:09,149 - INFO - Epoch [203/300] (136476) train_loss: 23.7901, val_loss: 25.1789, lr: 0.000309, 158.59s
2024-03-01 22:16:34,715 - INFO - epoch complete!
2024-03-01 22:16:34,715 - INFO - evaluating now!
2024-03-01 22:16:46,183 - INFO - Epoch [204/300] (137145) train_loss: 23.8112, val_loss: 25.4637, lr: 0.000305, 157.03s
2024-03-01 22:19:13,085 - INFO - epoch complete!
2024-03-01 22:19:13,085 - INFO - evaluating now!
2024-03-01 22:19:25,222 - INFO - Epoch [205/300] (137814) train_loss: 23.7594, val_loss: 25.3069, lr: 0.000301, 159.04s
2024-03-01 22:22:00,627 - INFO - epoch complete!
2024-03-01 22:22:00,628 - INFO - evaluating now!
2024-03-01 22:22:12,565 - INFO - Epoch [206/300] (138483) train_loss: 23.7585, val_loss: 25.4283, lr: 0.000297, 167.34s
2024-03-01 22:24:40,506 - INFO - epoch complete!
2024-03-01 22:24:40,507 - INFO - evaluating now!
2024-03-01 22:24:52,505 - INFO - Epoch [207/300] (139152) train_loss: 23.7295, val_loss: 25.1997, lr: 0.000293, 159.94s
2024-03-01 22:27:19,587 - INFO - epoch complete!
2024-03-01 22:27:19,587 - INFO - evaluating now!
2024-03-01 22:27:31,499 - INFO - Epoch [208/300] (139821) train_loss: 23.7226, val_loss: 25.3000, lr: 0.000289, 158.99s
2024-03-01 22:29:57,908 - INFO - epoch complete!
2024-03-01 22:29:57,909 - INFO - evaluating now!
2024-03-01 22:30:09,809 - INFO - Epoch [209/300] (140490) train_loss: 23.6940, val_loss: 25.1579, lr: 0.000285, 158.31s
2024-03-01 22:32:43,958 - INFO - epoch complete!
2024-03-01 22:32:43,959 - INFO - evaluating now!
2024-03-01 22:32:55,017 - INFO - Epoch [210/300] (141159) train_loss: 23.6853, val_loss: 25.2941, lr: 0.000282, 165.21s
2024-03-01 22:35:27,103 - INFO - epoch complete!
2024-03-01 22:35:27,104 - INFO - evaluating now!
2024-03-01 22:35:39,012 - INFO - Epoch [211/300] (141828) train_loss: 23.6517, val_loss: 25.4256, lr: 0.000278, 163.99s
2024-03-01 22:38:07,032 - INFO - epoch complete!
2024-03-01 22:38:07,033 - INFO - evaluating now!
2024-03-01 22:38:18,527 - INFO - Epoch [212/300] (142497) train_loss: 23.6325, val_loss: 25.2047, lr: 0.000274, 159.51s
2024-03-01 22:40:44,115 - INFO - epoch complete!
2024-03-01 22:40:44,115 - INFO - evaluating now!
2024-03-01 22:40:55,281 - INFO - Epoch [213/300] (143166) train_loss: 23.6408, val_loss: 25.3391, lr: 0.000270, 156.75s
2024-03-01 22:43:23,224 - INFO - epoch complete!
2024-03-01 22:43:23,225 - INFO - evaluating now!
2024-03-01 22:43:34,401 - INFO - Epoch [214/300] (143835) train_loss: 23.5907, val_loss: 25.2842, lr: 0.000267, 159.12s
2024-03-01 22:46:01,730 - INFO - epoch complete!
2024-03-01 22:46:01,730 - INFO - evaluating now!
2024-03-01 22:46:12,795 - INFO - Epoch [215/300] (144504) train_loss: 23.6166, val_loss: 25.3761, lr: 0.000263, 158.39s
2024-03-01 22:48:40,949 - INFO - epoch complete!
2024-03-01 22:48:40,950 - INFO - evaluating now!
2024-03-01 22:48:52,129 - INFO - Epoch [216/300] (145173) train_loss: 23.6153, val_loss: 25.3608, lr: 0.000260, 159.33s
2024-03-01 22:51:24,351 - INFO - epoch complete!
2024-03-01 22:51:24,352 - INFO - evaluating now!
2024-03-01 22:51:36,252 - INFO - Epoch [217/300] (145842) train_loss: 23.5460, val_loss: 25.3168, lr: 0.000256, 164.12s
2024-03-01 22:54:06,340 - INFO - epoch complete!
2024-03-01 22:54:06,340 - INFO - evaluating now!
2024-03-01 22:54:18,597 - INFO - Epoch [218/300] (146511) train_loss: 23.5489, val_loss: 25.3335, lr: 0.000252, 162.35s
2024-03-01 22:56:47,896 - INFO - epoch complete!
2024-03-01 22:56:47,897 - INFO - evaluating now!
2024-03-01 22:57:00,096 - INFO - Epoch [219/300] (147180) train_loss: 23.5370, val_loss: 25.3701, lr: 0.000249, 161.50s
2024-03-01 22:59:28,054 - INFO - epoch complete!
2024-03-01 22:59:28,055 - INFO - evaluating now!
2024-03-01 22:59:40,297 - INFO - Epoch [220/300] (147849) train_loss: 23.5606, val_loss: 25.6278, lr: 0.000245, 160.20s
2024-03-01 23:02:08,594 - INFO - epoch complete!
2024-03-01 23:02:08,595 - INFO - evaluating now!
2024-03-01 23:02:20,835 - INFO - Epoch [221/300] (148518) train_loss: 23.4894, val_loss: 25.1769, lr: 0.000242, 160.54s
2024-03-01 23:04:52,433 - INFO - epoch complete!
2024-03-01 23:04:52,433 - INFO - evaluating now!
2024-03-01 23:05:04,689 - INFO - Epoch [222/300] (149187) train_loss: 23.4670, val_loss: 25.2487, lr: 0.000239, 163.85s
2024-03-01 23:07:32,399 - INFO - epoch complete!
2024-03-01 23:07:32,399 - INFO - evaluating now!
2024-03-01 23:07:44,067 - INFO - Epoch [223/300] (149856) train_loss: 23.4893, val_loss: 25.1899, lr: 0.000235, 159.38s
2024-03-01 23:10:13,644 - INFO - epoch complete!
2024-03-01 23:10:13,645 - INFO - evaluating now!
2024-03-01 23:10:25,900 - INFO - Epoch [224/300] (150525) train_loss: 23.4574, val_loss: 25.3650, lr: 0.000232, 161.83s
2024-03-01 23:12:55,656 - INFO - epoch complete!
2024-03-01 23:12:55,657 - INFO - evaluating now!
2024-03-01 23:13:07,830 - INFO - Epoch [225/300] (151194) train_loss: 23.4171, val_loss: 25.3951, lr: 0.000228, 161.93s
2024-03-01 23:15:37,933 - INFO - epoch complete!
2024-03-01 23:15:37,934 - INFO - evaluating now!
2024-03-01 23:15:50,203 - INFO - Epoch [226/300] (151863) train_loss: 23.4397, val_loss: 25.3740, lr: 0.000225, 162.37s
2024-03-01 23:18:19,839 - INFO - epoch complete!
2024-03-01 23:18:19,840 - INFO - evaluating now!
2024-03-01 23:18:32,023 - INFO - Epoch [227/300] (152532) train_loss: 23.3773, val_loss: 25.2795, lr: 0.000222, 161.82s
2024-03-01 23:20:59,232 - INFO - epoch complete!
2024-03-01 23:20:59,232 - INFO - evaluating now!
2024-03-01 23:21:11,337 - INFO - Epoch [228/300] (153201) train_loss: 23.4254, val_loss: 25.2242, lr: 0.000219, 159.31s
2024-03-01 23:23:40,452 - INFO - epoch complete!
2024-03-01 23:23:40,453 - INFO - evaluating now!
2024-03-01 23:23:52,680 - INFO - Epoch [229/300] (153870) train_loss: 23.3709, val_loss: 25.2863, lr: 0.000216, 161.34s
2024-03-01 23:26:19,138 - INFO - epoch complete!
2024-03-01 23:26:19,139 - INFO - evaluating now!
2024-03-01 23:26:30,884 - INFO - Epoch [230/300] (154539) train_loss: 23.3680, val_loss: 25.2190, lr: 0.000212, 158.20s
2024-03-01 23:28:57,287 - INFO - epoch complete!
2024-03-01 23:28:57,288 - INFO - evaluating now!
2024-03-01 23:29:09,117 - INFO - Epoch [231/300] (155208) train_loss: 23.3448, val_loss: 25.3126, lr: 0.000209, 158.23s
2024-03-01 23:31:39,523 - INFO - epoch complete!
2024-03-01 23:31:39,523 - INFO - evaluating now!
2024-03-01 23:31:51,608 - INFO - Epoch [232/300] (155877) train_loss: 23.3242, val_loss: 25.2396, lr: 0.000206, 162.49s
2024-03-01 23:34:19,105 - INFO - epoch complete!
2024-03-01 23:34:19,106 - INFO - evaluating now!
2024-03-01 23:34:30,489 - INFO - Epoch [233/300] (156546) train_loss: 23.3253, val_loss: 25.2973, lr: 0.000203, 158.88s
2024-03-01 23:36:55,509 - INFO - epoch complete!
2024-03-01 23:36:55,509 - INFO - evaluating now!
2024-03-01 23:37:07,385 - INFO - Epoch [234/300] (157215) train_loss: 23.3080, val_loss: 25.3144, lr: 0.000200, 156.90s
2024-03-01 23:39:33,839 - INFO - epoch complete!
2024-03-01 23:39:33,839 - INFO - evaluating now!
2024-03-01 23:39:45,284 - INFO - Epoch [235/300] (157884) train_loss: 23.2998, val_loss: 25.3658, lr: 0.000197, 157.90s
2024-03-01 23:42:10,729 - INFO - epoch complete!
2024-03-01 23:42:10,730 - INFO - evaluating now!
2024-03-01 23:42:22,830 - INFO - Epoch [236/300] (158553) train_loss: 23.3183, val_loss: 25.2229, lr: 0.000194, 157.55s
2024-03-01 23:42:22,830 - WARNING - Early stopping at epoch: 236
2024-03-01 23:42:22,830 - INFO - Trained totally 237 epochs, average train time is 148.590s, average eval time is 11.844s
2024-03-01 23:42:23,136 - INFO - Loaded model at 186
2024-03-01 23:42:23,137 - INFO - Saved model at ./libcity/cache/7058/model_cache/PDFormer_PeMS08.m
2024-03-01 23:42:23,190 - INFO - Start evaluating ...
2024-03-01 23:42:42,222 - INFO - Note that you select the average mode to evaluate!
2024-03-01 23:42:42,230 - INFO - Evaluate result is saved at ./libcity/cache/7058/evaluate_cache/2024_03_01_23_42_42_PDFormer_PeMS08_average.csv
2024-03-01 23:42:42,237 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   11.840948   inf  19.836498   11.856964     0.079086    19.733356
2   11.980822   inf  20.278234   11.996642     0.080506    20.175316
3   12.135580   inf  20.696148   12.151730     0.081575    20.594456
4   12.295077   inf  21.089977   12.311671     0.082690    20.990673
5   12.437845   inf  21.443478   12.454928     0.083617    21.346325
6   12.570986   inf  21.758965   12.588469     0.084496    21.663258
7   12.705440   inf  22.052288   12.723323     0.085339    21.957899
8   12.832235   inf  22.318932   12.850442     0.086086    22.225386
9   12.949749   inf  22.565413   12.968230     0.086810    22.472195
10  13.064409   inf  22.795862   13.083184     0.087527    22.703339
11  13.177077   inf  23.016897   13.196041     0.088289    22.924835
12  13.295561   inf  23.228104   13.314815     0.089105    23.136616
