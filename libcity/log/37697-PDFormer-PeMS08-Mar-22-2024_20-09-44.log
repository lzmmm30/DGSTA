2024-03-22 20:09:44,194 - INFO - Log directory: ./libcity/log
2024-03-22 20:09:44,194 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS08, exp_id=37697
2024-03-22 20:09:44,195 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS08', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 2776, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 21, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 6, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS08'], 'geo_file': 'PeMS08', 'rel_file': 'PeMS08', 'adp_file': 'PeMS08', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 37697}
2024-03-22 20:09:44,810 - INFO - Loaded file PeMS08.geo, num_nodes=170
2024-03-22 20:09:44,813 - INFO - set_weight_link_or_dist: link
2024-03-22 20:09:44,814 - INFO - init_weight_inf_or_zero: zero
2024-03-22 20:09:44,818 - INFO - Loaded file PeMS08.rel, shape=(170, 170)
2024-03-22 20:09:44,818 - INFO - Max adj_mx value = 1.0
2024-03-22 20:10:10,065 - INFO - Loading file PeMS08.dyna
2024-03-22 20:10:14,586 - INFO - Loaded file PeMS08.dyna, shape=(17856, 170, 1)
2024-03-22 20:10:14,628 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS08.npy
2024-03-22 20:10:14,629 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS08_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-03-22 20:10:30,555 - INFO - train	x: (10700, 12, 170, 9), y: (10700, 12, 170, 9), ind: (10700,)
2024-03-22 20:10:30,555 - INFO - eval	x: (3566, 12, 170, 9), y: (3566, 12, 170, 9), ind: (3566,)
2024-03-22 20:10:30,555 - INFO - test	x: (3567, 12, 170, 9), y: (3567, 12, 170, 9), ind: (3567,)
2024-03-22 20:10:31,717 - INFO - StandardScaler mean: 229.8431355598314, std: 145.62553066568907
2024-03-22 20:10:31,717 - INFO - NoneScaler
2024-03-22 20:10:33,341 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS08_21_3_16_5.npy
2024-03-22 20:10:33,358 - INFO - Use use_curriculum_learning!
2024-03-22 20:10:40,469 - INFO - Number of isolated points: 0
2024-03-22 20:10:40,495 - INFO - Number of isolated points: 0
2024-03-22 20:10:40,623 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-03-22 20:10:40,629 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-03-22 20:10:40,629 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,629 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-03-22 20:10:40,629 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-22 20:10:40,630 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,631 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,632 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-22 20:10:40,633 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,634 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,635 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,636 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-22 20:10:40,637 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,638 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,639 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-22 20:10:40,640 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,641 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-22 20:10:40,642 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,643 - INFO - encoder_blocks.5.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,644 - INFO - encoder_blocks.5.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - encoder_blocks.5.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - encoder_blocks.5.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - encoder_blocks.5.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - encoder_blocks.5.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,645 - INFO - skip_convs.5.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-22 20:10:40,646 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2024-03-22 20:10:40,646 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-03-22 20:10:40,646 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-03-22 20:10:40,646 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-03-22 20:10:40,646 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-03-22 20:10:40,647 - INFO - Total parameter numbers: 1104093
2024-03-22 20:10:40,652 - INFO - You select `adamw` optimizer.
2024-03-22 20:10:40,654 - INFO - You select `cosinelr` lr_scheduler.
2024-03-22 20:10:40,654 - WARNING - Received none train loss func and will use the loss func defined in the model.module.
2024-03-22 20:10:40,656 - INFO - Number of isolated points: 0
2024-03-22 20:10:40,689 - INFO - Start training ...
2024-03-22 20:10:40,689 - INFO - num_batches:669
2024-03-22 20:10:40,841 - INFO - Training: task_level increase from 0 to 1
2024-03-22 20:10:40,841 - INFO - Current batches_seen is 0
2024-03-22 20:12:43,398 - INFO - epoch complete!
2024-03-22 20:12:43,399 - INFO - evaluating now!
2024-03-22 20:12:53,855 - INFO - Epoch [0/300] (669) train_loss: 249.6575, val_loss: 249.6504, lr: 0.000201, 133.17s
2024-03-22 20:12:53,973 - INFO - Saved model at 0
2024-03-22 20:12:53,974 - INFO - Val loss decrease from inf to 249.6504, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch0.tar
2024-03-22 20:14:56,970 - INFO - epoch complete!
2024-03-22 20:14:56,971 - INFO - evaluating now!
2024-03-22 20:15:07,491 - INFO - Epoch [1/300] (1338) train_loss: 53.4308, val_loss: 193.3839, lr: 0.000401, 133.52s
2024-03-22 20:15:07,601 - INFO - Saved model at 1
2024-03-22 20:15:07,602 - INFO - Val loss decrease from 249.6504 to 193.3839, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch1.tar
2024-03-22 20:17:10,835 - INFO - epoch complete!
2024-03-22 20:17:10,836 - INFO - evaluating now!
2024-03-22 20:17:21,808 - INFO - Epoch [2/300] (2007) train_loss: 35.9987, val_loss: 195.0387, lr: 0.000600, 134.21s
2024-03-22 20:19:25,143 - INFO - epoch complete!
2024-03-22 20:19:25,144 - INFO - evaluating now!
2024-03-22 20:19:35,658 - INFO - Epoch [3/300] (2676) train_loss: 32.4185, val_loss: 194.1883, lr: 0.000800, 133.85s
2024-03-22 20:19:53,959 - INFO - Training: task_level increase from 1 to 2
2024-03-22 20:19:53,959 - INFO - Current batches_seen is 2776
2024-03-22 20:21:38,475 - INFO - epoch complete!
2024-03-22 20:21:38,476 - INFO - evaluating now!
2024-03-22 20:21:48,942 - INFO - Epoch [4/300] (3345) train_loss: 36.3741, val_loss: 170.7521, lr: 0.000999, 133.28s
2024-03-22 20:21:49,051 - INFO - Saved model at 4
2024-03-22 20:21:49,051 - INFO - Val loss decrease from 193.3839 to 170.7521, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch4.tar
2024-03-22 20:23:52,169 - INFO - epoch complete!
2024-03-22 20:23:52,170 - INFO - evaluating now!
2024-03-22 20:24:02,839 - INFO - Epoch [5/300] (4014) train_loss: 31.0676, val_loss: 171.9939, lr: 0.000999, 133.79s
2024-03-22 20:26:05,695 - INFO - epoch complete!
2024-03-22 20:26:05,696 - INFO - evaluating now!
2024-03-22 20:26:16,126 - INFO - Epoch [6/300] (4683) train_loss: 30.0317, val_loss: 173.4715, lr: 0.000999, 133.29s
2024-03-22 20:28:24,094 - INFO - epoch complete!
2024-03-22 20:28:24,095 - INFO - evaluating now!
2024-03-22 20:28:36,438 - INFO - Epoch [7/300] (5352) train_loss: 29.2139, val_loss: 172.6262, lr: 0.000998, 140.31s
2024-03-22 20:29:20,160 - INFO - Training: task_level increase from 2 to 3
2024-03-22 20:29:20,160 - INFO - Current batches_seen is 5552
2024-03-22 20:30:57,190 - INFO - epoch complete!
2024-03-22 20:30:57,191 - INFO - evaluating now!
2024-03-22 20:31:07,656 - INFO - Epoch [8/300] (6021) train_loss: 31.0174, val_loss: 155.1409, lr: 0.000998, 151.22s
2024-03-22 20:31:07,762 - INFO - Saved model at 8
2024-03-22 20:31:07,763 - INFO - Val loss decrease from 170.7521 to 155.1409, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch8.tar
2024-03-22 20:33:11,892 - INFO - epoch complete!
2024-03-22 20:33:11,892 - INFO - evaluating now!
2024-03-22 20:33:23,272 - INFO - Epoch [9/300] (6690) train_loss: 29.6529, val_loss: 155.4556, lr: 0.000998, 135.51s
2024-03-22 20:35:26,548 - INFO - epoch complete!
2024-03-22 20:35:26,549 - INFO - evaluating now!
2024-03-22 20:35:37,165 - INFO - Epoch [10/300] (7359) train_loss: 29.1322, val_loss: 156.5373, lr: 0.000997, 133.89s
2024-03-22 20:37:40,123 - INFO - epoch complete!
2024-03-22 20:37:40,123 - INFO - evaluating now!
2024-03-22 20:37:50,601 - INFO - Epoch [11/300] (8028) train_loss: 28.9947, val_loss: 155.5857, lr: 0.000996, 133.43s
2024-03-22 20:38:45,914 - INFO - Training: task_level increase from 3 to 4
2024-03-22 20:38:45,914 - INFO - Current batches_seen is 8328
2024-03-22 20:39:53,521 - INFO - epoch complete!
2024-03-22 20:39:53,521 - INFO - evaluating now!
2024-03-22 20:40:04,093 - INFO - Epoch [12/300] (8697) train_loss: 29.9478, val_loss: 144.1048, lr: 0.000996, 133.49s
2024-03-22 20:40:04,207 - INFO - Saved model at 12
2024-03-22 20:40:04,208 - INFO - Val loss decrease from 155.1409 to 144.1048, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch12.tar
2024-03-22 20:42:07,414 - INFO - epoch complete!
2024-03-22 20:42:07,414 - INFO - evaluating now!
2024-03-22 20:42:18,134 - INFO - Epoch [13/300] (9366) train_loss: 29.6323, val_loss: 142.6492, lr: 0.000995, 133.93s
2024-03-22 20:42:18,251 - INFO - Saved model at 13
2024-03-22 20:42:18,252 - INFO - Val loss decrease from 144.1048 to 142.6492, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch13.tar
2024-03-22 20:44:21,530 - INFO - epoch complete!
2024-03-22 20:44:21,530 - INFO - evaluating now!
2024-03-22 20:44:32,137 - INFO - Epoch [14/300] (10035) train_loss: 29.4449, val_loss: 142.5034, lr: 0.000994, 133.88s
2024-03-22 20:44:32,247 - INFO - Saved model at 14
2024-03-22 20:44:32,247 - INFO - Val loss decrease from 142.6492 to 142.5034, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch14.tar
2024-03-22 20:46:40,276 - INFO - epoch complete!
2024-03-22 20:46:40,277 - INFO - evaluating now!
2024-03-22 20:46:50,688 - INFO - Epoch [15/300] (10704) train_loss: 28.9474, val_loss: 141.9174, lr: 0.000994, 138.44s
2024-03-22 20:46:50,789 - INFO - Saved model at 15
2024-03-22 20:46:50,790 - INFO - Val loss decrease from 142.5034 to 141.9174, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch15.tar
2024-03-22 20:48:04,223 - INFO - Training: task_level increase from 4 to 5
2024-03-22 20:48:04,224 - INFO - Current batches_seen is 11104
2024-03-22 20:48:53,597 - INFO - epoch complete!
2024-03-22 20:48:53,597 - INFO - evaluating now!
2024-03-22 20:49:04,074 - INFO - Epoch [16/300] (11373) train_loss: 29.7584, val_loss: 127.4619, lr: 0.000993, 133.28s
2024-03-22 20:49:04,184 - INFO - Saved model at 16
2024-03-22 20:49:04,185 - INFO - Val loss decrease from 141.9174 to 127.4619, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch16.tar
2024-03-22 20:51:07,383 - INFO - epoch complete!
2024-03-22 20:51:07,384 - INFO - evaluating now!
2024-03-22 20:51:17,926 - INFO - Epoch [17/300] (12042) train_loss: 29.8042, val_loss: 126.3461, lr: 0.000992, 133.74s
2024-03-22 20:51:18,034 - INFO - Saved model at 17
2024-03-22 20:51:18,035 - INFO - Val loss decrease from 127.4619 to 126.3461, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch17.tar
2024-03-22 20:53:21,598 - INFO - epoch complete!
2024-03-22 20:53:21,599 - INFO - evaluating now!
2024-03-22 20:53:32,244 - INFO - Epoch [18/300] (12711) train_loss: 29.2013, val_loss: 125.3004, lr: 0.000991, 134.21s
2024-03-22 20:53:32,349 - INFO - Saved model at 18
2024-03-22 20:53:32,350 - INFO - Val loss decrease from 126.3461 to 125.3004, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch18.tar
2024-03-22 20:55:39,118 - INFO - epoch complete!
2024-03-22 20:55:39,118 - INFO - evaluating now!
2024-03-22 20:55:49,649 - INFO - Epoch [19/300] (13380) train_loss: 28.8928, val_loss: 124.7057, lr: 0.000990, 137.30s
2024-03-22 20:55:49,758 - INFO - Saved model at 19
2024-03-22 20:55:49,759 - INFO - Val loss decrease from 125.3004 to 124.7057, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch19.tar
2024-03-22 20:57:22,131 - INFO - Training: task_level increase from 5 to 6
2024-03-22 20:57:22,132 - INFO - Current batches_seen is 13880
2024-03-22 20:57:53,330 - INFO - epoch complete!
2024-03-22 20:57:53,331 - INFO - evaluating now!
2024-03-22 20:58:04,025 - INFO - Epoch [20/300] (14049) train_loss: 28.8678, val_loss: 123.9268, lr: 0.000989, 134.27s
2024-03-22 20:58:04,140 - INFO - Saved model at 20
2024-03-22 20:58:04,141 - INFO - Val loss decrease from 124.7057 to 123.9268, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch20.tar
2024-03-22 21:00:09,286 - INFO - epoch complete!
2024-03-22 21:00:09,287 - INFO - evaluating now!
2024-03-22 21:00:19,838 - INFO - Epoch [21/300] (14718) train_loss: 29.3589, val_loss: 123.7340, lr: 0.000988, 135.70s
2024-03-22 21:00:19,941 - INFO - Saved model at 21
2024-03-22 21:00:19,942 - INFO - Val loss decrease from 123.9268 to 123.7340, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch21.tar
2024-03-22 21:02:23,426 - INFO - epoch complete!
2024-03-22 21:02:23,426 - INFO - evaluating now!
2024-03-22 21:02:34,052 - INFO - Epoch [22/300] (15387) train_loss: 28.8708, val_loss: 123.6541, lr: 0.000987, 134.11s
2024-03-22 21:02:34,156 - INFO - Saved model at 22
2024-03-22 21:02:34,157 - INFO - Val loss decrease from 123.7340 to 123.6541, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch22.tar
2024-03-22 21:04:39,460 - INFO - epoch complete!
2024-03-22 21:04:39,461 - INFO - evaluating now!
2024-03-22 21:04:51,853 - INFO - Epoch [23/300] (16056) train_loss: 28.4377, val_loss: 124.1616, lr: 0.000986, 137.70s
2024-03-22 21:06:44,424 - INFO - Training: task_level increase from 6 to 7
2024-03-22 21:06:44,425 - INFO - Current batches_seen is 16656
2024-03-22 21:06:57,129 - INFO - epoch complete!
2024-03-22 21:06:57,130 - INFO - evaluating now!
2024-03-22 21:07:07,739 - INFO - Epoch [24/300] (16725) train_loss: 29.2704, val_loss: 107.3751, lr: 0.000985, 135.89s
2024-03-22 21:07:07,849 - INFO - Saved model at 24
2024-03-22 21:07:07,849 - INFO - Val loss decrease from 123.6541 to 107.3751, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch24.tar
2024-03-22 21:09:11,425 - INFO - epoch complete!
2024-03-22 21:09:11,426 - INFO - evaluating now!
2024-03-22 21:09:21,957 - INFO - Epoch [25/300] (17394) train_loss: 28.9586, val_loss: 106.9620, lr: 0.000983, 134.11s
2024-03-22 21:09:22,065 - INFO - Saved model at 25
2024-03-22 21:09:22,066 - INFO - Val loss decrease from 107.3751 to 106.9620, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch25.tar
2024-03-22 21:11:25,705 - INFO - epoch complete!
2024-03-22 21:11:25,705 - INFO - evaluating now!
2024-03-22 21:11:36,269 - INFO - Epoch [26/300] (18063) train_loss: 28.6419, val_loss: 106.8758, lr: 0.000982, 134.20s
2024-03-22 21:11:36,374 - INFO - Saved model at 26
2024-03-22 21:11:36,375 - INFO - Val loss decrease from 106.9620 to 106.8758, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch26.tar
2024-03-22 21:13:40,059 - INFO - epoch complete!
2024-03-22 21:13:40,060 - INFO - evaluating now!
2024-03-22 21:13:50,636 - INFO - Epoch [27/300] (18732) train_loss: 28.3107, val_loss: 107.1181, lr: 0.000981, 134.26s
2024-03-22 21:15:54,190 - INFO - epoch complete!
2024-03-22 21:15:54,191 - INFO - evaluating now!
2024-03-22 21:16:04,749 - INFO - Epoch [28/300] (19401) train_loss: 28.0795, val_loss: 107.2593, lr: 0.000979, 134.11s
2024-03-22 21:16:10,519 - INFO - Training: task_level increase from 7 to 8
2024-03-22 21:16:10,519 - INFO - Current batches_seen is 19432
2024-03-22 21:18:12,068 - INFO - epoch complete!
2024-03-22 21:18:12,070 - INFO - evaluating now!
2024-03-22 21:18:22,695 - INFO - Epoch [29/300] (20070) train_loss: 29.0768, val_loss: 91.6328, lr: 0.000978, 137.95s
2024-03-22 21:18:22,797 - INFO - Saved model at 29
2024-03-22 21:18:22,797 - INFO - Val loss decrease from 106.8758 to 91.6328, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch29.tar
2024-03-22 21:20:35,884 - INFO - epoch complete!
2024-03-22 21:20:35,885 - INFO - evaluating now!
2024-03-22 21:20:46,428 - INFO - Epoch [30/300] (20739) train_loss: 28.5735, val_loss: 91.7278, lr: 0.000976, 143.63s
2024-03-22 21:22:59,118 - INFO - epoch complete!
2024-03-22 21:22:59,118 - INFO - evaluating now!
2024-03-22 21:23:09,793 - INFO - Epoch [31/300] (21408) train_loss: 28.2074, val_loss: 91.8593, lr: 0.000975, 143.36s
2024-03-22 21:25:20,072 - INFO - epoch complete!
2024-03-22 21:25:20,073 - INFO - evaluating now!
2024-03-22 21:25:30,663 - INFO - Epoch [32/300] (22077) train_loss: 28.0447, val_loss: 90.9494, lr: 0.000973, 140.87s
2024-03-22 21:25:30,762 - INFO - Saved model at 32
2024-03-22 21:25:30,763 - INFO - Val loss decrease from 91.6328 to 90.9494, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch32.tar
2024-03-22 21:25:54,972 - INFO - Training: task_level increase from 8 to 9
2024-03-22 21:25:54,972 - INFO - Current batches_seen is 22208
2024-03-22 21:27:34,087 - INFO - epoch complete!
2024-03-22 21:27:34,087 - INFO - evaluating now!
2024-03-22 21:27:44,664 - INFO - Epoch [33/300] (22746) train_loss: 28.6711, val_loss: 77.2889, lr: 0.000972, 133.90s
2024-03-22 21:27:44,770 - INFO - Saved model at 33
2024-03-22 21:27:44,771 - INFO - Val loss decrease from 90.9494 to 77.2889, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch33.tar
2024-03-22 21:29:48,741 - INFO - epoch complete!
2024-03-22 21:29:48,741 - INFO - evaluating now!
2024-03-22 21:29:59,414 - INFO - Epoch [34/300] (23415) train_loss: 28.4019, val_loss: 76.9584, lr: 0.000970, 134.64s
2024-03-22 21:29:59,517 - INFO - Saved model at 34
2024-03-22 21:29:59,517 - INFO - Val loss decrease from 77.2889 to 76.9584, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch34.tar
2024-03-22 21:32:02,922 - INFO - epoch complete!
2024-03-22 21:32:02,923 - INFO - evaluating now!
2024-03-22 21:32:13,288 - INFO - Epoch [35/300] (24084) train_loss: 28.0457, val_loss: 77.2487, lr: 0.000968, 133.77s
2024-03-22 21:34:39,034 - INFO - epoch complete!
2024-03-22 21:34:39,034 - INFO - evaluating now!
2024-03-22 21:34:51,374 - INFO - Epoch [36/300] (24753) train_loss: 28.0481, val_loss: 77.2015, lr: 0.000967, 158.09s
2024-03-22 21:35:41,780 - INFO - Training: task_level increase from 9 to 10
2024-03-22 21:35:41,781 - INFO - Current batches_seen is 24984
2024-03-22 21:37:06,326 - INFO - epoch complete!
2024-03-22 21:37:06,327 - INFO - evaluating now!
2024-03-22 21:37:16,809 - INFO - Epoch [37/300] (25422) train_loss: 28.6004, val_loss: 60.8763, lr: 0.000965, 145.43s
2024-03-22 21:37:16,919 - INFO - Saved model at 37
2024-03-22 21:37:16,920 - INFO - Val loss decrease from 76.9584 to 60.8763, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch37.tar
2024-03-22 21:39:19,021 - INFO - epoch complete!
2024-03-22 21:39:19,022 - INFO - evaluating now!
2024-03-22 21:39:29,466 - INFO - Epoch [38/300] (26091) train_loss: 28.3434, val_loss: 61.9604, lr: 0.000963, 132.55s
2024-03-22 21:41:31,822 - INFO - epoch complete!
2024-03-22 21:41:31,823 - INFO - evaluating now!
2024-03-22 21:41:42,426 - INFO - Epoch [39/300] (26760) train_loss: 28.1599, val_loss: 60.4634, lr: 0.000961, 132.96s
2024-03-22 21:41:42,536 - INFO - Saved model at 39
2024-03-22 21:41:42,537 - INFO - Val loss decrease from 60.8763 to 60.4634, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch39.tar
2024-03-22 21:43:47,016 - INFO - epoch complete!
2024-03-22 21:43:47,017 - INFO - evaluating now!
2024-03-22 21:43:57,737 - INFO - Epoch [40/300] (27429) train_loss: 27.9177, val_loss: 60.5694, lr: 0.000959, 135.20s
2024-03-22 21:45:03,571 - INFO - Training: task_level increase from 10 to 11
2024-03-22 21:45:03,571 - INFO - Current batches_seen is 27760
2024-03-22 21:46:09,896 - INFO - epoch complete!
2024-03-22 21:46:09,896 - INFO - evaluating now!
2024-03-22 21:46:20,341 - INFO - Epoch [41/300] (28098) train_loss: 28.2081, val_loss: 46.8443, lr: 0.000957, 142.60s
2024-03-22 21:46:20,451 - INFO - Saved model at 41
2024-03-22 21:46:20,452 - INFO - Val loss decrease from 60.4634 to 46.8443, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch41.tar
2024-03-22 21:48:27,290 - INFO - epoch complete!
2024-03-22 21:48:27,290 - INFO - evaluating now!
2024-03-22 21:48:37,875 - INFO - Epoch [42/300] (28767) train_loss: 28.2731, val_loss: 45.0582, lr: 0.000955, 137.42s
2024-03-22 21:48:37,993 - INFO - Saved model at 42
2024-03-22 21:48:37,993 - INFO - Val loss decrease from 46.8443 to 45.0582, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch42.tar
2024-03-22 21:50:41,281 - INFO - epoch complete!
2024-03-22 21:50:41,282 - INFO - evaluating now!
2024-03-22 21:50:51,858 - INFO - Epoch [43/300] (29436) train_loss: 28.1206, val_loss: 45.4837, lr: 0.000953, 133.86s
2024-03-22 21:52:54,838 - INFO - epoch complete!
2024-03-22 21:52:54,839 - INFO - evaluating now!
2024-03-22 21:53:05,315 - INFO - Epoch [44/300] (30105) train_loss: 28.0691, val_loss: 45.4844, lr: 0.000951, 133.46s
2024-03-22 21:54:24,189 - INFO - Training: task_level increase from 11 to 12
2024-03-22 21:54:24,190 - INFO - Current batches_seen is 30536
2024-03-22 21:55:07,679 - INFO - epoch complete!
2024-03-22 21:55:07,680 - INFO - evaluating now!
2024-03-22 21:55:18,201 - INFO - Epoch [45/300] (30774) train_loss: 28.3021, val_loss: 28.0547, lr: 0.000949, 132.89s
2024-03-22 21:55:18,308 - INFO - Saved model at 45
2024-03-22 21:55:18,308 - INFO - Val loss decrease from 45.0582 to 28.0547, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch45.tar
2024-03-22 21:57:21,125 - INFO - epoch complete!
2024-03-22 21:57:21,126 - INFO - evaluating now!
2024-03-22 21:57:31,623 - INFO - Epoch [46/300] (31443) train_loss: 28.3296, val_loss: 28.1362, lr: 0.000947, 133.31s
2024-03-22 21:59:38,218 - INFO - epoch complete!
2024-03-22 21:59:38,218 - INFO - evaluating now!
2024-03-22 21:59:48,701 - INFO - Epoch [47/300] (32112) train_loss: 28.1073, val_loss: 28.1718, lr: 0.000944, 137.08s
2024-03-22 22:01:50,696 - INFO - epoch complete!
2024-03-22 22:01:50,697 - INFO - evaluating now!
2024-03-22 22:02:01,138 - INFO - Epoch [48/300] (32781) train_loss: 28.2124, val_loss: 28.5595, lr: 0.000942, 132.44s
2024-03-22 22:04:04,887 - INFO - epoch complete!
2024-03-22 22:04:04,888 - INFO - evaluating now!
2024-03-22 22:04:15,373 - INFO - Epoch [49/300] (33450) train_loss: 28.0543, val_loss: 27.7896, lr: 0.000940, 134.23s
2024-03-22 22:04:15,483 - INFO - Saved model at 49
2024-03-22 22:04:15,483 - INFO - Val loss decrease from 28.0547 to 27.7896, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch49.tar
2024-03-22 22:06:18,055 - INFO - epoch complete!
2024-03-22 22:06:18,056 - INFO - evaluating now!
2024-03-22 22:06:28,589 - INFO - Epoch [50/300] (34119) train_loss: 27.7777, val_loss: 27.6955, lr: 0.000937, 133.11s
2024-03-22 22:06:28,692 - INFO - Saved model at 50
2024-03-22 22:06:28,692 - INFO - Val loss decrease from 27.7896 to 27.6955, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch50.tar
2024-03-22 22:08:31,446 - INFO - epoch complete!
2024-03-22 22:08:31,446 - INFO - evaluating now!
2024-03-22 22:08:41,910 - INFO - Epoch [51/300] (34788) train_loss: 27.7347, val_loss: 27.6187, lr: 0.000935, 133.22s
2024-03-22 22:08:42,016 - INFO - Saved model at 51
2024-03-22 22:08:42,017 - INFO - Val loss decrease from 27.6955 to 27.6187, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch51.tar
2024-03-22 22:10:46,691 - INFO - epoch complete!
2024-03-22 22:10:46,692 - INFO - evaluating now!
2024-03-22 22:10:57,177 - INFO - Epoch [52/300] (35457) train_loss: 27.7478, val_loss: 27.9012, lr: 0.000932, 135.16s
2024-03-22 22:12:49,385 - INFO - epoch complete!
2024-03-22 22:12:49,386 - INFO - evaluating now!
2024-03-22 22:12:55,773 - INFO - Epoch [53/300] (36126) train_loss: 27.5915, val_loss: 27.5780, lr: 0.000930, 118.59s
2024-03-22 22:12:55,824 - INFO - Saved model at 53
2024-03-22 22:12:55,824 - INFO - Val loss decrease from 27.6187 to 27.5780, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch53.tar
2024-03-22 22:14:41,830 - INFO - epoch complete!
2024-03-22 22:14:41,831 - INFO - evaluating now!
2024-03-22 22:14:48,277 - INFO - Epoch [54/300] (36795) train_loss: 27.6006, val_loss: 28.0497, lr: 0.000927, 112.45s
2024-03-22 22:16:29,123 - INFO - epoch complete!
2024-03-22 22:16:29,124 - INFO - evaluating now!
2024-03-22 22:16:35,592 - INFO - Epoch [55/300] (37464) train_loss: 27.5745, val_loss: 27.2043, lr: 0.000925, 107.31s
2024-03-22 22:16:35,644 - INFO - Saved model at 55
2024-03-22 22:16:35,644 - INFO - Val loss decrease from 27.5780 to 27.2043, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch55.tar
2024-03-22 22:18:37,559 - INFO - epoch complete!
2024-03-22 22:18:37,560 - INFO - evaluating now!
2024-03-22 22:18:48,723 - INFO - Epoch [56/300] (38133) train_loss: 27.4711, val_loss: 27.7812, lr: 0.000922, 133.08s
2024-03-22 22:20:57,537 - INFO - epoch complete!
2024-03-22 22:20:57,537 - INFO - evaluating now!
2024-03-22 22:21:09,220 - INFO - Epoch [57/300] (38802) train_loss: 27.3067, val_loss: 27.3119, lr: 0.000920, 140.50s
2024-03-22 22:23:18,726 - INFO - epoch complete!
2024-03-22 22:23:18,726 - INFO - evaluating now!
2024-03-22 22:23:30,433 - INFO - Epoch [58/300] (39471) train_loss: 27.4063, val_loss: 26.9825, lr: 0.000917, 141.21s
2024-03-22 22:23:30,558 - INFO - Saved model at 58
2024-03-22 22:23:30,559 - INFO - Val loss decrease from 27.2043 to 26.9825, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch58.tar
2024-03-22 22:25:41,703 - INFO - epoch complete!
2024-03-22 22:25:41,704 - INFO - evaluating now!
2024-03-22 22:25:52,877 - INFO - Epoch [59/300] (40140) train_loss: 27.3292, val_loss: 27.1529, lr: 0.000914, 142.32s
2024-03-22 22:28:02,436 - INFO - epoch complete!
2024-03-22 22:28:02,437 - INFO - evaluating now!
2024-03-22 22:28:13,528 - INFO - Epoch [60/300] (40809) train_loss: 27.4076, val_loss: 27.6480, lr: 0.000911, 140.65s
2024-03-22 22:30:22,640 - INFO - epoch complete!
2024-03-22 22:30:22,641 - INFO - evaluating now!
2024-03-22 22:30:34,384 - INFO - Epoch [61/300] (41478) train_loss: 27.2405, val_loss: 27.6041, lr: 0.000908, 140.85s
2024-03-22 22:32:52,892 - INFO - epoch complete!
2024-03-22 22:32:52,892 - INFO - evaluating now!
2024-03-22 22:33:03,941 - INFO - Epoch [62/300] (42147) train_loss: 27.3636, val_loss: 27.1925, lr: 0.000906, 149.56s
2024-03-22 22:35:22,044 - INFO - epoch complete!
2024-03-22 22:35:22,044 - INFO - evaluating now!
2024-03-22 22:35:33,088 - INFO - Epoch [63/300] (42816) train_loss: 27.2128, val_loss: 27.0843, lr: 0.000903, 149.15s
2024-03-22 22:37:46,201 - INFO - epoch complete!
2024-03-22 22:37:46,202 - INFO - evaluating now!
2024-03-22 22:37:57,694 - INFO - Epoch [64/300] (43485) train_loss: 27.0148, val_loss: 26.6054, lr: 0.000900, 144.61s
2024-03-22 22:37:57,834 - INFO - Saved model at 64
2024-03-22 22:37:57,835 - INFO - Val loss decrease from 26.9825 to 26.6054, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch64.tar
2024-03-22 22:40:13,516 - INFO - epoch complete!
2024-03-22 22:40:13,517 - INFO - evaluating now!
2024-03-22 22:40:25,597 - INFO - Epoch [65/300] (44154) train_loss: 27.0684, val_loss: 27.0811, lr: 0.000897, 147.76s
2024-03-22 22:42:38,574 - INFO - epoch complete!
2024-03-22 22:42:38,575 - INFO - evaluating now!
2024-03-22 22:42:49,602 - INFO - Epoch [66/300] (44823) train_loss: 26.9785, val_loss: 26.8944, lr: 0.000894, 144.00s
2024-03-22 22:45:00,485 - INFO - epoch complete!
2024-03-22 22:45:00,486 - INFO - evaluating now!
2024-03-22 22:45:13,641 - INFO - Epoch [67/300] (45492) train_loss: 26.9270, val_loss: 26.5860, lr: 0.000891, 144.04s
2024-03-22 22:45:13,762 - INFO - Saved model at 67
2024-03-22 22:45:13,762 - INFO - Val loss decrease from 26.6054 to 26.5860, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch67.tar
2024-03-22 22:47:28,223 - INFO - epoch complete!
2024-03-22 22:47:28,223 - INFO - evaluating now!
2024-03-22 22:47:39,927 - INFO - Epoch [68/300] (46161) train_loss: 26.8677, val_loss: 27.2727, lr: 0.000888, 146.16s
2024-03-22 22:49:49,222 - INFO - epoch complete!
2024-03-22 22:49:49,222 - INFO - evaluating now!
2024-03-22 22:50:00,317 - INFO - Epoch [69/300] (46830) train_loss: 26.8569, val_loss: 26.9131, lr: 0.000884, 140.39s
2024-03-22 22:52:12,965 - INFO - epoch complete!
2024-03-22 22:52:12,966 - INFO - evaluating now!
2024-03-22 22:52:24,637 - INFO - Epoch [70/300] (47499) train_loss: 26.8727, val_loss: 26.7394, lr: 0.000881, 144.32s
2024-03-22 22:54:39,647 - INFO - epoch complete!
2024-03-22 22:54:39,648 - INFO - evaluating now!
2024-03-22 22:54:50,791 - INFO - Epoch [71/300] (48168) train_loss: 26.8046, val_loss: 26.9291, lr: 0.000878, 146.15s
2024-03-22 22:56:56,872 - INFO - epoch complete!
2024-03-22 22:56:56,873 - INFO - evaluating now!
2024-03-22 22:57:06,483 - INFO - Epoch [72/300] (48837) train_loss: 26.9960, val_loss: 27.1716, lr: 0.000875, 135.69s
2024-03-22 22:59:15,283 - INFO - epoch complete!
2024-03-22 22:59:15,283 - INFO - evaluating now!
2024-03-22 22:59:27,159 - INFO - Epoch [73/300] (49506) train_loss: 26.7197, val_loss: 26.5916, lr: 0.000872, 140.67s
2024-03-22 23:01:37,088 - INFO - epoch complete!
2024-03-22 23:01:37,089 - INFO - evaluating now!
2024-03-22 23:01:48,167 - INFO - Epoch [74/300] (50175) train_loss: 26.7678, val_loss: 26.9893, lr: 0.000868, 141.01s
2024-03-22 23:03:56,955 - INFO - epoch complete!
2024-03-22 23:03:56,956 - INFO - evaluating now!
2024-03-22 23:04:08,051 - INFO - Epoch [75/300] (50844) train_loss: 26.6436, val_loss: 27.9732, lr: 0.000865, 139.88s
2024-03-22 23:06:20,997 - INFO - epoch complete!
2024-03-22 23:06:20,998 - INFO - evaluating now!
2024-03-22 23:06:32,636 - INFO - Epoch [76/300] (51513) train_loss: 26.7282, val_loss: 26.9949, lr: 0.000861, 144.58s
2024-03-22 23:08:41,453 - INFO - epoch complete!
2024-03-22 23:08:41,453 - INFO - evaluating now!
2024-03-22 23:08:52,470 - INFO - Epoch [77/300] (52182) train_loss: 26.6209, val_loss: 27.4333, lr: 0.000858, 139.83s
2024-03-22 23:11:13,523 - INFO - epoch complete!
2024-03-22 23:11:13,524 - INFO - evaluating now!
2024-03-22 23:11:24,733 - INFO - Epoch [78/300] (52851) train_loss: 26.5542, val_loss: 26.8633, lr: 0.000855, 152.26s
2024-03-22 23:13:40,338 - INFO - epoch complete!
2024-03-22 23:13:40,339 - INFO - evaluating now!
2024-03-22 23:13:51,453 - INFO - Epoch [79/300] (53520) train_loss: 26.6172, val_loss: 26.6651, lr: 0.000851, 146.72s
2024-03-22 23:16:17,912 - INFO - epoch complete!
2024-03-22 23:16:17,913 - INFO - evaluating now!
2024-03-22 23:16:29,152 - INFO - Epoch [80/300] (54189) train_loss: 26.5040, val_loss: 27.3905, lr: 0.000848, 157.70s
2024-03-22 23:18:41,197 - INFO - epoch complete!
2024-03-22 23:18:41,197 - INFO - evaluating now!
2024-03-22 23:18:52,330 - INFO - Epoch [81/300] (54858) train_loss: 26.5134, val_loss: 27.0691, lr: 0.000844, 143.18s
2024-03-22 23:21:07,429 - INFO - epoch complete!
2024-03-22 23:21:07,430 - INFO - evaluating now!
2024-03-22 23:21:18,566 - INFO - Epoch [82/300] (55527) train_loss: 26.4233, val_loss: 27.0319, lr: 0.000840, 146.24s
2024-03-22 23:23:29,447 - INFO - epoch complete!
2024-03-22 23:23:29,448 - INFO - evaluating now!
2024-03-22 23:23:40,504 - INFO - Epoch [83/300] (56196) train_loss: 26.4674, val_loss: 26.2522, lr: 0.000837, 141.94s
2024-03-22 23:23:40,619 - INFO - Saved model at 83
2024-03-22 23:23:40,620 - INFO - Val loss decrease from 26.5860 to 26.2522, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch83.tar
2024-03-22 23:25:55,474 - INFO - epoch complete!
2024-03-22 23:25:55,475 - INFO - evaluating now!
2024-03-22 23:26:06,549 - INFO - Epoch [84/300] (56865) train_loss: 26.4010, val_loss: 26.5061, lr: 0.000833, 145.93s
2024-03-22 23:28:17,789 - INFO - epoch complete!
2024-03-22 23:28:17,790 - INFO - evaluating now!
2024-03-22 23:28:29,019 - INFO - Epoch [85/300] (57534) train_loss: 26.4079, val_loss: 26.3967, lr: 0.000830, 142.47s
2024-03-22 23:30:38,880 - INFO - epoch complete!
2024-03-22 23:30:38,881 - INFO - evaluating now!
2024-03-22 23:30:49,980 - INFO - Epoch [86/300] (58203) train_loss: 26.3675, val_loss: 26.5210, lr: 0.000826, 140.96s
2024-03-22 23:33:02,102 - INFO - epoch complete!
2024-03-22 23:33:02,103 - INFO - evaluating now!
2024-03-22 23:33:13,582 - INFO - Epoch [87/300] (58872) train_loss: 26.3155, val_loss: 26.8777, lr: 0.000822, 143.60s
2024-03-22 23:35:22,878 - INFO - epoch complete!
2024-03-22 23:35:22,879 - INFO - evaluating now!
2024-03-22 23:35:33,941 - INFO - Epoch [88/300] (59541) train_loss: 26.1604, val_loss: 26.5397, lr: 0.000818, 140.36s
2024-03-22 23:37:43,813 - INFO - epoch complete!
2024-03-22 23:37:43,813 - INFO - evaluating now!
2024-03-22 23:37:54,929 - INFO - Epoch [89/300] (60210) train_loss: 26.1678, val_loss: 26.3360, lr: 0.000815, 140.99s
2024-03-22 23:40:04,442 - INFO - epoch complete!
2024-03-22 23:40:04,443 - INFO - evaluating now!
2024-03-22 23:40:15,611 - INFO - Epoch [90/300] (60879) train_loss: 26.2294, val_loss: 26.0552, lr: 0.000811, 140.68s
2024-03-22 23:40:15,726 - INFO - Saved model at 90
2024-03-22 23:40:15,727 - INFO - Val loss decrease from 26.2522 to 26.0552, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch90.tar
2024-03-22 23:42:24,844 - INFO - epoch complete!
2024-03-22 23:42:24,845 - INFO - evaluating now!
2024-03-22 23:42:36,026 - INFO - Epoch [91/300] (61548) train_loss: 26.1210, val_loss: 27.0536, lr: 0.000807, 140.30s
2024-03-22 23:44:46,715 - INFO - epoch complete!
2024-03-22 23:44:46,715 - INFO - evaluating now!
2024-03-22 23:44:59,894 - INFO - Epoch [92/300] (62217) train_loss: 26.0386, val_loss: 26.0168, lr: 0.000803, 143.87s
2024-03-22 23:45:00,024 - INFO - Saved model at 92
2024-03-22 23:45:00,025 - INFO - Val loss decrease from 26.0552 to 26.0168, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch92.tar
2024-03-22 23:47:08,708 - INFO - epoch complete!
2024-03-22 23:47:08,708 - INFO - evaluating now!
2024-03-22 23:47:21,855 - INFO - Epoch [93/300] (62886) train_loss: 26.0603, val_loss: 26.6203, lr: 0.000799, 141.83s
2024-03-22 23:49:31,707 - INFO - epoch complete!
2024-03-22 23:49:31,707 - INFO - evaluating now!
2024-03-22 23:49:42,920 - INFO - Epoch [94/300] (63555) train_loss: 26.1302, val_loss: 26.3118, lr: 0.000795, 141.06s
2024-03-22 23:51:52,209 - INFO - epoch complete!
2024-03-22 23:51:52,210 - INFO - evaluating now!
2024-03-22 23:52:03,792 - INFO - Epoch [95/300] (64224) train_loss: 26.0822, val_loss: 26.1963, lr: 0.000791, 140.87s
2024-03-22 23:54:12,750 - INFO - epoch complete!
2024-03-22 23:54:12,751 - INFO - evaluating now!
2024-03-22 23:54:25,649 - INFO - Epoch [96/300] (64893) train_loss: 25.9572, val_loss: 26.4559, lr: 0.000787, 141.86s
2024-03-22 23:56:37,197 - INFO - epoch complete!
2024-03-22 23:56:37,198 - INFO - evaluating now!
2024-03-22 23:56:48,814 - INFO - Epoch [97/300] (65562) train_loss: 25.9901, val_loss: 26.5571, lr: 0.000783, 143.16s
2024-03-22 23:58:57,996 - INFO - epoch complete!
2024-03-22 23:58:57,997 - INFO - evaluating now!
2024-03-22 23:59:09,188 - INFO - Epoch [98/300] (66231) train_loss: 26.0259, val_loss: 26.1980, lr: 0.000779, 140.37s
2024-03-23 00:01:17,587 - INFO - epoch complete!
2024-03-23 00:01:17,588 - INFO - evaluating now!
2024-03-23 00:01:28,486 - INFO - Epoch [99/300] (66900) train_loss: 25.9766, val_loss: 26.2641, lr: 0.000775, 139.30s
2024-03-23 00:03:35,318 - INFO - epoch complete!
2024-03-23 00:03:35,318 - INFO - evaluating now!
2024-03-23 00:03:46,093 - INFO - Epoch [100/300] (67569) train_loss: 26.0139, val_loss: 26.1960, lr: 0.000771, 137.61s
2024-03-23 00:05:52,897 - INFO - epoch complete!
2024-03-23 00:05:52,898 - INFO - evaluating now!
2024-03-23 00:06:03,787 - INFO - Epoch [101/300] (68238) train_loss: 25.8022, val_loss: 26.0945, lr: 0.000767, 137.69s
2024-03-23 00:08:13,225 - INFO - epoch complete!
2024-03-23 00:08:13,225 - INFO - evaluating now!
2024-03-23 00:08:24,243 - INFO - Epoch [102/300] (68907) train_loss: 25.9161, val_loss: 26.7981, lr: 0.000763, 140.46s
2024-03-23 00:10:31,201 - INFO - epoch complete!
2024-03-23 00:10:31,202 - INFO - evaluating now!
2024-03-23 00:10:42,017 - INFO - Epoch [103/300] (69576) train_loss: 25.8746, val_loss: 26.1001, lr: 0.000758, 137.77s
2024-03-23 00:12:51,994 - INFO - epoch complete!
2024-03-23 00:12:51,994 - INFO - evaluating now!
2024-03-23 00:13:04,719 - INFO - Epoch [104/300] (70245) train_loss: 25.7402, val_loss: 26.4329, lr: 0.000754, 142.70s
2024-03-23 00:15:11,435 - INFO - epoch complete!
2024-03-23 00:15:11,436 - INFO - evaluating now!
2024-03-23 00:15:22,415 - INFO - Epoch [105/300] (70914) train_loss: 25.7403, val_loss: 26.2033, lr: 0.000750, 137.70s
2024-03-23 00:17:32,564 - INFO - epoch complete!
2024-03-23 00:17:32,565 - INFO - evaluating now!
2024-03-23 00:17:43,445 - INFO - Epoch [106/300] (71583) train_loss: 25.6620, val_loss: 26.2517, lr: 0.000746, 141.03s
2024-03-23 00:19:50,257 - INFO - epoch complete!
2024-03-23 00:19:50,258 - INFO - evaluating now!
2024-03-23 00:20:01,152 - INFO - Epoch [107/300] (72252) train_loss: 25.6973, val_loss: 26.0942, lr: 0.000742, 137.71s
2024-03-23 00:22:10,948 - INFO - epoch complete!
2024-03-23 00:22:10,948 - INFO - evaluating now!
2024-03-23 00:22:21,765 - INFO - Epoch [108/300] (72921) train_loss: 25.7377, val_loss: 26.3628, lr: 0.000737, 140.61s
2024-03-23 00:24:28,509 - INFO - epoch complete!
2024-03-23 00:24:28,510 - INFO - evaluating now!
2024-03-23 00:24:39,333 - INFO - Epoch [109/300] (73590) train_loss: 25.7072, val_loss: 25.7443, lr: 0.000733, 137.57s
2024-03-23 00:24:39,451 - INFO - Saved model at 109
2024-03-23 00:24:39,452 - INFO - Val loss decrease from 26.0168 to 25.7443, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch109.tar
2024-03-23 00:26:46,012 - INFO - epoch complete!
2024-03-23 00:26:46,013 - INFO - evaluating now!
2024-03-23 00:26:56,828 - INFO - Epoch [110/300] (74259) train_loss: 25.5944, val_loss: 26.3219, lr: 0.000729, 137.38s
2024-03-23 00:29:03,452 - INFO - epoch complete!
2024-03-23 00:29:03,453 - INFO - evaluating now!
2024-03-23 00:29:14,379 - INFO - Epoch [111/300] (74928) train_loss: 25.6624, val_loss: 25.9995, lr: 0.000724, 137.55s
2024-03-23 00:31:21,057 - INFO - epoch complete!
2024-03-23 00:31:21,058 - INFO - evaluating now!
2024-03-23 00:31:31,967 - INFO - Epoch [112/300] (75597) train_loss: 25.6172, val_loss: 26.3173, lr: 0.000720, 137.59s
2024-03-23 00:33:46,290 - INFO - epoch complete!
2024-03-23 00:33:46,291 - INFO - evaluating now!
2024-03-23 00:33:57,216 - INFO - Epoch [113/300] (76266) train_loss: 25.5193, val_loss: 25.6818, lr: 0.000716, 145.25s
2024-03-23 00:33:57,354 - INFO - Saved model at 113
2024-03-23 00:33:57,355 - INFO - Val loss decrease from 25.7443 to 25.6818, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch113.tar
2024-03-23 00:36:18,074 - INFO - epoch complete!
2024-03-23 00:36:18,075 - INFO - evaluating now!
2024-03-23 00:36:28,977 - INFO - Epoch [114/300] (76935) train_loss: 25.5629, val_loss: 25.7931, lr: 0.000711, 151.62s
2024-03-23 00:38:43,070 - INFO - epoch complete!
2024-03-23 00:38:43,071 - INFO - evaluating now!
2024-03-23 00:38:54,022 - INFO - Epoch [115/300] (77604) train_loss: 25.5891, val_loss: 26.9755, lr: 0.000707, 145.04s
2024-03-23 00:41:00,920 - INFO - epoch complete!
2024-03-23 00:41:00,921 - INFO - evaluating now!
2024-03-23 00:41:11,795 - INFO - Epoch [116/300] (78273) train_loss: 25.4735, val_loss: 25.8925, lr: 0.000702, 137.77s
2024-03-23 00:43:18,908 - INFO - epoch complete!
2024-03-23 00:43:18,909 - INFO - evaluating now!
2024-03-23 00:43:29,693 - INFO - Epoch [117/300] (78942) train_loss: 25.4839, val_loss: 26.7945, lr: 0.000698, 137.90s
2024-03-23 00:45:50,878 - INFO - epoch complete!
2024-03-23 00:45:50,878 - INFO - evaluating now!
2024-03-23 00:46:01,706 - INFO - Epoch [118/300] (79611) train_loss: 25.4402, val_loss: 25.6626, lr: 0.000694, 152.01s
2024-03-23 00:46:01,815 - INFO - Saved model at 118
2024-03-23 00:46:01,815 - INFO - Val loss decrease from 25.6818 to 25.6626, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch118.tar
2024-03-23 00:48:09,354 - INFO - epoch complete!
2024-03-23 00:48:09,354 - INFO - evaluating now!
2024-03-23 00:48:20,201 - INFO - Epoch [119/300] (80280) train_loss: 25.4805, val_loss: 26.2587, lr: 0.000689, 138.39s
2024-03-23 00:50:49,907 - INFO - epoch complete!
2024-03-23 00:50:49,908 - INFO - evaluating now!
2024-03-23 00:51:02,591 - INFO - Epoch [120/300] (80949) train_loss: 25.3433, val_loss: 26.3494, lr: 0.000685, 162.39s
2024-03-23 00:53:10,538 - INFO - epoch complete!
2024-03-23 00:53:10,539 - INFO - evaluating now!
2024-03-23 00:53:21,349 - INFO - Epoch [121/300] (81618) train_loss: 25.4279, val_loss: 26.7574, lr: 0.000680, 138.76s
2024-03-23 00:55:33,878 - INFO - epoch complete!
2024-03-23 00:55:33,879 - INFO - evaluating now!
2024-03-23 00:55:44,696 - INFO - Epoch [122/300] (82287) train_loss: 25.3733, val_loss: 26.1190, lr: 0.000676, 143.35s
2024-03-23 00:57:51,421 - INFO - epoch complete!
2024-03-23 00:57:51,422 - INFO - evaluating now!
2024-03-23 00:58:02,272 - INFO - Epoch [123/300] (82956) train_loss: 25.4027, val_loss: 25.8148, lr: 0.000671, 137.57s
2024-03-23 01:00:09,837 - INFO - epoch complete!
2024-03-23 01:00:09,838 - INFO - evaluating now!
2024-03-23 01:00:20,621 - INFO - Epoch [124/300] (83625) train_loss: 25.3686, val_loss: 25.7767, lr: 0.000666, 138.35s
2024-03-23 01:02:38,217 - INFO - epoch complete!
2024-03-23 01:02:38,218 - INFO - evaluating now!
2024-03-23 01:02:48,994 - INFO - Epoch [125/300] (84294) train_loss: 25.2579, val_loss: 25.6048, lr: 0.000662, 148.37s
2024-03-23 01:02:49,106 - INFO - Saved model at 125
2024-03-23 01:02:49,106 - INFO - Val loss decrease from 25.6626 to 25.6048, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch125.tar
2024-03-23 01:04:57,564 - INFO - epoch complete!
2024-03-23 01:04:57,565 - INFO - evaluating now!
2024-03-23 01:05:08,481 - INFO - Epoch [126/300] (84963) train_loss: 25.2763, val_loss: 25.6350, lr: 0.000657, 139.37s
2024-03-23 01:07:15,141 - INFO - epoch complete!
2024-03-23 01:07:15,142 - INFO - evaluating now!
2024-03-23 01:07:26,007 - INFO - Epoch [127/300] (85632) train_loss: 25.2110, val_loss: 25.8626, lr: 0.000653, 137.52s
2024-03-23 01:09:34,836 - INFO - epoch complete!
2024-03-23 01:09:34,837 - INFO - evaluating now!
2024-03-23 01:09:45,806 - INFO - Epoch [128/300] (86301) train_loss: 25.2374, val_loss: 25.5007, lr: 0.000648, 139.80s
2024-03-23 01:09:45,913 - INFO - Saved model at 128
2024-03-23 01:09:45,914 - INFO - Val loss decrease from 25.6048 to 25.5007, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch128.tar
2024-03-23 01:12:00,232 - INFO - epoch complete!
2024-03-23 01:12:00,232 - INFO - evaluating now!
2024-03-23 01:12:11,084 - INFO - Epoch [129/300] (86970) train_loss: 25.1510, val_loss: 25.6247, lr: 0.000644, 145.17s
2024-03-23 01:14:22,362 - INFO - epoch complete!
2024-03-23 01:14:22,363 - INFO - evaluating now!
2024-03-23 01:14:33,271 - INFO - Epoch [130/300] (87639) train_loss: 25.1953, val_loss: 25.4823, lr: 0.000639, 142.19s
2024-03-23 01:14:33,374 - INFO - Saved model at 130
2024-03-23 01:14:33,375 - INFO - Val loss decrease from 25.5007 to 25.4823, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch130.tar
2024-03-23 01:16:41,109 - INFO - epoch complete!
2024-03-23 01:16:41,110 - INFO - evaluating now!
2024-03-23 01:16:51,984 - INFO - Epoch [131/300] (88308) train_loss: 25.0945, val_loss: 26.1338, lr: 0.000634, 138.61s
2024-03-23 01:19:01,671 - INFO - epoch complete!
2024-03-23 01:19:01,672 - INFO - evaluating now!
2024-03-23 01:19:12,549 - INFO - Epoch [132/300] (88977) train_loss: 25.0523, val_loss: 25.5567, lr: 0.000630, 140.56s
2024-03-23 01:21:19,201 - INFO - epoch complete!
2024-03-23 01:21:19,203 - INFO - evaluating now!
2024-03-23 01:21:30,094 - INFO - Epoch [133/300] (89646) train_loss: 25.0456, val_loss: 25.5697, lr: 0.000625, 137.54s
2024-03-23 01:23:37,045 - INFO - epoch complete!
2024-03-23 01:23:37,046 - INFO - evaluating now!
2024-03-23 01:23:47,954 - INFO - Epoch [134/300] (90315) train_loss: 25.0580, val_loss: 26.1192, lr: 0.000620, 137.86s
2024-03-23 01:26:03,031 - INFO - epoch complete!
2024-03-23 01:26:03,032 - INFO - evaluating now!
2024-03-23 01:26:13,896 - INFO - Epoch [135/300] (90984) train_loss: 25.0824, val_loss: 25.4575, lr: 0.000616, 145.94s
2024-03-23 01:26:13,996 - INFO - Saved model at 135
2024-03-23 01:26:13,997 - INFO - Val loss decrease from 25.4823 to 25.4575, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch135.tar
2024-03-23 01:28:21,023 - INFO - epoch complete!
2024-03-23 01:28:21,024 - INFO - evaluating now!
2024-03-23 01:28:31,838 - INFO - Epoch [136/300] (91653) train_loss: 25.0011, val_loss: 25.5513, lr: 0.000611, 137.84s
2024-03-23 01:30:38,579 - INFO - epoch complete!
2024-03-23 01:30:38,580 - INFO - evaluating now!
2024-03-23 01:30:49,369 - INFO - Epoch [137/300] (92322) train_loss: 25.1229, val_loss: 25.5735, lr: 0.000606, 137.53s
2024-03-23 01:32:57,091 - INFO - epoch complete!
2024-03-23 01:32:57,092 - INFO - evaluating now!
2024-03-23 01:33:07,922 - INFO - Epoch [138/300] (92991) train_loss: 24.9619, val_loss: 25.4510, lr: 0.000602, 138.55s
2024-03-23 01:33:08,028 - INFO - Saved model at 138
2024-03-23 01:33:08,028 - INFO - Val loss decrease from 25.4575 to 25.4510, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch138.tar
2024-03-23 01:35:17,571 - INFO - epoch complete!
2024-03-23 01:35:17,572 - INFO - evaluating now!
2024-03-23 01:35:28,394 - INFO - Epoch [139/300] (93660) train_loss: 24.9743, val_loss: 25.7004, lr: 0.000597, 140.37s
2024-03-23 01:37:35,497 - INFO - epoch complete!
2024-03-23 01:37:35,498 - INFO - evaluating now!
2024-03-23 01:37:46,342 - INFO - Epoch [140/300] (94329) train_loss: 24.9509, val_loss: 25.6134, lr: 0.000592, 137.95s
2024-03-23 01:39:53,418 - INFO - epoch complete!
2024-03-23 01:39:53,419 - INFO - evaluating now!
2024-03-23 01:40:04,341 - INFO - Epoch [141/300] (94998) train_loss: 24.9847, val_loss: 25.4297, lr: 0.000588, 138.00s
2024-03-23 01:40:04,449 - INFO - Saved model at 141
2024-03-23 01:40:04,450 - INFO - Val loss decrease from 25.4510 to 25.4297, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch141.tar
2024-03-23 01:42:11,156 - INFO - epoch complete!
2024-03-23 01:42:11,157 - INFO - evaluating now!
2024-03-23 01:42:22,001 - INFO - Epoch [142/300] (95667) train_loss: 24.8439, val_loss: 26.1784, lr: 0.000583, 137.55s
2024-03-23 01:44:40,198 - INFO - epoch complete!
2024-03-23 01:44:40,199 - INFO - evaluating now!
2024-03-23 01:44:51,088 - INFO - Epoch [143/300] (96336) train_loss: 24.8743, val_loss: 25.4546, lr: 0.000578, 149.09s
2024-03-23 01:46:57,680 - INFO - epoch complete!
2024-03-23 01:46:57,681 - INFO - evaluating now!
2024-03-23 01:47:08,476 - INFO - Epoch [144/300] (97005) train_loss: 24.8954, val_loss: 25.4544, lr: 0.000574, 137.39s
2024-03-23 01:49:21,046 - INFO - epoch complete!
2024-03-23 01:49:21,047 - INFO - evaluating now!
2024-03-23 01:49:31,899 - INFO - Epoch [145/300] (97674) train_loss: 24.8471, val_loss: 25.6084, lr: 0.000569, 143.42s
2024-03-23 01:51:42,185 - INFO - epoch complete!
2024-03-23 01:51:42,186 - INFO - evaluating now!
2024-03-23 01:51:53,073 - INFO - Epoch [146/300] (98343) train_loss: 24.8422, val_loss: 25.5714, lr: 0.000564, 141.17s
2024-03-23 01:53:59,443 - INFO - epoch complete!
2024-03-23 01:53:59,444 - INFO - evaluating now!
2024-03-23 01:54:10,273 - INFO - Epoch [147/300] (99012) train_loss: 24.7473, val_loss: 25.3734, lr: 0.000559, 137.20s
2024-03-23 01:54:10,373 - INFO - Saved model at 147
2024-03-23 01:54:10,374 - INFO - Val loss decrease from 25.4297 to 25.3734, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch147.tar
2024-03-23 01:56:26,647 - INFO - epoch complete!
2024-03-23 01:56:26,648 - INFO - evaluating now!
2024-03-23 01:56:37,564 - INFO - Epoch [148/300] (99681) train_loss: 24.7159, val_loss: 25.4874, lr: 0.000555, 147.19s
2024-03-23 01:58:43,987 - INFO - epoch complete!
2024-03-23 01:58:43,988 - INFO - evaluating now!
2024-03-23 01:58:54,780 - INFO - Epoch [149/300] (100350) train_loss: 24.7474, val_loss: 25.5545, lr: 0.000550, 137.22s
2024-03-23 02:01:01,107 - INFO - epoch complete!
2024-03-23 02:01:01,109 - INFO - evaluating now!
2024-03-23 02:01:11,934 - INFO - Epoch [150/300] (101019) train_loss: 24.7755, val_loss: 25.5512, lr: 0.000545, 137.15s
2024-03-23 02:03:17,836 - INFO - epoch complete!
2024-03-23 02:03:17,837 - INFO - evaluating now!
2024-03-23 02:03:28,562 - INFO - Epoch [151/300] (101688) train_loss: 24.7072, val_loss: 25.2471, lr: 0.000541, 136.63s
2024-03-23 02:03:28,673 - INFO - Saved model at 151
2024-03-23 02:03:28,673 - INFO - Val loss decrease from 25.3734 to 25.2471, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch151.tar
2024-03-23 02:05:35,502 - INFO - epoch complete!
2024-03-23 02:05:35,503 - INFO - evaluating now!
2024-03-23 02:05:46,363 - INFO - Epoch [152/300] (102357) train_loss: 24.6989, val_loss: 25.4864, lr: 0.000536, 137.69s
2024-03-23 02:08:03,274 - INFO - epoch complete!
2024-03-23 02:08:03,274 - INFO - evaluating now!
2024-03-23 02:08:14,152 - INFO - Epoch [153/300] (103026) train_loss: 24.6633, val_loss: 25.5933, lr: 0.000531, 147.79s
2024-03-23 02:10:36,303 - INFO - epoch complete!
2024-03-23 02:10:36,304 - INFO - evaluating now!
2024-03-23 02:10:49,208 - INFO - Epoch [154/300] (103695) train_loss: 24.6880, val_loss: 25.8746, lr: 0.000526, 155.05s
2024-03-23 02:12:58,015 - INFO - epoch complete!
2024-03-23 02:12:58,016 - INFO - evaluating now!
2024-03-23 02:13:09,035 - INFO - Epoch [155/300] (104364) train_loss: 24.6036, val_loss: 25.3515, lr: 0.000522, 139.83s
2024-03-23 02:15:16,317 - INFO - epoch complete!
2024-03-23 02:15:16,317 - INFO - evaluating now!
2024-03-23 02:15:27,115 - INFO - Epoch [156/300] (105033) train_loss: 24.5889, val_loss: 25.3810, lr: 0.000517, 138.08s
2024-03-23 02:17:33,475 - INFO - epoch complete!
2024-03-23 02:17:33,476 - INFO - evaluating now!
2024-03-23 02:17:44,350 - INFO - Epoch [157/300] (105702) train_loss: 24.5242, val_loss: 25.3669, lr: 0.000512, 137.23s
2024-03-23 02:19:50,891 - INFO - epoch complete!
2024-03-23 02:19:50,892 - INFO - evaluating now!
2024-03-23 02:20:01,608 - INFO - Epoch [158/300] (106371) train_loss: 24.5120, val_loss: 25.1559, lr: 0.000508, 137.26s
2024-03-23 02:20:01,718 - INFO - Saved model at 158
2024-03-23 02:20:01,719 - INFO - Val loss decrease from 25.2471 to 25.1559, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch158.tar
2024-03-23 02:22:07,689 - INFO - epoch complete!
2024-03-23 02:22:07,689 - INFO - evaluating now!
2024-03-23 02:22:18,422 - INFO - Epoch [159/300] (107040) train_loss: 24.5964, val_loss: 25.3954, lr: 0.000503, 136.70s
2024-03-23 02:24:24,280 - INFO - epoch complete!
2024-03-23 02:24:24,281 - INFO - evaluating now!
2024-03-23 02:24:35,087 - INFO - Epoch [160/300] (107709) train_loss: 24.5363, val_loss: 25.5542, lr: 0.000498, 136.66s
2024-03-23 02:26:41,136 - INFO - epoch complete!
2024-03-23 02:26:41,137 - INFO - evaluating now!
2024-03-23 02:26:51,998 - INFO - Epoch [161/300] (108378) train_loss: 24.4894, val_loss: 25.3514, lr: 0.000494, 136.91s
2024-03-23 02:28:58,169 - INFO - epoch complete!
2024-03-23 02:28:58,170 - INFO - evaluating now!
2024-03-23 02:29:09,037 - INFO - Epoch [162/300] (109047) train_loss: 24.4582, val_loss: 25.3316, lr: 0.000489, 137.04s
2024-03-23 02:31:19,181 - INFO - epoch complete!
2024-03-23 02:31:19,183 - INFO - evaluating now!
2024-03-23 02:31:30,085 - INFO - Epoch [163/300] (109716) train_loss: 24.4851, val_loss: 25.2675, lr: 0.000484, 141.05s
2024-03-23 02:33:48,436 - INFO - epoch complete!
2024-03-23 02:33:48,437 - INFO - evaluating now!
2024-03-23 02:33:59,251 - INFO - Epoch [164/300] (110385) train_loss: 24.4029, val_loss: 25.3154, lr: 0.000480, 149.17s
2024-03-23 02:36:05,008 - INFO - epoch complete!
2024-03-23 02:36:05,009 - INFO - evaluating now!
2024-03-23 02:36:15,768 - INFO - Epoch [165/300] (111054) train_loss: 24.3643, val_loss: 25.3947, lr: 0.000475, 136.52s
2024-03-23 02:38:21,905 - INFO - epoch complete!
2024-03-23 02:38:21,906 - INFO - evaluating now!
2024-03-23 02:38:32,700 - INFO - Epoch [166/300] (111723) train_loss: 24.3775, val_loss: 25.2573, lr: 0.000470, 136.93s
2024-03-23 02:40:39,068 - INFO - epoch complete!
2024-03-23 02:40:39,069 - INFO - evaluating now!
2024-03-23 02:40:49,846 - INFO - Epoch [167/300] (112392) train_loss: 24.3531, val_loss: 25.4284, lr: 0.000466, 137.14s
2024-03-23 02:42:56,106 - INFO - epoch complete!
2024-03-23 02:42:56,107 - INFO - evaluating now!
2024-03-23 02:43:06,929 - INFO - Epoch [168/300] (113061) train_loss: 24.3362, val_loss: 25.1056, lr: 0.000461, 137.08s
2024-03-23 02:43:07,040 - INFO - Saved model at 168
2024-03-23 02:43:07,041 - INFO - Val loss decrease from 25.1559 to 25.1056, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch168.tar
2024-03-23 02:45:13,539 - INFO - epoch complete!
2024-03-23 02:45:13,539 - INFO - evaluating now!
2024-03-23 02:45:24,333 - INFO - Epoch [169/300] (113730) train_loss: 24.3353, val_loss: 25.3325, lr: 0.000456, 137.29s
2024-03-23 02:47:30,455 - INFO - epoch complete!
2024-03-23 02:47:30,455 - INFO - evaluating now!
2024-03-23 02:47:41,228 - INFO - Epoch [170/300] (114399) train_loss: 24.3367, val_loss: 25.2915, lr: 0.000452, 136.89s
2024-03-23 02:49:52,452 - INFO - epoch complete!
2024-03-23 02:49:52,452 - INFO - evaluating now!
2024-03-23 02:50:03,339 - INFO - Epoch [171/300] (115068) train_loss: 24.3028, val_loss: 25.2192, lr: 0.000447, 142.11s
2024-03-23 02:52:10,804 - INFO - epoch complete!
2024-03-23 02:52:10,805 - INFO - evaluating now!
2024-03-23 02:52:21,587 - INFO - Epoch [172/300] (115737) train_loss: 24.2839, val_loss: 25.4784, lr: 0.000443, 138.25s
2024-03-23 02:54:28,091 - INFO - epoch complete!
2024-03-23 02:54:28,091 - INFO - evaluating now!
2024-03-23 02:54:38,982 - INFO - Epoch [173/300] (116406) train_loss: 24.2083, val_loss: 25.3071, lr: 0.000438, 137.39s
2024-03-23 02:57:08,348 - INFO - epoch complete!
2024-03-23 02:57:08,349 - INFO - evaluating now!
2024-03-23 02:57:21,016 - INFO - Epoch [174/300] (117075) train_loss: 24.1885, val_loss: 25.3493, lr: 0.000434, 162.03s
2024-03-23 02:59:37,465 - INFO - epoch complete!
2024-03-23 02:59:37,466 - INFO - evaluating now!
2024-03-23 02:59:48,254 - INFO - Epoch [175/300] (117744) train_loss: 24.2160, val_loss: 25.5707, lr: 0.000429, 147.24s
2024-03-23 03:01:54,445 - INFO - epoch complete!
2024-03-23 03:01:54,446 - INFO - evaluating now!
2024-03-23 03:02:05,317 - INFO - Epoch [176/300] (118413) train_loss: 24.1584, val_loss: 25.4417, lr: 0.000424, 137.06s
2024-03-23 03:04:11,943 - INFO - epoch complete!
2024-03-23 03:04:11,944 - INFO - evaluating now!
2024-03-23 03:04:22,710 - INFO - Epoch [177/300] (119082) train_loss: 24.1824, val_loss: 25.0424, lr: 0.000420, 137.39s
2024-03-23 03:04:22,821 - INFO - Saved model at 177
2024-03-23 03:04:22,821 - INFO - Val loss decrease from 25.1056 to 25.0424, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch177.tar
2024-03-23 03:06:29,082 - INFO - epoch complete!
2024-03-23 03:06:29,083 - INFO - evaluating now!
2024-03-23 03:06:39,910 - INFO - Epoch [178/300] (119751) train_loss: 24.1396, val_loss: 25.4630, lr: 0.000415, 137.09s
2024-03-23 03:08:45,959 - INFO - epoch complete!
2024-03-23 03:08:45,960 - INFO - evaluating now!
2024-03-23 03:08:56,798 - INFO - Epoch [179/300] (120420) train_loss: 24.1024, val_loss: 25.1428, lr: 0.000411, 136.89s
2024-03-23 03:11:08,477 - INFO - epoch complete!
2024-03-23 03:11:08,478 - INFO - evaluating now!
2024-03-23 03:11:19,263 - INFO - Epoch [180/300] (121089) train_loss: 24.1008, val_loss: 25.4057, lr: 0.000406, 142.46s
2024-03-23 03:13:30,739 - INFO - epoch complete!
2024-03-23 03:13:30,740 - INFO - evaluating now!
2024-03-23 03:13:41,622 - INFO - Epoch [181/300] (121758) train_loss: 24.0555, val_loss: 25.1587, lr: 0.000402, 142.36s
2024-03-23 03:15:48,124 - INFO - epoch complete!
2024-03-23 03:15:48,125 - INFO - evaluating now!
2024-03-23 03:15:58,966 - INFO - Epoch [182/300] (122427) train_loss: 24.0889, val_loss: 25.3131, lr: 0.000398, 137.34s
2024-03-23 03:18:05,412 - INFO - epoch complete!
2024-03-23 03:18:05,413 - INFO - evaluating now!
2024-03-23 03:18:16,274 - INFO - Epoch [183/300] (123096) train_loss: 24.0610, val_loss: 25.2479, lr: 0.000393, 137.31s
2024-03-23 03:20:22,373 - INFO - epoch complete!
2024-03-23 03:20:22,374 - INFO - evaluating now!
2024-03-23 03:20:33,139 - INFO - Epoch [184/300] (123765) train_loss: 24.0291, val_loss: 25.1116, lr: 0.000389, 136.86s
2024-03-23 03:22:39,378 - INFO - epoch complete!
2024-03-23 03:22:39,379 - INFO - evaluating now!
2024-03-23 03:22:50,279 - INFO - Epoch [185/300] (124434) train_loss: 24.0102, val_loss: 25.1188, lr: 0.000384, 137.14s
2024-03-23 03:24:56,630 - INFO - epoch complete!
2024-03-23 03:24:56,630 - INFO - evaluating now!
2024-03-23 03:25:07,450 - INFO - Epoch [186/300] (125103) train_loss: 23.9946, val_loss: 25.2199, lr: 0.000380, 137.17s
2024-03-23 03:27:13,714 - INFO - epoch complete!
2024-03-23 03:27:13,714 - INFO - evaluating now!
2024-03-23 03:27:24,509 - INFO - Epoch [187/300] (125772) train_loss: 23.9995, val_loss: 25.2744, lr: 0.000376, 137.06s
2024-03-23 03:29:30,942 - INFO - epoch complete!
2024-03-23 03:29:30,943 - INFO - evaluating now!
2024-03-23 03:29:41,785 - INFO - Epoch [188/300] (126441) train_loss: 23.9502, val_loss: 25.0123, lr: 0.000371, 137.27s
2024-03-23 03:29:41,892 - INFO - Saved model at 188
2024-03-23 03:29:41,893 - INFO - Val loss decrease from 25.0424 to 25.0123, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch188.tar
2024-03-23 03:31:48,143 - INFO - epoch complete!
2024-03-23 03:31:48,144 - INFO - evaluating now!
2024-03-23 03:31:58,960 - INFO - Epoch [189/300] (127110) train_loss: 23.8787, val_loss: 25.2381, lr: 0.000367, 137.07s
2024-03-23 03:34:14,323 - INFO - epoch complete!
2024-03-23 03:34:14,324 - INFO - evaluating now!
2024-03-23 03:34:25,117 - INFO - Epoch [190/300] (127779) train_loss: 23.9091, val_loss: 25.4264, lr: 0.000363, 146.16s
2024-03-23 03:36:31,579 - INFO - epoch complete!
2024-03-23 03:36:31,580 - INFO - evaluating now!
2024-03-23 03:36:42,355 - INFO - Epoch [191/300] (128448) train_loss: 23.8864, val_loss: 25.3350, lr: 0.000358, 137.24s
2024-03-23 03:38:48,575 - INFO - epoch complete!
2024-03-23 03:38:48,576 - INFO - evaluating now!
2024-03-23 03:38:59,452 - INFO - Epoch [192/300] (129117) train_loss: 23.8840, val_loss: 25.0329, lr: 0.000354, 137.10s
2024-03-23 03:41:17,835 - INFO - epoch complete!
2024-03-23 03:41:17,836 - INFO - evaluating now!
2024-03-23 03:41:28,631 - INFO - Epoch [193/300] (129786) train_loss: 23.8580, val_loss: 25.3953, lr: 0.000350, 149.18s
2024-03-23 03:43:35,196 - INFO - epoch complete!
2024-03-23 03:43:35,197 - INFO - evaluating now!
2024-03-23 03:43:45,949 - INFO - Epoch [194/300] (130455) train_loss: 23.8285, val_loss: 24.9301, lr: 0.000346, 137.32s
2024-03-23 03:43:46,059 - INFO - Saved model at 194
2024-03-23 03:43:46,060 - INFO - Val loss decrease from 25.0123 to 24.9301, saving to ./libcity/cache/37697/model_cache/PDFormer_PeMS08_epoch194.tar
2024-03-23 03:45:52,265 - INFO - epoch complete!
2024-03-23 03:45:52,266 - INFO - evaluating now!
2024-03-23 03:46:03,136 - INFO - Epoch [195/300] (131124) train_loss: 23.8073, val_loss: 25.1533, lr: 0.000342, 137.08s
2024-03-23 03:48:10,675 - INFO - epoch complete!
2024-03-23 03:48:10,676 - INFO - evaluating now!
2024-03-23 03:48:21,621 - INFO - Epoch [196/300] (131793) train_loss: 23.7891, val_loss: 25.2357, lr: 0.000337, 138.48s
2024-03-23 03:50:28,346 - INFO - epoch complete!
2024-03-23 03:50:28,346 - INFO - evaluating now!
2024-03-23 03:50:39,204 - INFO - Epoch [197/300] (132462) train_loss: 23.7815, val_loss: 25.1793, lr: 0.000333, 137.58s
2024-03-23 03:52:52,157 - INFO - epoch complete!
2024-03-23 03:52:52,158 - INFO - evaluating now!
2024-03-23 03:53:03,013 - INFO - Epoch [198/300] (133131) train_loss: 23.7339, val_loss: 25.1352, lr: 0.000329, 143.81s
2024-03-23 03:55:10,449 - INFO - epoch complete!
2024-03-23 03:55:10,450 - INFO - evaluating now!
2024-03-23 03:55:21,294 - INFO - Epoch [199/300] (133800) train_loss: 23.7384, val_loss: 25.5047, lr: 0.000325, 138.28s
2024-03-23 03:57:38,085 - INFO - epoch complete!
2024-03-23 03:57:38,085 - INFO - evaluating now!
2024-03-23 03:57:48,916 - INFO - Epoch [200/300] (134469) train_loss: 23.7158, val_loss: 25.0964, lr: 0.000321, 147.62s
2024-03-23 03:59:54,740 - INFO - epoch complete!
2024-03-23 03:59:54,741 - INFO - evaluating now!
2024-03-23 04:00:05,533 - INFO - Epoch [201/300] (135138) train_loss: 23.7045, val_loss: 25.2193, lr: 0.000317, 136.62s
2024-03-23 04:02:11,268 - INFO - epoch complete!
2024-03-23 04:02:11,269 - INFO - evaluating now!
2024-03-23 04:02:22,092 - INFO - Epoch [202/300] (135807) train_loss: 23.6926, val_loss: 25.3697, lr: 0.000313, 136.56s
2024-03-23 04:04:28,020 - INFO - epoch complete!
2024-03-23 04:04:28,021 - INFO - evaluating now!
2024-03-23 04:04:38,861 - INFO - Epoch [203/300] (136476) train_loss: 23.6689, val_loss: 25.3888, lr: 0.000309, 136.77s
2024-03-23 04:06:45,380 - INFO - epoch complete!
2024-03-23 04:06:45,381 - INFO - evaluating now!
2024-03-23 04:06:56,221 - INFO - Epoch [204/300] (137145) train_loss: 23.6592, val_loss: 25.1840, lr: 0.000305, 137.36s
2024-03-23 04:09:17,018 - INFO - epoch complete!
2024-03-23 04:09:17,019 - INFO - evaluating now!
2024-03-23 04:09:27,860 - INFO - Epoch [205/300] (137814) train_loss: 23.6263, val_loss: 25.0455, lr: 0.000301, 151.64s
2024-03-23 04:11:34,307 - INFO - epoch complete!
2024-03-23 04:11:34,308 - INFO - evaluating now!
2024-03-23 04:11:45,197 - INFO - Epoch [206/300] (138483) train_loss: 23.5995, val_loss: 25.2630, lr: 0.000297, 137.34s
2024-03-23 04:13:52,001 - INFO - epoch complete!
2024-03-23 04:13:52,002 - INFO - evaluating now!
2024-03-23 04:14:02,859 - INFO - Epoch [207/300] (139152) train_loss: 23.5792, val_loss: 25.2136, lr: 0.000293, 137.66s
2024-03-23 04:16:15,869 - INFO - epoch complete!
2024-03-23 04:16:15,870 - INFO - evaluating now!
2024-03-23 04:16:26,730 - INFO - Epoch [208/300] (139821) train_loss: 23.5658, val_loss: 25.5098, lr: 0.000289, 143.87s
2024-03-23 04:18:33,276 - INFO - epoch complete!
2024-03-23 04:18:33,276 - INFO - evaluating now!
2024-03-23 04:18:44,123 - INFO - Epoch [209/300] (140490) train_loss: 23.5771, val_loss: 25.0066, lr: 0.000285, 137.39s
2024-03-23 04:20:50,516 - INFO - epoch complete!
2024-03-23 04:20:50,517 - INFO - evaluating now!
2024-03-23 04:21:01,343 - INFO - Epoch [210/300] (141159) train_loss: 23.5509, val_loss: 25.1282, lr: 0.000282, 137.22s
2024-03-23 04:23:07,832 - INFO - epoch complete!
2024-03-23 04:23:07,833 - INFO - evaluating now!
2024-03-23 04:23:18,634 - INFO - Epoch [211/300] (141828) train_loss: 23.5179, val_loss: 25.3880, lr: 0.000278, 137.29s
2024-03-23 04:25:25,224 - INFO - epoch complete!
2024-03-23 04:25:25,224 - INFO - evaluating now!
2024-03-23 04:25:36,170 - INFO - Epoch [212/300] (142497) train_loss: 23.5006, val_loss: 25.0786, lr: 0.000274, 137.54s
2024-03-23 04:27:44,845 - INFO - epoch complete!
2024-03-23 04:27:44,846 - INFO - evaluating now!
2024-03-23 04:27:55,672 - INFO - Epoch [213/300] (143166) train_loss: 23.5058, val_loss: 25.2970, lr: 0.000270, 139.50s
2024-03-23 04:30:06,786 - INFO - epoch complete!
2024-03-23 04:30:06,786 - INFO - evaluating now!
2024-03-23 04:30:17,626 - INFO - Epoch [214/300] (143835) train_loss: 23.4854, val_loss: 25.1985, lr: 0.000267, 141.95s
2024-03-23 04:32:42,068 - INFO - epoch complete!
2024-03-23 04:32:42,070 - INFO - evaluating now!
2024-03-23 04:32:52,864 - INFO - Epoch [215/300] (144504) train_loss: 23.4663, val_loss: 25.0453, lr: 0.000263, 155.24s
2024-03-23 04:35:04,185 - INFO - epoch complete!
2024-03-23 04:35:04,186 - INFO - evaluating now!
2024-03-23 04:35:15,041 - INFO - Epoch [216/300] (145173) train_loss: 23.4658, val_loss: 25.0582, lr: 0.000260, 142.18s
2024-03-23 04:37:21,272 - INFO - epoch complete!
2024-03-23 04:37:21,273 - INFO - evaluating now!
2024-03-23 04:37:32,087 - INFO - Epoch [217/300] (145842) train_loss: 23.4222, val_loss: 25.2062, lr: 0.000256, 137.05s
2024-03-23 04:39:38,349 - INFO - epoch complete!
2024-03-23 04:39:38,350 - INFO - evaluating now!
2024-03-23 04:39:49,228 - INFO - Epoch [218/300] (146511) train_loss: 23.4290, val_loss: 24.9639, lr: 0.000252, 137.14s
2024-03-23 04:42:07,035 - INFO - epoch complete!
2024-03-23 04:42:07,036 - INFO - evaluating now!
2024-03-23 04:42:17,830 - INFO - Epoch [219/300] (147180) train_loss: 23.3977, val_loss: 25.0312, lr: 0.000249, 148.60s
2024-03-23 04:44:24,415 - INFO - epoch complete!
2024-03-23 04:44:24,416 - INFO - evaluating now!
2024-03-23 04:44:35,242 - INFO - Epoch [220/300] (147849) train_loss: 23.3922, val_loss: 25.1057, lr: 0.000245, 137.41s
2024-03-23 04:46:49,756 - INFO - epoch complete!
2024-03-23 04:46:49,757 - INFO - evaluating now!
2024-03-23 04:47:00,560 - INFO - Epoch [221/300] (148518) train_loss: 23.3307, val_loss: 25.1121, lr: 0.000242, 145.32s
2024-03-23 04:49:07,089 - INFO - epoch complete!
2024-03-23 04:49:07,089 - INFO - evaluating now!
2024-03-23 04:49:17,927 - INFO - Epoch [222/300] (149187) train_loss: 23.3462, val_loss: 24.9994, lr: 0.000239, 137.37s
2024-03-23 04:51:32,873 - INFO - epoch complete!
2024-03-23 04:51:32,873 - INFO - evaluating now!
2024-03-23 04:51:43,888 - INFO - Epoch [223/300] (149856) train_loss: 23.3435, val_loss: 25.1792, lr: 0.000235, 145.96s
2024-03-23 04:53:57,282 - INFO - epoch complete!
2024-03-23 04:53:57,283 - INFO - evaluating now!
2024-03-23 04:54:08,146 - INFO - Epoch [224/300] (150525) train_loss: 23.3015, val_loss: 25.1091, lr: 0.000232, 144.26s
2024-03-23 04:56:15,600 - INFO - epoch complete!
2024-03-23 04:56:15,600 - INFO - evaluating now!
2024-03-23 04:56:26,477 - INFO - Epoch [225/300] (151194) train_loss: 23.2916, val_loss: 24.9942, lr: 0.000228, 138.33s
2024-03-23 04:58:56,555 - INFO - epoch complete!
2024-03-23 04:58:56,556 - INFO - evaluating now!
2024-03-23 04:59:09,352 - INFO - Epoch [226/300] (151863) train_loss: 23.2800, val_loss: 25.1302, lr: 0.000225, 162.87s
2024-03-23 05:01:24,095 - INFO - epoch complete!
2024-03-23 05:01:24,096 - INFO - evaluating now!
2024-03-23 05:01:35,041 - INFO - Epoch [227/300] (152532) train_loss: 23.2423, val_loss: 25.3249, lr: 0.000222, 145.69s
2024-03-23 05:03:43,955 - INFO - epoch complete!
2024-03-23 05:03:43,955 - INFO - evaluating now!
2024-03-23 05:03:54,809 - INFO - Epoch [228/300] (153201) train_loss: 23.2507, val_loss: 25.0717, lr: 0.000219, 139.77s
2024-03-23 05:06:01,742 - INFO - epoch complete!
2024-03-23 05:06:01,743 - INFO - evaluating now!
2024-03-23 05:06:12,631 - INFO - Epoch [229/300] (153870) train_loss: 23.2167, val_loss: 25.0457, lr: 0.000216, 137.82s
2024-03-23 05:08:20,025 - INFO - epoch complete!
2024-03-23 05:08:20,026 - INFO - evaluating now!
2024-03-23 05:08:31,002 - INFO - Epoch [230/300] (154539) train_loss: 23.2200, val_loss: 25.2082, lr: 0.000212, 138.37s
2024-03-23 05:10:38,400 - INFO - epoch complete!
2024-03-23 05:10:38,401 - INFO - evaluating now!
2024-03-23 05:10:49,367 - INFO - Epoch [231/300] (155208) train_loss: 23.1942, val_loss: 25.1596, lr: 0.000209, 138.36s
2024-03-23 05:12:56,908 - INFO - epoch complete!
2024-03-23 05:12:56,909 - INFO - evaluating now!
2024-03-23 05:13:07,884 - INFO - Epoch [232/300] (155877) train_loss: 23.2119, val_loss: 25.2011, lr: 0.000206, 138.52s
2024-03-23 05:15:15,014 - INFO - epoch complete!
2024-03-23 05:15:15,015 - INFO - evaluating now!
2024-03-23 05:15:25,983 - INFO - Epoch [233/300] (156546) train_loss: 23.1870, val_loss: 25.0174, lr: 0.000203, 138.10s
2024-03-23 05:17:33,044 - INFO - epoch complete!
2024-03-23 05:17:33,044 - INFO - evaluating now!
2024-03-23 05:17:43,965 - INFO - Epoch [234/300] (157215) train_loss: 23.1617, val_loss: 25.0061, lr: 0.000200, 137.98s
2024-03-23 05:19:51,330 - INFO - epoch complete!
2024-03-23 05:19:51,330 - INFO - evaluating now!
2024-03-23 05:20:02,225 - INFO - Epoch [235/300] (157884) train_loss: 23.1383, val_loss: 25.0686, lr: 0.000197, 138.26s
2024-03-23 05:22:09,474 - INFO - epoch complete!
2024-03-23 05:22:09,475 - INFO - evaluating now!
2024-03-23 05:22:20,431 - INFO - Epoch [236/300] (158553) train_loss: 23.1410, val_loss: 25.0887, lr: 0.000194, 138.21s
2024-03-23 05:24:27,695 - INFO - epoch complete!
2024-03-23 05:24:27,696 - INFO - evaluating now!
2024-03-23 05:24:38,658 - INFO - Epoch [237/300] (159222) train_loss: 23.1248, val_loss: 25.1741, lr: 0.000192, 138.23s
2024-03-23 05:26:46,340 - INFO - epoch complete!
2024-03-23 05:26:46,341 - INFO - evaluating now!
2024-03-23 05:26:57,226 - INFO - Epoch [238/300] (159891) train_loss: 23.1117, val_loss: 25.0037, lr: 0.000189, 138.57s
2024-03-23 05:29:04,842 - INFO - epoch complete!
2024-03-23 05:29:04,843 - INFO - evaluating now!
2024-03-23 05:29:15,808 - INFO - Epoch [239/300] (160560) train_loss: 23.0853, val_loss: 25.0578, lr: 0.000186, 138.58s
2024-03-23 05:31:27,425 - INFO - epoch complete!
2024-03-23 05:31:27,426 - INFO - evaluating now!
2024-03-23 05:31:38,332 - INFO - Epoch [240/300] (161229) train_loss: 23.0761, val_loss: 25.2191, lr: 0.000183, 142.52s
2024-03-23 05:33:45,810 - INFO - epoch complete!
2024-03-23 05:33:45,811 - INFO - evaluating now!
2024-03-23 05:33:56,705 - INFO - Epoch [241/300] (161898) train_loss: 23.0678, val_loss: 25.1305, lr: 0.000180, 138.37s
2024-03-23 05:36:06,325 - INFO - epoch complete!
2024-03-23 05:36:06,325 - INFO - evaluating now!
2024-03-23 05:36:17,322 - INFO - Epoch [242/300] (162567) train_loss: 23.0523, val_loss: 25.1823, lr: 0.000178, 140.62s
2024-03-23 05:38:25,518 - INFO - epoch complete!
2024-03-23 05:38:25,519 - INFO - evaluating now!
2024-03-23 05:38:36,452 - INFO - Epoch [243/300] (163236) train_loss: 23.0500, val_loss: 25.1694, lr: 0.000175, 139.13s
2024-03-23 05:40:43,509 - INFO - epoch complete!
2024-03-23 05:40:43,510 - INFO - evaluating now!
2024-03-23 05:40:54,469 - INFO - Epoch [244/300] (163905) train_loss: 23.0422, val_loss: 25.0833, lr: 0.000173, 138.02s
2024-03-23 05:40:54,470 - WARNING - Early stopping at epoch: 244
2024-03-23 05:40:54,470 - INFO - Trained totally 245 epochs, average train time is 128.719s, average eval time is 10.901s
2024-03-23 05:40:54,598 - INFO - Loaded model at 194
2024-03-23 05:40:54,600 - INFO - Saved model at ./libcity/cache/37697/model_cache/PDFormer_PeMS08.m
2024-03-23 05:40:54,708 - INFO - Start evaluating ...
2024-03-23 05:41:23,219 - INFO - Note that you select the average mode to evaluate!
2024-03-23 05:41:23,228 - INFO - Evaluate result is saved at ./libcity/cache/37697/evaluate_cache/2024_03_23_05_41_23_PDFormer_PeMS08_average.csv
2024-03-23 05:41:23,247 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   11.672573   inf  19.573837   11.688436     0.077519    19.466642
2   11.865945   inf  20.112122   11.882353     0.078554    20.011499
3   12.044495   inf  20.548969   12.061327     0.079542    20.449841
4   12.209836   inf  20.937180   12.227073     0.080541    20.841272
5   12.362706   inf  21.281240   12.380534     0.081440    21.188631
6   12.501812   inf  21.591347   12.520030     0.082327    21.499641
7   12.632783   inf  21.875559   12.651315     0.083178    21.784386
8   12.753807   inf  22.134674   12.772621     0.084002    22.044926
9   12.876631   inf  22.382210   12.895651     0.084862    22.292490
10  12.989834   inf  22.604555   13.009078     0.085678    22.515411
11  13.108047   inf  22.807644   13.127483     0.086556    22.718706
12  13.236529   inf  23.022512   13.256317     0.087430    22.934391
