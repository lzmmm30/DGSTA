2024-03-05 10:36:51,588 - INFO - Log directory: ./libcity/log
2024-03-05 10:36:51,589 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS08, exp_id=38389
2024-03-05 10:36:51,589 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS08', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 2776, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 21, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 6, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 2, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS08'], 'geo_file': 'PeMS08', 'rel_file': 'PeMS08', 'adp_file': 'PeMS08', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 38389}
2024-03-05 10:36:52,261 - INFO - Loaded file PeMS08.geo, num_nodes=170
2024-03-05 10:36:52,265 - INFO - set_weight_link_or_dist: link
2024-03-05 10:36:52,265 - INFO - init_weight_inf_or_zero: zero
2024-03-05 10:36:52,270 - INFO - Loaded file PeMS08.rel, shape=(170, 170)
2024-03-05 10:36:52,270 - INFO - Max adj_mx value = 1.0
2024-03-05 10:37:19,043 - INFO - Loading file PeMS08.dyna
2024-03-05 10:37:23,933 - INFO - Loaded file PeMS08.dyna, shape=(17856, 170, 1)
2024-03-05 10:37:23,964 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS08.npy
2024-03-05 10:37:23,966 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS08_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-03-05 10:37:40,405 - INFO - train	x: (10700, 12, 170, 9), y: (10700, 12, 170, 9), ind: (10700,)
2024-03-05 10:37:40,406 - INFO - eval	x: (3566, 12, 170, 9), y: (3566, 12, 170, 9), ind: (3566,)
2024-03-05 10:37:40,406 - INFO - test	x: (3567, 12, 170, 9), y: (3567, 12, 170, 9), ind: (3567,)
2024-03-05 10:37:41,084 - INFO - StandardScaler mean: 229.8431355598314, std: 145.62553066568907
2024-03-05 10:37:41,085 - INFO - NoneScaler
2024-03-05 10:37:42,718 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS08_21_3_16_5.npy
2024-03-05 10:37:49,864 - INFO - Use use_curriculum_learning!
2024-03-05 10:37:49,973 - INFO - Number of isolated points: 0
2024-03-05 10:37:50,002 - INFO - Number of isolated points: 0
2024-03-05 10:37:50,128 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-03-05 10:37:50,134 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,135 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,136 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,137 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,138 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,139 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,140 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,141 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,142 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,143 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,144 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,155 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,155 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,156 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,157 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,158 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-05 10:37:50,159 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-05 10:37:50,160 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,161 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,162 - INFO - encoder_blocks.5.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,163 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-05 10:37:50,164 - INFO - encoder_blocks.5.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,165 - INFO - skip_convs.5.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-05 10:37:50,166 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2024-03-05 10:37:50,166 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-03-05 10:37:50,166 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-03-05 10:37:50,166 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-03-05 10:37:50,166 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-03-05 10:37:50,168 - INFO - Total parameter numbers: 1104093
2024-03-05 10:37:50,172 - INFO - You select `adamw` optimizer.
2024-03-05 10:37:50,175 - INFO - You select `cosinelr` lr_scheduler.
2024-03-05 10:37:50,175 - WARNING - Received none train loss func and will use the loss func defined in the model.
2024-03-05 10:37:50,179 - INFO - Number of isolated points: 0
2024-03-05 10:37:50,214 - INFO - Start training ...
2024-03-05 10:37:50,214 - INFO - num_batches:669
2024-03-05 10:37:50,385 - INFO - Training: task_level increase from 0 to 1
2024-03-05 10:37:50,385 - INFO - Current batches_seen is 0
2024-03-05 10:39:49,906 - INFO - epoch complete!
2024-03-05 10:39:49,907 - INFO - evaluating now!
2024-03-05 10:40:00,375 - INFO - Epoch [0/300] (669) train_loss: 249.6016, val_loss: 249.5215, lr: 0.000201, 130.16s
2024-03-05 10:40:00,484 - INFO - Saved model at 0
2024-03-05 10:40:00,485 - INFO - Val loss decrease from inf to 249.5215, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch0.tar
2024-03-05 10:42:00,096 - INFO - epoch complete!
2024-03-05 10:42:00,097 - INFO - evaluating now!
2024-03-05 10:42:12,198 - INFO - Epoch [1/300] (1338) train_loss: 53.6999, val_loss: 193.4399, lr: 0.000401, 131.71s
2024-03-05 10:42:12,301 - INFO - Saved model at 1
2024-03-05 10:42:12,302 - INFO - Val loss decrease from 249.5215 to 193.4399, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch1.tar
2024-03-05 10:44:12,266 - INFO - epoch complete!
2024-03-05 10:44:12,267 - INFO - evaluating now!
2024-03-05 10:44:22,956 - INFO - Epoch [2/300] (2007) train_loss: 36.5272, val_loss: 194.0797, lr: 0.000600, 130.65s
2024-03-05 10:46:22,444 - INFO - epoch complete!
2024-03-05 10:46:22,445 - INFO - evaluating now!
2024-03-05 10:46:32,938 - INFO - Epoch [3/300] (2676) train_loss: 32.2816, val_loss: 194.4836, lr: 0.000800, 129.98s
2024-03-05 10:46:50,809 - INFO - Training: task_level increase from 1 to 2
2024-03-05 10:46:50,809 - INFO - Current batches_seen is 2776
2024-03-05 10:48:32,780 - INFO - epoch complete!
2024-03-05 10:48:32,781 - INFO - evaluating now!
2024-03-05 10:48:43,232 - INFO - Epoch [4/300] (3345) train_loss: 36.5244, val_loss: 170.4669, lr: 0.000999, 130.29s
2024-03-05 10:48:43,340 - INFO - Saved model at 4
2024-03-05 10:48:43,341 - INFO - Val loss decrease from 193.4399 to 170.4669, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch4.tar
2024-03-05 10:50:43,151 - INFO - epoch complete!
2024-03-05 10:50:43,152 - INFO - evaluating now!
2024-03-05 10:50:53,576 - INFO - Epoch [5/300] (4014) train_loss: 31.0420, val_loss: 171.8354, lr: 0.000999, 130.23s
2024-03-05 10:52:52,789 - INFO - epoch complete!
2024-03-05 10:52:52,789 - INFO - evaluating now!
2024-03-05 10:53:02,633 - INFO - Epoch [6/300] (4683) train_loss: 30.3921, val_loss: 172.5812, lr: 0.000999, 129.06s
2024-03-05 10:55:01,465 - INFO - epoch complete!
2024-03-05 10:55:01,465 - INFO - evaluating now!
2024-03-05 10:55:13,623 - INFO - Epoch [7/300] (5352) train_loss: 29.6418, val_loss: 173.1912, lr: 0.000998, 130.99s
2024-03-05 10:55:49,558 - INFO - Training: task_level increase from 2 to 3
2024-03-05 10:55:49,558 - INFO - Current batches_seen is 5552
2024-03-05 10:57:13,380 - INFO - epoch complete!
2024-03-05 10:57:13,381 - INFO - evaluating now!
2024-03-05 10:57:23,913 - INFO - Epoch [8/300] (6021) train_loss: 30.8323, val_loss: 155.2316, lr: 0.000998, 130.29s
2024-03-05 10:57:24,020 - INFO - Saved model at 8
2024-03-05 10:57:24,021 - INFO - Val loss decrease from 170.4669 to 155.2316, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch8.tar
2024-03-05 10:59:23,194 - INFO - epoch complete!
2024-03-05 10:59:23,195 - INFO - evaluating now!
2024-03-05 10:59:33,622 - INFO - Epoch [9/300] (6690) train_loss: 29.4977, val_loss: 155.4084, lr: 0.000998, 129.60s
2024-03-05 11:01:33,205 - INFO - epoch complete!
2024-03-05 11:01:33,206 - INFO - evaluating now!
2024-03-05 11:01:43,672 - INFO - Epoch [10/300] (7359) train_loss: 29.2725, val_loss: 156.8124, lr: 0.000997, 130.05s
2024-03-05 11:03:40,709 - INFO - epoch complete!
2024-03-05 11:03:40,710 - INFO - evaluating now!
2024-03-05 11:03:51,143 - INFO - Epoch [11/300] (8028) train_loss: 28.7100, val_loss: 155.6366, lr: 0.000996, 127.47s
2024-03-05 11:04:44,724 - INFO - Training: task_level increase from 3 to 4
2024-03-05 11:04:44,724 - INFO - Current batches_seen is 8328
2024-03-05 11:05:50,459 - INFO - epoch complete!
2024-03-05 11:05:50,459 - INFO - evaluating now!
2024-03-05 11:06:00,934 - INFO - Epoch [12/300] (8697) train_loss: 29.8868, val_loss: 143.1933, lr: 0.000996, 129.79s
2024-03-05 11:06:01,036 - INFO - Saved model at 12
2024-03-05 11:06:01,036 - INFO - Val loss decrease from 155.2316 to 143.1933, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch12.tar
2024-03-05 11:08:00,629 - INFO - epoch complete!
2024-03-05 11:08:00,630 - INFO - evaluating now!
2024-03-05 11:08:11,195 - INFO - Epoch [13/300] (9366) train_loss: 29.7886, val_loss: 142.8726, lr: 0.000995, 130.16s
2024-03-05 11:08:11,305 - INFO - Saved model at 13
2024-03-05 11:08:11,305 - INFO - Val loss decrease from 143.1933 to 142.8726, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch13.tar
2024-03-05 11:10:10,661 - INFO - epoch complete!
2024-03-05 11:10:10,662 - INFO - evaluating now!
2024-03-05 11:10:21,171 - INFO - Epoch [14/300] (10035) train_loss: 29.2458, val_loss: 142.4150, lr: 0.000994, 129.86s
2024-03-05 11:10:21,278 - INFO - Saved model at 14
2024-03-05 11:10:21,279 - INFO - Val loss decrease from 142.8726 to 142.4150, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch14.tar
2024-03-05 11:12:20,934 - INFO - epoch complete!
2024-03-05 11:12:20,935 - INFO - evaluating now!
2024-03-05 11:12:31,365 - INFO - Epoch [15/300] (10704) train_loss: 28.7647, val_loss: 142.0583, lr: 0.000994, 130.09s
2024-03-05 11:12:31,471 - INFO - Saved model at 15
2024-03-05 11:12:31,472 - INFO - Val loss decrease from 142.4150 to 142.0583, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch15.tar
2024-03-05 11:13:42,786 - INFO - Training: task_level increase from 4 to 5
2024-03-05 11:13:42,786 - INFO - Current batches_seen is 11104
2024-03-05 11:14:30,851 - INFO - epoch complete!
2024-03-05 11:14:30,851 - INFO - evaluating now!
2024-03-05 11:14:41,375 - INFO - Epoch [16/300] (11373) train_loss: 29.5940, val_loss: 126.7829, lr: 0.000993, 129.90s
2024-03-05 11:14:41,483 - INFO - Saved model at 16
2024-03-05 11:14:41,483 - INFO - Val loss decrease from 142.0583 to 126.7829, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch16.tar
2024-03-05 11:16:41,189 - INFO - epoch complete!
2024-03-05 11:16:41,190 - INFO - evaluating now!
2024-03-05 11:16:51,633 - INFO - Epoch [17/300] (12042) train_loss: 29.5571, val_loss: 125.7148, lr: 0.000992, 130.15s
2024-03-05 11:16:51,739 - INFO - Saved model at 17
2024-03-05 11:16:51,740 - INFO - Val loss decrease from 126.7829 to 125.7148, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch17.tar
2024-03-05 11:18:51,251 - INFO - epoch complete!
2024-03-05 11:18:51,251 - INFO - evaluating now!
2024-03-05 11:19:01,735 - INFO - Epoch [18/300] (12711) train_loss: 28.9671, val_loss: 125.0782, lr: 0.000991, 129.99s
2024-03-05 11:19:01,844 - INFO - Saved model at 18
2024-03-05 11:19:01,845 - INFO - Val loss decrease from 125.7148 to 125.0782, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch18.tar
2024-03-05 11:21:01,336 - INFO - epoch complete!
2024-03-05 11:21:01,337 - INFO - evaluating now!
2024-03-05 11:21:12,037 - INFO - Epoch [19/300] (13380) train_loss: 28.8028, val_loss: 125.9038, lr: 0.000990, 130.19s
2024-03-05 11:22:41,000 - INFO - Training: task_level increase from 5 to 6
2024-03-05 11:22:41,001 - INFO - Current batches_seen is 13880
2024-03-05 11:23:10,940 - INFO - epoch complete!
2024-03-05 11:23:10,941 - INFO - evaluating now!
2024-03-05 11:23:21,487 - INFO - Epoch [20/300] (14049) train_loss: 28.7108, val_loss: 122.4333, lr: 0.000989, 129.45s
2024-03-05 11:23:21,594 - INFO - Saved model at 20
2024-03-05 11:23:21,595 - INFO - Val loss decrease from 125.0782 to 122.4333, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch20.tar
2024-03-05 11:25:20,684 - INFO - epoch complete!
2024-03-05 11:25:20,685 - INFO - evaluating now!
2024-03-05 11:25:31,190 - INFO - Epoch [21/300] (14718) train_loss: 29.0921, val_loss: 122.8024, lr: 0.000988, 129.59s
2024-03-05 11:27:31,129 - INFO - epoch complete!
2024-03-05 11:27:31,130 - INFO - evaluating now!
2024-03-05 11:27:41,597 - INFO - Epoch [22/300] (15387) train_loss: 28.9257, val_loss: 122.7364, lr: 0.000987, 130.41s
2024-03-05 11:29:40,964 - INFO - epoch complete!
2024-03-05 11:29:40,965 - INFO - evaluating now!
2024-03-05 11:29:51,359 - INFO - Epoch [23/300] (16056) train_loss: 28.4852, val_loss: 122.7618, lr: 0.000986, 129.76s
2024-03-05 11:31:39,081 - INFO - Training: task_level increase from 6 to 7
2024-03-05 11:31:39,081 - INFO - Current batches_seen is 16656
2024-03-05 11:31:51,379 - INFO - epoch complete!
2024-03-05 11:31:51,379 - INFO - evaluating now!
2024-03-05 11:32:01,857 - INFO - Epoch [24/300] (16725) train_loss: 29.1132, val_loss: 107.9509, lr: 0.000985, 130.50s
2024-03-05 11:32:01,965 - INFO - Saved model at 24
2024-03-05 11:32:01,965 - INFO - Val loss decrease from 122.4333 to 107.9509, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch24.tar
2024-03-05 11:34:00,852 - INFO - epoch complete!
2024-03-05 11:34:00,853 - INFO - evaluating now!
2024-03-05 11:34:11,380 - INFO - Epoch [25/300] (17394) train_loss: 28.8961, val_loss: 106.2498, lr: 0.000983, 129.41s
2024-03-05 11:34:11,484 - INFO - Saved model at 25
2024-03-05 11:34:11,485 - INFO - Val loss decrease from 107.9509 to 106.2498, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch25.tar
2024-03-05 11:36:09,057 - INFO - epoch complete!
2024-03-05 11:36:09,058 - INFO - evaluating now!
2024-03-05 11:36:19,224 - INFO - Epoch [26/300] (18063) train_loss: 28.5956, val_loss: 106.5885, lr: 0.000982, 127.74s
2024-03-05 11:38:18,610 - INFO - epoch complete!
2024-03-05 11:38:18,610 - INFO - evaluating now!
2024-03-05 11:38:29,085 - INFO - Epoch [27/300] (18732) train_loss: 28.3667, val_loss: 107.1906, lr: 0.000981, 129.86s
2024-03-05 11:40:28,844 - INFO - epoch complete!
2024-03-05 11:40:28,844 - INFO - evaluating now!
2024-03-05 11:40:39,264 - INFO - Epoch [28/300] (19401) train_loss: 28.2482, val_loss: 106.2423, lr: 0.000979, 130.18s
2024-03-05 11:40:39,365 - INFO - Saved model at 28
2024-03-05 11:40:39,366 - INFO - Val loss decrease from 106.2498 to 106.2423, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch28.tar
2024-03-05 11:40:44,928 - INFO - Training: task_level increase from 7 to 8
2024-03-05 11:40:44,928 - INFO - Current batches_seen is 19432
2024-03-05 11:42:38,826 - INFO - epoch complete!
2024-03-05 11:42:38,827 - INFO - evaluating now!
2024-03-05 11:42:49,385 - INFO - Epoch [29/300] (20070) train_loss: 28.9889, val_loss: 91.3275, lr: 0.000978, 130.02s
2024-03-05 11:42:49,485 - INFO - Saved model at 29
2024-03-05 11:42:49,486 - INFO - Val loss decrease from 106.2423 to 91.3275, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch29.tar
2024-03-05 11:44:49,366 - INFO - epoch complete!
2024-03-05 11:44:49,367 - INFO - evaluating now!
2024-03-05 11:44:59,935 - INFO - Epoch [30/300] (20739) train_loss: 28.5484, val_loss: 92.0058, lr: 0.000976, 130.45s
2024-03-05 11:46:59,734 - INFO - epoch complete!
2024-03-05 11:46:59,734 - INFO - evaluating now!
2024-03-05 11:47:10,369 - INFO - Epoch [31/300] (21408) train_loss: 28.2271, val_loss: 90.9708, lr: 0.000975, 130.43s
2024-03-05 11:47:10,479 - INFO - Saved model at 31
2024-03-05 11:47:10,480 - INFO - Val loss decrease from 91.3275 to 90.9708, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch31.tar
2024-03-05 11:49:10,497 - INFO - epoch complete!
2024-03-05 11:49:10,497 - INFO - evaluating now!
2024-03-05 11:49:21,158 - INFO - Epoch [32/300] (22077) train_loss: 28.1571, val_loss: 90.7948, lr: 0.000973, 130.68s
2024-03-05 11:49:21,267 - INFO - Saved model at 32
2024-03-05 11:49:21,268 - INFO - Val loss decrease from 90.9708 to 90.7948, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch32.tar
2024-03-05 11:49:44,804 - INFO - Training: task_level increase from 8 to 9
2024-03-05 11:49:44,805 - INFO - Current batches_seen is 22208
2024-03-05 11:51:20,945 - INFO - epoch complete!
2024-03-05 11:51:20,946 - INFO - evaluating now!
2024-03-05 11:51:31,350 - INFO - Epoch [33/300] (22746) train_loss: 28.6936, val_loss: 77.3121, lr: 0.000972, 130.08s
2024-03-05 11:51:31,457 - INFO - Saved model at 33
2024-03-05 11:51:31,458 - INFO - Val loss decrease from 90.7948 to 77.3121, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch33.tar
2024-03-05 11:53:31,519 - INFO - epoch complete!
2024-03-05 11:53:31,520 - INFO - evaluating now!
2024-03-05 11:53:42,118 - INFO - Epoch [34/300] (23415) train_loss: 28.3474, val_loss: 76.5475, lr: 0.000970, 130.66s
2024-03-05 11:53:42,219 - INFO - Saved model at 34
2024-03-05 11:53:42,219 - INFO - Val loss decrease from 77.3121 to 76.5475, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch34.tar
2024-03-05 11:55:41,769 - INFO - epoch complete!
2024-03-05 11:55:41,769 - INFO - evaluating now!
2024-03-05 11:55:52,193 - INFO - Epoch [35/300] (24084) train_loss: 28.0846, val_loss: 77.0473, lr: 0.000968, 129.97s
2024-03-05 11:57:51,623 - INFO - epoch complete!
2024-03-05 11:57:51,624 - INFO - evaluating now!
2024-03-05 11:58:02,108 - INFO - Epoch [36/300] (24753) train_loss: 28.1004, val_loss: 77.5349, lr: 0.000967, 129.91s
2024-03-05 11:58:43,909 - INFO - Training: task_level increase from 9 to 10
2024-03-05 11:58:43,909 - INFO - Current batches_seen is 24984
2024-03-05 12:00:03,124 - INFO - epoch complete!
2024-03-05 12:00:03,125 - INFO - evaluating now!
2024-03-05 12:00:13,826 - INFO - Epoch [37/300] (25422) train_loss: 28.5757, val_loss: 60.8729, lr: 0.000965, 131.72s
2024-03-05 12:00:13,926 - INFO - Saved model at 37
2024-03-05 12:00:13,927 - INFO - Val loss decrease from 76.5475 to 60.8729, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch37.tar
2024-03-05 12:02:18,437 - INFO - epoch complete!
2024-03-05 12:02:18,438 - INFO - evaluating now!
2024-03-05 12:02:29,407 - INFO - Epoch [38/300] (26091) train_loss: 28.2618, val_loss: 60.3677, lr: 0.000963, 135.48s
2024-03-05 12:02:29,515 - INFO - Saved model at 38
2024-03-05 12:02:29,516 - INFO - Val loss decrease from 60.8729 to 60.3677, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch38.tar
2024-03-05 12:04:36,239 - INFO - epoch complete!
2024-03-05 12:04:36,240 - INFO - evaluating now!
2024-03-05 12:04:47,299 - INFO - Epoch [39/300] (26760) train_loss: 28.2586, val_loss: 60.1473, lr: 0.000961, 137.78s
2024-03-05 12:04:47,408 - INFO - Saved model at 39
2024-03-05 12:04:47,408 - INFO - Val loss decrease from 60.3677 to 60.1473, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch39.tar
2024-03-05 12:06:54,267 - INFO - epoch complete!
2024-03-05 12:06:54,268 - INFO - evaluating now!
2024-03-05 12:07:05,322 - INFO - Epoch [40/300] (27429) train_loss: 27.9464, val_loss: 60.4437, lr: 0.000959, 137.91s
2024-03-05 12:08:08,120 - INFO - Training: task_level increase from 10 to 11
2024-03-05 12:08:08,120 - INFO - Current batches_seen is 27760
2024-03-05 12:09:12,410 - INFO - epoch complete!
2024-03-05 12:09:12,411 - INFO - evaluating now!
2024-03-05 12:09:23,435 - INFO - Epoch [41/300] (28098) train_loss: 28.4032, val_loss: 46.6033, lr: 0.000957, 138.11s
2024-03-05 12:09:23,542 - INFO - Saved model at 41
2024-03-05 12:09:23,543 - INFO - Val loss decrease from 60.1473 to 46.6033, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch41.tar
2024-03-05 12:11:28,849 - INFO - epoch complete!
2024-03-05 12:11:28,850 - INFO - evaluating now!
2024-03-05 12:11:39,344 - INFO - Epoch [42/300] (28767) train_loss: 28.3875, val_loss: 45.9737, lr: 0.000955, 135.80s
2024-03-05 12:11:39,451 - INFO - Saved model at 42
2024-03-05 12:11:39,452 - INFO - Val loss decrease from 46.6033 to 45.9737, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch42.tar
2024-03-05 12:13:40,886 - INFO - epoch complete!
2024-03-05 12:13:40,887 - INFO - evaluating now!
2024-03-05 12:13:51,362 - INFO - Epoch [43/300] (29436) train_loss: 28.0859, val_loss: 45.4881, lr: 0.000953, 131.91s
2024-03-05 12:13:51,469 - INFO - Saved model at 43
2024-03-05 12:13:51,470 - INFO - Val loss decrease from 45.9737 to 45.4881, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch43.tar
2024-03-05 12:15:50,840 - INFO - epoch complete!
2024-03-05 12:15:50,840 - INFO - evaluating now!
2024-03-05 12:16:01,318 - INFO - Epoch [44/300] (30105) train_loss: 28.0153, val_loss: 45.3705, lr: 0.000951, 129.85s
2024-03-05 12:16:01,427 - INFO - Saved model at 44
2024-03-05 12:16:01,427 - INFO - Val loss decrease from 45.4881 to 45.3705, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch44.tar
2024-03-05 12:17:18,530 - INFO - Training: task_level increase from 11 to 12
2024-03-05 12:17:18,530 - INFO - Current batches_seen is 30536
2024-03-05 12:18:00,822 - INFO - epoch complete!
2024-03-05 12:18:00,823 - INFO - evaluating now!
2024-03-05 12:18:11,654 - INFO - Epoch [45/300] (30774) train_loss: 28.2861, val_loss: 29.0910, lr: 0.000949, 130.23s
2024-03-05 12:18:11,760 - INFO - Saved model at 45
2024-03-05 12:18:11,760 - INFO - Val loss decrease from 45.3705 to 29.0910, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch45.tar
2024-03-05 12:20:12,530 - INFO - epoch complete!
2024-03-05 12:20:12,531 - INFO - evaluating now!
2024-03-05 12:20:23,165 - INFO - Epoch [46/300] (31443) train_loss: 28.2970, val_loss: 27.7130, lr: 0.000947, 131.40s
2024-03-05 12:20:23,277 - INFO - Saved model at 46
2024-03-05 12:20:23,278 - INFO - Val loss decrease from 29.0910 to 27.7130, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch46.tar
2024-03-05 12:22:22,602 - INFO - epoch complete!
2024-03-05 12:22:22,603 - INFO - evaluating now!
2024-03-05 12:22:33,045 - INFO - Epoch [47/300] (32112) train_loss: 28.2984, val_loss: 27.6813, lr: 0.000944, 129.77s
2024-03-05 12:22:33,150 - INFO - Saved model at 47
2024-03-05 12:22:33,150 - INFO - Val loss decrease from 27.7130 to 27.6813, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch47.tar
2024-03-05 12:24:32,563 - INFO - epoch complete!
2024-03-05 12:24:32,564 - INFO - evaluating now!
2024-03-05 12:24:42,987 - INFO - Epoch [48/300] (32781) train_loss: 28.1837, val_loss: 27.5172, lr: 0.000942, 129.84s
2024-03-05 12:24:43,094 - INFO - Saved model at 48
2024-03-05 12:24:43,095 - INFO - Val loss decrease from 27.6813 to 27.5172, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch48.tar
2024-03-05 12:26:42,861 - INFO - epoch complete!
2024-03-05 12:26:42,862 - INFO - evaluating now!
2024-03-05 12:26:53,376 - INFO - Epoch [49/300] (33450) train_loss: 27.9469, val_loss: 27.8447, lr: 0.000940, 130.28s
2024-03-05 12:28:53,294 - INFO - epoch complete!
2024-03-05 12:28:53,295 - INFO - evaluating now!
2024-03-05 12:29:03,769 - INFO - Epoch [50/300] (34119) train_loss: 27.8898, val_loss: 27.8064, lr: 0.000937, 130.39s
2024-03-05 12:31:02,829 - INFO - epoch complete!
2024-03-05 12:31:02,830 - INFO - evaluating now!
2024-03-05 12:31:13,445 - INFO - Epoch [51/300] (34788) train_loss: 27.7988, val_loss: 27.4401, lr: 0.000935, 129.67s
2024-03-05 12:31:13,553 - INFO - Saved model at 51
2024-03-05 12:31:13,554 - INFO - Val loss decrease from 27.5172 to 27.4401, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch51.tar
2024-03-05 12:33:13,304 - INFO - epoch complete!
2024-03-05 12:33:13,305 - INFO - evaluating now!
2024-03-05 12:33:23,859 - INFO - Epoch [52/300] (35457) train_loss: 27.9245, val_loss: 28.0174, lr: 0.000932, 130.30s
2024-03-05 12:35:24,456 - INFO - epoch complete!
2024-03-05 12:35:24,457 - INFO - evaluating now!
2024-03-05 12:35:35,145 - INFO - Epoch [53/300] (36126) train_loss: 27.7617, val_loss: 28.6044, lr: 0.000930, 131.29s
2024-03-05 12:37:36,799 - INFO - epoch complete!
2024-03-05 12:37:36,800 - INFO - evaluating now!
2024-03-05 12:37:47,621 - INFO - Epoch [54/300] (36795) train_loss: 27.6784, val_loss: 27.4244, lr: 0.000927, 132.47s
2024-03-05 12:37:47,737 - INFO - Saved model at 54
2024-03-05 12:37:47,738 - INFO - Val loss decrease from 27.4401 to 27.4244, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch54.tar
2024-03-05 12:39:50,384 - INFO - epoch complete!
2024-03-05 12:39:50,385 - INFO - evaluating now!
2024-03-05 12:40:01,089 - INFO - Epoch [55/300] (37464) train_loss: 27.6554, val_loss: 27.3686, lr: 0.000925, 133.35s
2024-03-05 12:40:01,198 - INFO - Saved model at 55
2024-03-05 12:40:01,199 - INFO - Val loss decrease from 27.4244 to 27.3686, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch55.tar
2024-03-05 12:42:04,005 - INFO - epoch complete!
2024-03-05 12:42:04,006 - INFO - evaluating now!
2024-03-05 12:42:15,035 - INFO - Epoch [56/300] (38133) train_loss: 27.6270, val_loss: 27.7860, lr: 0.000922, 133.84s
2024-03-05 12:44:18,560 - INFO - epoch complete!
2024-03-05 12:44:18,561 - INFO - evaluating now!
2024-03-05 12:44:29,369 - INFO - Epoch [57/300] (38802) train_loss: 27.4755, val_loss: 27.0403, lr: 0.000920, 134.33s
2024-03-05 12:44:29,472 - INFO - Saved model at 57
2024-03-05 12:44:29,473 - INFO - Val loss decrease from 27.3686 to 27.0403, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch57.tar
2024-03-05 12:46:31,563 - INFO - epoch complete!
2024-03-05 12:46:31,564 - INFO - evaluating now!
2024-03-05 12:46:42,311 - INFO - Epoch [58/300] (39471) train_loss: 27.5257, val_loss: 26.7513, lr: 0.000917, 132.84s
2024-03-05 12:46:42,412 - INFO - Saved model at 58
2024-03-05 12:46:42,413 - INFO - Val loss decrease from 27.0403 to 26.7513, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch58.tar
2024-03-05 12:48:45,366 - INFO - epoch complete!
2024-03-05 12:48:45,367 - INFO - evaluating now!
2024-03-05 12:48:56,206 - INFO - Epoch [59/300] (40140) train_loss: 27.4569, val_loss: 27.4564, lr: 0.000914, 133.79s
2024-03-05 12:50:57,032 - INFO - epoch complete!
2024-03-05 12:50:57,032 - INFO - evaluating now!
2024-03-05 12:51:07,683 - INFO - Epoch [60/300] (40809) train_loss: 27.4168, val_loss: 27.2341, lr: 0.000911, 131.48s
2024-03-05 12:53:09,683 - INFO - epoch complete!
2024-03-05 12:53:09,684 - INFO - evaluating now!
2024-03-05 12:53:20,486 - INFO - Epoch [61/300] (41478) train_loss: 27.3106, val_loss: 26.9993, lr: 0.000908, 132.80s
2024-03-05 12:55:21,469 - INFO - epoch complete!
2024-03-05 12:55:21,470 - INFO - evaluating now!
2024-03-05 12:55:32,032 - INFO - Epoch [62/300] (42147) train_loss: 27.2220, val_loss: 27.2569, lr: 0.000906, 131.55s
2024-03-05 12:57:33,943 - INFO - epoch complete!
2024-03-05 12:57:33,944 - INFO - evaluating now!
2024-03-05 12:57:44,734 - INFO - Epoch [63/300] (42816) train_loss: 27.2042, val_loss: 26.8235, lr: 0.000903, 132.70s
2024-03-05 12:59:46,066 - INFO - epoch complete!
2024-03-05 12:59:46,067 - INFO - evaluating now!
2024-03-05 12:59:56,769 - INFO - Epoch [64/300] (43485) train_loss: 27.2490, val_loss: 27.2133, lr: 0.000900, 132.03s
2024-03-05 13:01:59,496 - INFO - epoch complete!
2024-03-05 13:01:59,497 - INFO - evaluating now!
2024-03-05 13:02:10,394 - INFO - Epoch [65/300] (44154) train_loss: 27.1071, val_loss: 28.3089, lr: 0.000897, 133.62s
2024-03-05 13:04:12,495 - INFO - epoch complete!
2024-03-05 13:04:12,496 - INFO - evaluating now!
2024-03-05 13:04:23,210 - INFO - Epoch [66/300] (44823) train_loss: 27.2300, val_loss: 28.1055, lr: 0.000894, 132.82s
2024-03-05 13:06:25,154 - INFO - epoch complete!
2024-03-05 13:06:25,155 - INFO - evaluating now!
2024-03-05 13:06:35,686 - INFO - Epoch [67/300] (45492) train_loss: 27.0037, val_loss: 27.4571, lr: 0.000891, 132.47s
2024-03-05 13:08:37,243 - INFO - epoch complete!
2024-03-05 13:08:37,244 - INFO - evaluating now!
2024-03-05 13:08:48,033 - INFO - Epoch [68/300] (46161) train_loss: 27.0227, val_loss: 26.8290, lr: 0.000888, 132.35s
2024-03-05 13:10:49,557 - INFO - epoch complete!
2024-03-05 13:10:49,558 - INFO - evaluating now!
2024-03-05 13:11:00,247 - INFO - Epoch [69/300] (46830) train_loss: 26.9558, val_loss: 26.8301, lr: 0.000884, 132.21s
2024-03-05 13:13:00,206 - INFO - epoch complete!
2024-03-05 13:13:00,207 - INFO - evaluating now!
2024-03-05 13:13:10,901 - INFO - Epoch [70/300] (47499) train_loss: 26.8320, val_loss: 26.8027, lr: 0.000881, 130.65s
2024-03-05 13:15:11,487 - INFO - epoch complete!
2024-03-05 13:15:11,488 - INFO - evaluating now!
2024-03-05 13:15:22,099 - INFO - Epoch [71/300] (48168) train_loss: 26.8732, val_loss: 26.9311, lr: 0.000878, 131.20s
2024-03-05 13:17:23,302 - INFO - epoch complete!
2024-03-05 13:17:23,303 - INFO - evaluating now!
2024-03-05 13:17:34,001 - INFO - Epoch [72/300] (48837) train_loss: 26.8711, val_loss: 26.7045, lr: 0.000875, 131.90s
2024-03-05 13:17:34,105 - INFO - Saved model at 72
2024-03-05 13:17:34,106 - INFO - Val loss decrease from 26.7513 to 26.7045, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch72.tar
2024-03-05 13:19:35,752 - INFO - epoch complete!
2024-03-05 13:19:35,753 - INFO - evaluating now!
2024-03-05 13:19:46,420 - INFO - Epoch [73/300] (49506) train_loss: 26.7329, val_loss: 26.7992, lr: 0.000872, 132.31s
2024-03-05 13:21:46,406 - INFO - epoch complete!
2024-03-05 13:21:46,406 - INFO - evaluating now!
2024-03-05 13:21:56,978 - INFO - Epoch [74/300] (50175) train_loss: 26.8831, val_loss: 27.0235, lr: 0.000868, 130.56s
2024-03-05 13:23:57,032 - INFO - epoch complete!
2024-03-05 13:23:57,033 - INFO - evaluating now!
2024-03-05 13:24:07,703 - INFO - Epoch [75/300] (50844) train_loss: 26.7517, val_loss: 27.9743, lr: 0.000865, 130.72s
2024-03-05 13:26:09,766 - INFO - epoch complete!
2024-03-05 13:26:09,767 - INFO - evaluating now!
2024-03-05 13:26:20,721 - INFO - Epoch [76/300] (51513) train_loss: 26.6857, val_loss: 26.7006, lr: 0.000861, 133.02s
2024-03-05 13:26:20,827 - INFO - Saved model at 76
2024-03-05 13:26:20,828 - INFO - Val loss decrease from 26.7045 to 26.7006, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch76.tar
2024-03-05 13:28:21,367 - INFO - epoch complete!
2024-03-05 13:28:21,368 - INFO - evaluating now!
2024-03-05 13:28:31,973 - INFO - Epoch [77/300] (52182) train_loss: 26.5943, val_loss: 26.5888, lr: 0.000858, 131.14s
2024-03-05 13:28:32,080 - INFO - Saved model at 77
2024-03-05 13:28:32,081 - INFO - Val loss decrease from 26.7006 to 26.5888, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch77.tar
2024-03-05 13:30:34,134 - INFO - epoch complete!
2024-03-05 13:30:34,135 - INFO - evaluating now!
2024-03-05 13:30:44,824 - INFO - Epoch [78/300] (52851) train_loss: 26.6510, val_loss: 27.0684, lr: 0.000855, 132.74s
2024-03-05 13:32:46,133 - INFO - epoch complete!
2024-03-05 13:32:46,134 - INFO - evaluating now!
2024-03-05 13:32:56,729 - INFO - Epoch [79/300] (53520) train_loss: 26.5324, val_loss: 26.3901, lr: 0.000851, 131.90s
2024-03-05 13:32:56,836 - INFO - Saved model at 79
2024-03-05 13:32:56,837 - INFO - Val loss decrease from 26.5888 to 26.3901, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch79.tar
2024-03-05 13:34:57,656 - INFO - epoch complete!
2024-03-05 13:34:57,657 - INFO - evaluating now!
2024-03-05 13:35:08,379 - INFO - Epoch [80/300] (54189) train_loss: 26.6538, val_loss: 26.7497, lr: 0.000848, 131.54s
2024-03-05 13:37:10,646 - INFO - epoch complete!
2024-03-05 13:37:10,647 - INFO - evaluating now!
2024-03-05 13:37:21,240 - INFO - Epoch [81/300] (54858) train_loss: 26.5445, val_loss: 26.7365, lr: 0.000844, 132.86s
2024-03-05 13:39:21,171 - INFO - epoch complete!
2024-03-05 13:39:21,172 - INFO - evaluating now!
2024-03-05 13:39:31,784 - INFO - Epoch [82/300] (55527) train_loss: 26.2590, val_loss: 26.7100, lr: 0.000840, 130.54s
2024-03-05 13:41:32,076 - INFO - epoch complete!
2024-03-05 13:41:32,077 - INFO - evaluating now!
2024-03-05 13:41:42,647 - INFO - Epoch [83/300] (56196) train_loss: 26.4463, val_loss: 27.1279, lr: 0.000837, 130.86s
2024-03-05 13:43:44,676 - INFO - epoch complete!
2024-03-05 13:43:44,677 - INFO - evaluating now!
2024-03-05 13:43:55,354 - INFO - Epoch [84/300] (56865) train_loss: 26.4953, val_loss: 26.7301, lr: 0.000833, 132.71s
2024-03-05 13:45:56,503 - INFO - epoch complete!
2024-03-05 13:45:56,504 - INFO - evaluating now!
2024-03-05 13:46:07,195 - INFO - Epoch [85/300] (57534) train_loss: 26.4315, val_loss: 26.8853, lr: 0.000830, 131.84s
2024-03-05 13:48:08,355 - INFO - epoch complete!
2024-03-05 13:48:08,356 - INFO - evaluating now!
2024-03-05 13:48:19,098 - INFO - Epoch [86/300] (58203) train_loss: 26.3137, val_loss: 26.4899, lr: 0.000826, 131.90s
2024-03-05 13:50:21,713 - INFO - epoch complete!
2024-03-05 13:50:21,714 - INFO - evaluating now!
2024-03-05 13:50:32,460 - INFO - Epoch [87/300] (58872) train_loss: 26.4103, val_loss: 26.4258, lr: 0.000822, 133.36s
2024-03-05 13:52:32,007 - INFO - epoch complete!
2024-03-05 13:52:32,008 - INFO - evaluating now!
2024-03-05 13:52:42,284 - INFO - Epoch [88/300] (59541) train_loss: 26.2976, val_loss: 26.7535, lr: 0.000818, 129.82s
2024-03-05 13:54:42,273 - INFO - epoch complete!
2024-03-05 13:54:42,274 - INFO - evaluating now!
2024-03-05 13:54:52,948 - INFO - Epoch [89/300] (60210) train_loss: 26.1829, val_loss: 25.9773, lr: 0.000815, 130.66s
2024-03-05 13:54:53,058 - INFO - Saved model at 89
2024-03-05 13:54:53,059 - INFO - Val loss decrease from 26.3901 to 25.9773, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch89.tar
2024-03-05 13:56:55,287 - INFO - epoch complete!
2024-03-05 13:56:55,288 - INFO - evaluating now!
2024-03-05 13:57:05,981 - INFO - Epoch [90/300] (60879) train_loss: 26.1731, val_loss: 26.2526, lr: 0.000811, 132.92s
2024-03-05 13:59:08,767 - INFO - epoch complete!
2024-03-05 13:59:08,767 - INFO - evaluating now!
2024-03-05 13:59:19,574 - INFO - Epoch [91/300] (61548) train_loss: 26.1955, val_loss: 26.6933, lr: 0.000807, 133.59s
2024-03-05 14:01:20,038 - INFO - epoch complete!
2024-03-05 14:01:20,039 - INFO - evaluating now!
2024-03-05 14:01:30,419 - INFO - Epoch [92/300] (62217) train_loss: 26.1674, val_loss: 26.5792, lr: 0.000803, 130.84s
2024-03-05 14:03:29,554 - INFO - epoch complete!
2024-03-05 14:03:29,555 - INFO - evaluating now!
2024-03-05 14:03:40,053 - INFO - Epoch [93/300] (62886) train_loss: 26.1361, val_loss: 26.5371, lr: 0.000799, 129.63s
2024-03-05 14:05:41,203 - INFO - epoch complete!
2024-03-05 14:05:41,204 - INFO - evaluating now!
2024-03-05 14:05:51,849 - INFO - Epoch [94/300] (63555) train_loss: 26.0866, val_loss: 26.2275, lr: 0.000795, 131.80s
2024-03-05 14:07:53,538 - INFO - epoch complete!
2024-03-05 14:07:53,539 - INFO - evaluating now!
2024-03-05 14:08:04,158 - INFO - Epoch [95/300] (64224) train_loss: 26.0234, val_loss: 26.7775, lr: 0.000791, 132.31s
2024-03-05 14:10:04,744 - INFO - epoch complete!
2024-03-05 14:10:04,745 - INFO - evaluating now!
2024-03-05 14:10:15,423 - INFO - Epoch [96/300] (64893) train_loss: 26.0501, val_loss: 26.0365, lr: 0.000787, 131.26s
2024-03-05 14:12:16,130 - INFO - epoch complete!
2024-03-05 14:12:16,131 - INFO - evaluating now!
2024-03-05 14:12:26,794 - INFO - Epoch [97/300] (65562) train_loss: 25.9725, val_loss: 26.2654, lr: 0.000783, 131.37s
2024-03-05 14:14:27,755 - INFO - epoch complete!
2024-03-05 14:14:27,756 - INFO - evaluating now!
2024-03-05 14:14:38,447 - INFO - Epoch [98/300] (66231) train_loss: 25.9789, val_loss: 26.3722, lr: 0.000779, 131.65s
2024-03-05 14:16:39,367 - INFO - epoch complete!
2024-03-05 14:16:39,368 - INFO - evaluating now!
2024-03-05 14:16:49,959 - INFO - Epoch [99/300] (66900) train_loss: 25.9646, val_loss: 26.5388, lr: 0.000775, 131.51s
2024-03-05 14:18:49,788 - INFO - epoch complete!
2024-03-05 14:18:49,789 - INFO - evaluating now!
2024-03-05 14:19:00,264 - INFO - Epoch [100/300] (67569) train_loss: 25.9930, val_loss: 26.3792, lr: 0.000771, 130.30s
2024-03-05 14:21:00,165 - INFO - epoch complete!
2024-03-05 14:21:00,166 - INFO - evaluating now!
2024-03-05 14:21:10,671 - INFO - Epoch [101/300] (68238) train_loss: 25.8315, val_loss: 26.0760, lr: 0.000767, 130.41s
2024-03-05 14:23:14,387 - INFO - epoch complete!
2024-03-05 14:23:14,388 - INFO - evaluating now!
2024-03-05 14:23:25,197 - INFO - Epoch [102/300] (68907) train_loss: 25.8565, val_loss: 26.1363, lr: 0.000763, 134.52s
2024-03-05 14:25:26,588 - INFO - epoch complete!
2024-03-05 14:25:26,589 - INFO - evaluating now!
2024-03-05 14:25:37,186 - INFO - Epoch [103/300] (69576) train_loss: 25.8494, val_loss: 26.1852, lr: 0.000758, 131.99s
2024-03-05 14:27:38,764 - INFO - epoch complete!
2024-03-05 14:27:38,765 - INFO - evaluating now!
2024-03-05 14:27:49,479 - INFO - Epoch [104/300] (70245) train_loss: 25.8353, val_loss: 26.4418, lr: 0.000754, 132.29s
2024-03-05 14:29:52,221 - INFO - epoch complete!
2024-03-05 14:29:52,222 - INFO - evaluating now!
2024-03-05 14:30:02,955 - INFO - Epoch [105/300] (70914) train_loss: 25.7859, val_loss: 26.4330, lr: 0.000750, 133.48s
2024-03-05 14:32:02,771 - INFO - epoch complete!
2024-03-05 14:32:02,772 - INFO - evaluating now!
2024-03-05 14:32:13,633 - INFO - Epoch [106/300] (71583) train_loss: 25.7542, val_loss: 25.8091, lr: 0.000746, 130.68s
2024-03-05 14:32:13,747 - INFO - Saved model at 106
2024-03-05 14:32:13,748 - INFO - Val loss decrease from 25.9773 to 25.8091, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch106.tar
2024-03-05 14:34:13,343 - INFO - epoch complete!
2024-03-05 14:34:13,344 - INFO - evaluating now!
2024-03-05 14:34:24,077 - INFO - Epoch [107/300] (72252) train_loss: 25.6975, val_loss: 25.9551, lr: 0.000742, 130.33s
2024-03-05 14:36:25,579 - INFO - epoch complete!
2024-03-05 14:36:25,580 - INFO - evaluating now!
2024-03-05 14:36:36,154 - INFO - Epoch [108/300] (72921) train_loss: 25.7383, val_loss: 25.9243, lr: 0.000737, 132.08s
2024-03-05 14:38:36,191 - INFO - epoch complete!
2024-03-05 14:38:36,192 - INFO - evaluating now!
2024-03-05 14:38:46,755 - INFO - Epoch [109/300] (73590) train_loss: 25.7446, val_loss: 26.0468, lr: 0.000733, 130.60s
2024-03-05 14:40:47,819 - INFO - epoch complete!
2024-03-05 14:40:47,820 - INFO - evaluating now!
2024-03-05 14:40:58,436 - INFO - Epoch [110/300] (74259) train_loss: 25.5528, val_loss: 26.3375, lr: 0.000729, 131.68s
2024-03-05 14:42:58,646 - INFO - epoch complete!
2024-03-05 14:42:58,647 - INFO - evaluating now!
2024-03-05 14:43:09,422 - INFO - Epoch [111/300] (74928) train_loss: 25.6317, val_loss: 25.9331, lr: 0.000724, 130.99s
2024-03-05 14:45:11,846 - INFO - epoch complete!
2024-03-05 14:45:11,847 - INFO - evaluating now!
2024-03-05 14:45:23,507 - INFO - Epoch [112/300] (75597) train_loss: 25.6816, val_loss: 26.1417, lr: 0.000720, 134.08s
2024-03-05 14:47:23,703 - INFO - epoch complete!
2024-03-05 14:47:23,704 - INFO - evaluating now!
2024-03-05 14:47:34,372 - INFO - Epoch [113/300] (76266) train_loss: 25.6434, val_loss: 26.8203, lr: 0.000716, 130.86s
2024-03-05 14:49:31,798 - INFO - epoch complete!
2024-03-05 14:49:31,799 - INFO - evaluating now!
2024-03-05 14:49:42,291 - INFO - Epoch [114/300] (76935) train_loss: 25.5415, val_loss: 25.7892, lr: 0.000711, 127.92s
2024-03-05 14:49:42,397 - INFO - Saved model at 114
2024-03-05 14:49:42,398 - INFO - Val loss decrease from 25.8091 to 25.7892, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch114.tar
2024-03-05 14:51:42,525 - INFO - epoch complete!
2024-03-05 14:51:42,525 - INFO - evaluating now!
2024-03-05 14:51:53,019 - INFO - Epoch [115/300] (77604) train_loss: 25.5547, val_loss: 26.2638, lr: 0.000707, 130.62s
2024-03-05 14:53:52,512 - INFO - epoch complete!
2024-03-05 14:53:52,513 - INFO - evaluating now!
2024-03-05 14:54:02,951 - INFO - Epoch [116/300] (78273) train_loss: 25.4719, val_loss: 27.1527, lr: 0.000702, 129.93s
2024-03-05 14:56:02,478 - INFO - epoch complete!
2024-03-05 14:56:02,479 - INFO - evaluating now!
2024-03-05 14:56:13,184 - INFO - Epoch [117/300] (78942) train_loss: 25.4891, val_loss: 25.9290, lr: 0.000698, 130.23s
2024-03-05 14:58:13,001 - INFO - epoch complete!
2024-03-05 14:58:13,002 - INFO - evaluating now!
2024-03-05 14:58:23,503 - INFO - Epoch [118/300] (79611) train_loss: 25.5002, val_loss: 26.1704, lr: 0.000694, 130.32s
2024-03-05 15:00:22,974 - INFO - epoch complete!
2024-03-05 15:00:22,975 - INFO - evaluating now!
2024-03-05 15:00:33,396 - INFO - Epoch [119/300] (80280) train_loss: 25.4729, val_loss: 26.1375, lr: 0.000689, 129.89s
2024-03-05 15:02:32,784 - INFO - epoch complete!
2024-03-05 15:02:32,784 - INFO - evaluating now!
2024-03-05 15:02:43,285 - INFO - Epoch [120/300] (80949) train_loss: 25.4508, val_loss: 25.5069, lr: 0.000685, 129.89s
2024-03-05 15:02:43,385 - INFO - Saved model at 120
2024-03-05 15:02:43,386 - INFO - Val loss decrease from 25.7892 to 25.5069, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch120.tar
2024-03-05 15:04:42,947 - INFO - epoch complete!
2024-03-05 15:04:42,948 - INFO - evaluating now!
2024-03-05 15:04:53,334 - INFO - Epoch [121/300] (81618) train_loss: 25.3338, val_loss: 25.8809, lr: 0.000680, 129.95s
2024-03-05 15:06:52,285 - INFO - epoch complete!
2024-03-05 15:06:52,286 - INFO - evaluating now!
2024-03-05 15:07:02,739 - INFO - Epoch [122/300] (82287) train_loss: 25.3269, val_loss: 25.6764, lr: 0.000676, 129.40s
2024-03-05 15:09:02,086 - INFO - epoch complete!
2024-03-05 15:09:02,086 - INFO - evaluating now!
2024-03-05 15:09:12,677 - INFO - Epoch [123/300] (82956) train_loss: 25.2184, val_loss: 26.4867, lr: 0.000671, 129.94s
2024-03-05 15:11:12,370 - INFO - epoch complete!
2024-03-05 15:11:12,370 - INFO - evaluating now!
2024-03-05 15:11:22,884 - INFO - Epoch [124/300] (83625) train_loss: 25.3428, val_loss: 26.1699, lr: 0.000666, 130.21s
2024-03-05 15:13:19,144 - INFO - epoch complete!
2024-03-05 15:13:19,145 - INFO - evaluating now!
2024-03-05 15:13:29,654 - INFO - Epoch [125/300] (84294) train_loss: 25.2885, val_loss: 25.7593, lr: 0.000662, 126.77s
2024-03-05 15:15:29,316 - INFO - epoch complete!
2024-03-05 15:15:29,317 - INFO - evaluating now!
2024-03-05 15:15:39,773 - INFO - Epoch [126/300] (84963) train_loss: 25.1930, val_loss: 25.4418, lr: 0.000657, 130.12s
2024-03-05 15:15:39,880 - INFO - Saved model at 126
2024-03-05 15:15:39,881 - INFO - Val loss decrease from 25.5069 to 25.4418, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch126.tar
2024-03-05 15:17:39,107 - INFO - epoch complete!
2024-03-05 15:17:39,107 - INFO - evaluating now!
2024-03-05 15:17:49,564 - INFO - Epoch [127/300] (85632) train_loss: 25.1965, val_loss: 26.1462, lr: 0.000653, 129.68s
2024-03-05 15:19:48,772 - INFO - epoch complete!
2024-03-05 15:19:48,773 - INFO - evaluating now!
2024-03-05 15:19:59,254 - INFO - Epoch [128/300] (86301) train_loss: 25.2538, val_loss: 25.5846, lr: 0.000648, 129.69s
2024-03-05 15:21:58,484 - INFO - epoch complete!
2024-03-05 15:21:58,485 - INFO - evaluating now!
2024-03-05 15:22:09,178 - INFO - Epoch [129/300] (86970) train_loss: 25.1366, val_loss: 25.8221, lr: 0.000644, 129.92s
2024-03-05 15:24:09,154 - INFO - epoch complete!
2024-03-05 15:24:09,155 - INFO - evaluating now!
2024-03-05 15:24:20,127 - INFO - Epoch [130/300] (87639) train_loss: 25.1446, val_loss: 26.1119, lr: 0.000639, 130.95s
2024-03-05 15:26:20,090 - INFO - epoch complete!
2024-03-05 15:26:20,090 - INFO - evaluating now!
2024-03-05 15:26:30,506 - INFO - Epoch [131/300] (88308) train_loss: 25.1302, val_loss: 25.6137, lr: 0.000634, 130.38s
2024-03-05 15:28:29,641 - INFO - epoch complete!
2024-03-05 15:28:29,641 - INFO - evaluating now!
2024-03-05 15:28:40,088 - INFO - Epoch [132/300] (88977) train_loss: 25.0324, val_loss: 26.1990, lr: 0.000630, 129.58s
2024-03-05 15:30:39,353 - INFO - epoch complete!
2024-03-05 15:30:39,354 - INFO - evaluating now!
2024-03-05 15:30:49,773 - INFO - Epoch [133/300] (89646) train_loss: 24.9530, val_loss: 25.5053, lr: 0.000625, 129.68s
2024-03-05 15:32:48,982 - INFO - epoch complete!
2024-03-05 15:32:48,983 - INFO - evaluating now!
2024-03-05 15:32:59,409 - INFO - Epoch [134/300] (90315) train_loss: 24.9992, val_loss: 26.0548, lr: 0.000620, 129.63s
2024-03-05 15:35:01,479 - INFO - epoch complete!
2024-03-05 15:35:01,480 - INFO - evaluating now!
2024-03-05 15:35:12,276 - INFO - Epoch [135/300] (90984) train_loss: 24.9603, val_loss: 25.8499, lr: 0.000616, 132.87s
2024-03-05 15:37:11,975 - INFO - epoch complete!
2024-03-05 15:37:11,976 - INFO - evaluating now!
2024-03-05 15:37:22,496 - INFO - Epoch [136/300] (91653) train_loss: 24.9849, val_loss: 26.3478, lr: 0.000611, 130.22s
2024-03-05 15:39:21,827 - INFO - epoch complete!
2024-03-05 15:39:21,828 - INFO - evaluating now!
2024-03-05 15:39:32,273 - INFO - Epoch [137/300] (92322) train_loss: 24.8857, val_loss: 26.1931, lr: 0.000606, 129.78s
2024-03-05 15:41:31,608 - INFO - epoch complete!
2024-03-05 15:41:31,609 - INFO - evaluating now!
2024-03-05 15:41:42,048 - INFO - Epoch [138/300] (92991) train_loss: 24.9160, val_loss: 25.6241, lr: 0.000602, 129.77s
2024-03-05 15:43:41,503 - INFO - epoch complete!
2024-03-05 15:43:41,504 - INFO - evaluating now!
2024-03-05 15:43:51,965 - INFO - Epoch [139/300] (93660) train_loss: 24.7869, val_loss: 26.0692, lr: 0.000597, 129.92s
2024-03-05 15:45:51,145 - INFO - epoch complete!
2024-03-05 15:45:51,146 - INFO - evaluating now!
2024-03-05 15:46:01,559 - INFO - Epoch [140/300] (94329) train_loss: 24.7532, val_loss: 25.6060, lr: 0.000592, 129.59s
2024-03-05 15:47:37,996 - INFO - epoch complete!
2024-03-05 15:47:37,997 - INFO - evaluating now!
2024-03-05 15:47:44,086 - INFO - Epoch [141/300] (94998) train_loss: 24.7787, val_loss: 26.2945, lr: 0.000588, 102.53s
2024-03-05 15:49:18,263 - INFO - epoch complete!
2024-03-05 15:49:18,264 - INFO - evaluating now!
2024-03-05 15:49:24,384 - INFO - Epoch [142/300] (95667) train_loss: 24.6895, val_loss: 25.9580, lr: 0.000583, 100.30s
2024-03-05 15:50:56,405 - INFO - epoch complete!
2024-03-05 15:50:56,406 - INFO - evaluating now!
2024-03-05 15:51:02,568 - INFO - Epoch [143/300] (96336) train_loss: 24.6418, val_loss: 25.5450, lr: 0.000578, 98.18s
2024-03-05 15:52:39,797 - INFO - epoch complete!
2024-03-05 15:52:39,797 - INFO - evaluating now!
2024-03-05 15:52:45,851 - INFO - Epoch [144/300] (97005) train_loss: 24.6350, val_loss: 25.6533, lr: 0.000574, 103.28s
2024-03-05 15:54:11,510 - INFO - epoch complete!
2024-03-05 15:54:11,511 - INFO - evaluating now!
2024-03-05 15:54:17,540 - INFO - Epoch [145/300] (97674) train_loss: 24.6468, val_loss: 25.3741, lr: 0.000569, 91.69s
2024-03-05 15:54:17,585 - INFO - Saved model at 145
2024-03-05 15:54:17,585 - INFO - Val loss decrease from 25.4418 to 25.3741, saving to ./libcity/cache/38389/model_cache/PDFormer_PeMS08_epoch145.tar
2024-03-05 15:55:42,931 - INFO - epoch complete!
2024-03-05 15:55:42,932 - INFO - evaluating now!
2024-03-05 15:55:48,959 - INFO - Epoch [146/300] (98343) train_loss: 24.5559, val_loss: 25.6216, lr: 0.000564, 91.37s
2024-03-05 15:57:13,908 - INFO - epoch complete!
2024-03-05 15:57:13,909 - INFO - evaluating now!
2024-03-05 15:57:19,947 - INFO - Epoch [147/300] (99012) train_loss: 24.6555, val_loss: 25.7949, lr: 0.000559, 90.99s
2024-03-05 15:58:44,793 - INFO - epoch complete!
2024-03-05 15:58:44,794 - INFO - evaluating now!
2024-03-05 15:58:50,838 - INFO - Epoch [148/300] (99681) train_loss: 24.4670, val_loss: 26.4822, lr: 0.000555, 90.89s
2024-03-05 16:00:15,909 - INFO - epoch complete!
2024-03-05 16:00:15,910 - INFO - evaluating now!
2024-03-05 16:00:21,935 - INFO - Epoch [149/300] (100350) train_loss: 24.4869, val_loss: 25.5067, lr: 0.000550, 91.10s
2024-03-05 16:01:46,781 - INFO - epoch complete!
2024-03-05 16:01:46,782 - INFO - evaluating now!
2024-03-05 16:01:52,818 - INFO - Epoch [150/300] (101019) train_loss: 24.4946, val_loss: 26.0012, lr: 0.000545, 90.88s
2024-03-05 16:03:28,812 - INFO - epoch complete!
2024-03-05 16:03:28,813 - INFO - evaluating now!
2024-03-05 16:03:34,894 - INFO - Epoch [151/300] (101688) train_loss: 24.3825, val_loss: 25.7494, lr: 0.000541, 102.08s
2024-03-05 16:04:59,851 - INFO - epoch complete!
2024-03-05 16:04:59,852 - INFO - evaluating now!
2024-03-05 16:05:06,132 - INFO - Epoch [152/300] (102357) train_loss: 24.3094, val_loss: 25.5908, lr: 0.000536, 91.24s
2024-03-05 16:06:35,483 - INFO - epoch complete!
2024-03-05 16:06:35,484 - INFO - evaluating now!
2024-03-05 16:06:41,565 - INFO - Epoch [153/300] (103026) train_loss: 24.3174, val_loss: 25.5730, lr: 0.000531, 95.43s
2024-03-05 16:08:09,686 - INFO - epoch complete!
2024-03-05 16:08:09,687 - INFO - evaluating now!
2024-03-05 16:08:15,726 - INFO - Epoch [154/300] (103695) train_loss: 24.3720, val_loss: 25.6905, lr: 0.000526, 94.16s
2024-03-05 16:09:40,854 - INFO - epoch complete!
2024-03-05 16:09:40,855 - INFO - evaluating now!
2024-03-05 16:09:46,883 - INFO - Epoch [155/300] (104364) train_loss: 24.2842, val_loss: 25.7091, lr: 0.000522, 91.16s
2024-03-05 16:11:22,426 - INFO - epoch complete!
2024-03-05 16:11:22,426 - INFO - evaluating now!
2024-03-05 16:11:28,518 - INFO - Epoch [156/300] (105033) train_loss: 24.2175, val_loss: 26.0394, lr: 0.000517, 101.63s
2024-03-05 16:12:57,511 - INFO - epoch complete!
2024-03-05 16:12:57,512 - INFO - evaluating now!
2024-03-05 16:13:03,731 - INFO - Epoch [157/300] (105702) train_loss: 24.1383, val_loss: 25.5868, lr: 0.000512, 95.21s
2024-03-05 16:14:40,080 - INFO - epoch complete!
2024-03-05 16:14:40,080 - INFO - evaluating now!
2024-03-05 16:14:46,135 - INFO - Epoch [158/300] (106371) train_loss: 24.1416, val_loss: 25.6051, lr: 0.000508, 102.40s
2024-03-05 16:16:21,012 - INFO - epoch complete!
2024-03-05 16:16:21,012 - INFO - evaluating now!
2024-03-05 16:16:27,038 - INFO - Epoch [159/300] (107040) train_loss: 24.1521, val_loss: 25.6609, lr: 0.000503, 100.90s
2024-03-05 16:17:52,966 - INFO - epoch complete!
2024-03-05 16:17:52,966 - INFO - evaluating now!
2024-03-05 16:17:59,006 - INFO - Epoch [160/300] (107709) train_loss: 24.1024, val_loss: 25.7640, lr: 0.000498, 91.97s
2024-03-05 16:19:25,171 - INFO - epoch complete!
2024-03-05 16:19:25,172 - INFO - evaluating now!
2024-03-05 16:19:31,196 - INFO - Epoch [161/300] (108378) train_loss: 24.0166, val_loss: 26.0223, lr: 0.000494, 92.19s
2024-03-05 16:20:56,853 - INFO - epoch complete!
2024-03-05 16:20:56,854 - INFO - evaluating now!
2024-03-05 16:21:03,010 - INFO - Epoch [162/300] (109047) train_loss: 23.9997, val_loss: 25.8133, lr: 0.000489, 91.81s
2024-03-05 16:22:28,886 - INFO - epoch complete!
2024-03-05 16:22:28,887 - INFO - evaluating now!
2024-03-05 16:22:34,914 - INFO - Epoch [163/300] (109716) train_loss: 24.0495, val_loss: 25.5751, lr: 0.000484, 91.90s
2024-03-05 16:24:00,999 - INFO - epoch complete!
2024-03-05 16:24:01,000 - INFO - evaluating now!
2024-03-05 16:24:07,336 - INFO - Epoch [164/300] (110385) train_loss: 23.9949, val_loss: 25.4579, lr: 0.000480, 92.42s
2024-03-05 16:25:33,683 - INFO - epoch complete!
2024-03-05 16:25:33,683 - INFO - evaluating now!
2024-03-05 16:25:39,726 - INFO - Epoch [165/300] (111054) train_loss: 24.0042, val_loss: 25.6723, lr: 0.000475, 92.39s
2024-03-05 16:27:05,838 - INFO - epoch complete!
2024-03-05 16:27:05,838 - INFO - evaluating now!
2024-03-05 16:27:12,038 - INFO - Epoch [166/300] (111723) train_loss: 23.9113, val_loss: 25.7223, lr: 0.000470, 92.31s
2024-03-05 16:28:38,242 - INFO - epoch complete!
2024-03-05 16:28:38,243 - INFO - evaluating now!
2024-03-05 16:28:44,352 - INFO - Epoch [167/300] (112392) train_loss: 23.8880, val_loss: 25.7932, lr: 0.000466, 92.31s
2024-03-05 16:30:11,387 - INFO - epoch complete!
2024-03-05 16:30:11,388 - INFO - evaluating now!
2024-03-05 16:30:17,830 - INFO - Epoch [168/300] (113061) train_loss: 23.7888, val_loss: 25.7870, lr: 0.000461, 93.48s
2024-03-05 16:31:45,834 - INFO - epoch complete!
2024-03-05 16:31:45,835 - INFO - evaluating now!
2024-03-05 16:31:51,852 - INFO - Epoch [169/300] (113730) train_loss: 23.7868, val_loss: 25.6672, lr: 0.000456, 94.02s
2024-03-05 16:33:19,256 - INFO - epoch complete!
2024-03-05 16:33:19,257 - INFO - evaluating now!
2024-03-05 16:33:25,333 - INFO - Epoch [170/300] (114399) train_loss: 23.8136, val_loss: 25.5582, lr: 0.000452, 93.48s
2024-03-05 16:34:52,472 - INFO - epoch complete!
2024-03-05 16:34:52,473 - INFO - evaluating now!
2024-03-05 16:34:58,514 - INFO - Epoch [171/300] (115068) train_loss: 23.8091, val_loss: 26.0999, lr: 0.000447, 93.18s
2024-03-05 16:36:24,575 - INFO - epoch complete!
2024-03-05 16:36:24,575 - INFO - evaluating now!
2024-03-05 16:36:30,610 - INFO - Epoch [172/300] (115737) train_loss: 23.7231, val_loss: 25.7409, lr: 0.000443, 92.10s
2024-03-05 16:37:56,393 - INFO - epoch complete!
2024-03-05 16:37:56,394 - INFO - evaluating now!
2024-03-05 16:38:02,468 - INFO - Epoch [173/300] (116406) train_loss: 23.7712, val_loss: 25.7214, lr: 0.000438, 91.86s
2024-03-05 16:39:28,551 - INFO - epoch complete!
2024-03-05 16:39:28,552 - INFO - evaluating now!
2024-03-05 16:39:34,687 - INFO - Epoch [174/300] (117075) train_loss: 23.6814, val_loss: 25.6701, lr: 0.000434, 92.22s
2024-03-05 16:41:01,088 - INFO - epoch complete!
2024-03-05 16:41:01,088 - INFO - evaluating now!
2024-03-05 16:41:07,358 - INFO - Epoch [175/300] (117744) train_loss: 23.6045, val_loss: 26.0361, lr: 0.000429, 92.67s
2024-03-05 16:42:33,321 - INFO - epoch complete!
2024-03-05 16:42:33,322 - INFO - evaluating now!
2024-03-05 16:42:39,441 - INFO - Epoch [176/300] (118413) train_loss: 23.6585, val_loss: 25.9696, lr: 0.000424, 92.08s
2024-03-05 16:44:05,614 - INFO - epoch complete!
2024-03-05 16:44:05,614 - INFO - evaluating now!
2024-03-05 16:44:11,826 - INFO - Epoch [177/300] (119082) train_loss: 23.6219, val_loss: 25.6654, lr: 0.000420, 92.38s
2024-03-05 16:45:40,698 - INFO - epoch complete!
2024-03-05 16:45:40,699 - INFO - evaluating now!
2024-03-05 16:45:46,836 - INFO - Epoch [178/300] (119751) train_loss: 23.5244, val_loss: 25.5544, lr: 0.000415, 95.01s
2024-03-05 16:47:20,313 - INFO - epoch complete!
2024-03-05 16:47:20,314 - INFO - evaluating now!
2024-03-05 16:47:27,298 - INFO - Epoch [179/300] (120420) train_loss: 23.5383, val_loss: 26.0227, lr: 0.000411, 100.46s
2024-03-05 16:49:01,874 - INFO - epoch complete!
2024-03-05 16:49:01,875 - INFO - evaluating now!
2024-03-05 16:49:08,985 - INFO - Epoch [180/300] (121089) train_loss: 23.5503, val_loss: 25.9239, lr: 0.000406, 101.69s
2024-03-05 16:50:43,033 - INFO - epoch complete!
2024-03-05 16:50:43,033 - INFO - evaluating now!
2024-03-05 16:50:50,056 - INFO - Epoch [181/300] (121758) train_loss: 23.4769, val_loss: 25.6111, lr: 0.000402, 101.07s
2024-03-05 16:52:33,034 - INFO - epoch complete!
2024-03-05 16:52:33,035 - INFO - evaluating now!
2024-03-05 16:52:40,044 - INFO - Epoch [182/300] (122427) train_loss: 23.4345, val_loss: 26.0250, lr: 0.000398, 109.99s
2024-03-05 16:54:14,790 - INFO - epoch complete!
2024-03-05 16:54:14,790 - INFO - evaluating now!
2024-03-05 16:54:21,776 - INFO - Epoch [183/300] (123096) train_loss: 23.4577, val_loss: 25.7168, lr: 0.000393, 101.73s
2024-03-05 16:55:55,605 - INFO - epoch complete!
2024-03-05 16:55:55,606 - INFO - evaluating now!
2024-03-05 16:56:02,652 - INFO - Epoch [184/300] (123765) train_loss: 23.3847, val_loss: 25.9263, lr: 0.000389, 100.88s
2024-03-05 16:57:37,451 - INFO - epoch complete!
2024-03-05 16:57:37,452 - INFO - evaluating now!
2024-03-05 16:57:44,489 - INFO - Epoch [185/300] (124434) train_loss: 23.3927, val_loss: 25.8353, lr: 0.000384, 101.84s
2024-03-05 16:59:19,195 - INFO - epoch complete!
2024-03-05 16:59:19,195 - INFO - evaluating now!
2024-03-05 16:59:25,733 - INFO - Epoch [186/300] (125103) train_loss: 23.3505, val_loss: 26.0599, lr: 0.000380, 101.24s
2024-03-05 17:01:00,602 - INFO - epoch complete!
2024-03-05 17:01:00,602 - INFO - evaluating now!
2024-03-05 17:01:07,710 - INFO - Epoch [187/300] (125772) train_loss: 23.3462, val_loss: 25.8323, lr: 0.000376, 101.98s
2024-03-05 17:02:42,451 - INFO - epoch complete!
2024-03-05 17:02:42,452 - INFO - evaluating now!
2024-03-05 17:02:49,512 - INFO - Epoch [188/300] (126441) train_loss: 23.2751, val_loss: 25.8701, lr: 0.000371, 101.80s
2024-03-05 17:04:23,891 - INFO - epoch complete!
2024-03-05 17:04:23,892 - INFO - evaluating now!
2024-03-05 17:04:30,924 - INFO - Epoch [189/300] (127110) train_loss: 23.2756, val_loss: 25.8166, lr: 0.000367, 101.41s
2024-03-05 17:06:06,038 - INFO - epoch complete!
2024-03-05 17:06:06,039 - INFO - evaluating now!
2024-03-05 17:06:13,149 - INFO - Epoch [190/300] (127779) train_loss: 23.2851, val_loss: 25.9746, lr: 0.000363, 102.22s
2024-03-05 17:07:48,179 - INFO - epoch complete!
2024-03-05 17:07:48,180 - INFO - evaluating now!
2024-03-05 17:07:55,212 - INFO - Epoch [191/300] (128448) train_loss: 23.2235, val_loss: 26.0759, lr: 0.000358, 102.06s
2024-03-05 17:09:29,956 - INFO - epoch complete!
2024-03-05 17:09:29,956 - INFO - evaluating now!
2024-03-05 17:09:37,048 - INFO - Epoch [192/300] (129117) train_loss: 23.1911, val_loss: 25.7701, lr: 0.000354, 101.84s
2024-03-05 17:11:12,487 - INFO - epoch complete!
2024-03-05 17:11:12,488 - INFO - evaluating now!
2024-03-05 17:11:19,617 - INFO - Epoch [193/300] (129786) train_loss: 23.2165, val_loss: 25.9625, lr: 0.000350, 102.57s
2024-03-05 17:12:54,437 - INFO - epoch complete!
2024-03-05 17:12:54,438 - INFO - evaluating now!
2024-03-05 17:13:01,052 - INFO - Epoch [194/300] (130455) train_loss: 23.1490, val_loss: 25.9904, lr: 0.000346, 101.43s
2024-03-05 17:14:36,347 - INFO - epoch complete!
2024-03-05 17:14:36,348 - INFO - evaluating now!
2024-03-05 17:14:43,425 - INFO - Epoch [195/300] (131124) train_loss: 23.1477, val_loss: 25.8435, lr: 0.000342, 102.37s
2024-03-05 17:14:43,425 - WARNING - Early stopping at epoch: 195
2024-03-05 17:14:43,426 - INFO - Trained totally 196 epochs, average train time is 112.036s, average eval time is 9.433s
2024-03-05 17:14:43,498 - INFO - Loaded model at 145
2024-03-05 17:14:43,499 - INFO - Saved model at ./libcity/cache/38389/model_cache/PDFormer_PeMS08.m
2024-03-05 17:14:43,544 - INFO - Start evaluating ...
2024-03-05 17:14:57,109 - INFO - Note that you select the average mode to evaluate!
2024-03-05 17:14:57,113 - INFO - Evaluate result is saved at ./libcity/cache/38389/evaluate_cache/2024_03_05_17_14_57_PDFormer_PeMS08_average.csv
2024-03-05 17:14:57,121 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   11.810679   inf  19.529205   11.826551     0.078701    19.422382
2   12.017363   inf  20.104536   12.033802     0.079693    20.001122
3   12.212392   inf  20.571758   12.229406     0.080789    20.470491
4   12.385894   inf  20.982853   12.403492     0.081797    20.885445
5   12.540405   inf  21.334692   12.558463     0.082851    21.240389
6   12.678697   inf  21.647465   12.697128     0.083781    21.554836
7   12.809052   inf  21.933683   12.827737     0.084665    21.842182
8   12.932791   inf  22.200548   12.951804     0.085467    22.110523
9   13.056328   inf  22.452337   13.075610     0.086289    22.362921
10  13.176977   inf  22.679890   13.196559     0.087115    22.591274
11  13.307940   inf  22.881763   13.327706     0.088147    22.793526
12  13.437184   inf  23.087593   13.457169     0.089162    23.000008
