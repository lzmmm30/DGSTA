2024-02-02 00:18:19,231 - INFO - Log directory: ./libcity/log
2024-02-02 00:18:19,231 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS08, exp_id=99976
2024-02-02 00:18:19,231 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS08', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 2776, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 21, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 6, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS08'], 'geo_file': 'PeMS08', 'rel_file': 'PeMS08', 'adp_file': 'PeMS08', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 99976}
2024-02-02 00:18:19,496 - INFO - Loaded file PeMS08.geo, num_nodes=170
2024-02-02 00:18:19,497 - INFO - set_weight_link_or_dist: link
2024-02-02 00:18:19,498 - INFO - init_weight_inf_or_zero: zero
2024-02-02 00:18:19,499 - INFO - Loaded file PeMS08.rel, shape=(170, 170)
2024-02-02 00:18:19,500 - INFO - Max adj_mx value = 1.0
2024-02-02 00:18:29,476 - INFO - Loading file PeMS08.dyna
2024-02-02 00:18:31,143 - INFO - Loaded file PeMS08.dyna, shape=(17856, 170, 1)
2024-02-02 00:18:31,163 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS08.npy
2024-02-02 00:18:31,163 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS08_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-02-02 00:18:38,040 - INFO - train	x: (10700, 12, 170, 9), y: (10700, 12, 170, 9), ind: (10700,)
2024-02-02 00:18:38,040 - INFO - eval	x: (3566, 12, 170, 9), y: (3566, 12, 170, 9), ind: (3566,)
2024-02-02 00:18:38,040 - INFO - test	x: (3567, 12, 170, 9), y: (3567, 12, 170, 9), ind: (3567,)
2024-02-02 00:18:38,508 - INFO - StandardScaler mean: 229.8431355598314, std: 145.62553066568907
2024-02-02 00:18:38,508 - INFO - NoneScaler
2024-02-02 00:18:39,780 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS08_21_3_16_5.npy
2024-02-02 00:18:39,782 - INFO - Use use_curriculum_learning!
2024-02-02 00:18:43,250 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (5): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-02-02 00:18:43,252 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,252 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,253 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,254 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 64])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,255 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,256 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,257 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,258 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,325 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,325 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,326 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,326 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,326 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,326 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,326 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,326 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,327 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,327 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,327 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,327 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,327 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,327 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 64])	cuda:0	True
2024-02-02 00:18:43,328 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,328 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-02-02 00:18:43,328 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,328 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,328 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,328 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-02-02 00:18:43,329 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,329 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,329 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,329 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-02-02 00:18:43,329 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,329 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-02-02 00:18:43,330 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,330 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,330 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,330 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-02-02 00:18:43,330 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,330 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,331 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-02-02 00:18:43,331 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,331 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,331 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,331 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,331 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,332 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,333 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,333 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,333 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,333 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,333 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,333 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,334 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,334 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,334 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,334 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,334 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,334 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,335 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 64])	cuda:0	True
2024-02-02 00:18:43,335 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,335 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-02-02 00:18:43,335 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,335 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,335 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,336 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-02-02 00:18:43,336 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,336 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,336 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,336 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-02-02 00:18:43,336 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.norm1.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.norm1.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.nodevec_p2	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.nodevec_p3	torch.Size([170, 40])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,337 - INFO - encoder_blocks.5.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.proj.weight	torch.Size([64, 64])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-02-02 00:18:43,338 - INFO - encoder_blocks.5.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.norm2.weight	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.norm2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - encoder_blocks.5.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.5.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - skip_convs.5.bias	torch.Size([256])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-02-02 00:18:43,339 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-02-02 00:18:43,340 - INFO - Total parameter numbers: 1109661
2024-02-02 00:18:43,342 - INFO - You select `adamw` optimizer.
2024-02-02 00:18:43,342 - INFO - You select `cosinelr` lr_scheduler.
2024-02-02 00:18:43,342 - WARNING - Received none train loss func and will use the loss func defined in the model.
2024-02-02 00:18:43,343 - INFO - Number of isolated points: 0
2024-02-02 00:18:43,356 - INFO - Start training ...
2024-02-02 00:18:43,356 - INFO - num_batches:669
2024-02-02 00:18:43,428 - INFO - Training: task_level increase from 0 to 1
2024-02-02 00:18:43,428 - INFO - Current batches_seen is 0
2024-02-02 00:21:36,764 - INFO - epoch complete!
2024-02-02 00:21:36,765 - INFO - evaluating now!
2024-02-02 00:21:51,518 - INFO - Epoch [0/300] (669) train_loss: 237.6307, val_loss: 266.2032, lr: 0.000201, 188.16s
2024-02-02 00:21:51,573 - INFO - Saved model at 0
2024-02-02 00:21:51,574 - INFO - Val loss decrease from inf to 266.2032, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch0.tar
2024-02-02 00:25:06,786 - INFO - epoch complete!
2024-02-02 00:25:06,787 - INFO - evaluating now!
2024-02-02 00:25:21,262 - INFO - Epoch [1/300] (1338) train_loss: 72.0306, val_loss: 272.4006, lr: 0.000401, 209.69s
2024-02-02 00:28:35,723 - INFO - epoch complete!
2024-02-02 00:28:35,724 - INFO - evaluating now!
2024-02-02 00:28:50,232 - INFO - Epoch [2/300] (2007) train_loss: 42.4554, val_loss: 273.0665, lr: 0.000600, 208.97s
2024-02-02 00:32:05,771 - INFO - epoch complete!
2024-02-02 00:32:05,772 - INFO - evaluating now!
2024-02-02 00:32:20,252 - INFO - Epoch [3/300] (2676) train_loss: 38.4476, val_loss: 241.8075, lr: 0.000800, 210.02s
2024-02-02 00:32:20,307 - INFO - Saved model at 3
2024-02-02 00:32:20,307 - INFO - Val loss decrease from 266.2032 to 241.8075, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch3.tar
2024-02-02 00:32:49,674 - INFO - Training: task_level increase from 1 to 2
2024-02-02 00:32:49,675 - INFO - Current batches_seen is 2776
2024-02-02 00:35:35,874 - INFO - epoch complete!
2024-02-02 00:35:35,874 - INFO - evaluating now!
2024-02-02 00:35:50,349 - INFO - Epoch [4/300] (3345) train_loss: 37.9907, val_loss: 224.8629, lr: 0.000999, 210.04s
2024-02-02 00:35:50,403 - INFO - Saved model at 4
2024-02-02 00:35:50,404 - INFO - Val loss decrease from 241.8075 to 224.8629, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch4.tar
2024-02-02 00:39:05,850 - INFO - epoch complete!
2024-02-02 00:39:05,851 - INFO - evaluating now!
2024-02-02 00:39:20,251 - INFO - Epoch [5/300] (4014) train_loss: 34.5742, val_loss: 219.5867, lr: 0.000999, 209.85s
2024-02-02 00:39:20,307 - INFO - Saved model at 5
2024-02-02 00:39:20,307 - INFO - Val loss decrease from 224.8629 to 219.5867, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch5.tar
2024-02-02 00:42:35,415 - INFO - epoch complete!
2024-02-02 00:42:35,416 - INFO - evaluating now!
2024-02-02 00:42:49,899 - INFO - Epoch [6/300] (4683) train_loss: 31.7348, val_loss: 232.0815, lr: 0.000999, 209.59s
2024-02-02 00:46:04,765 - INFO - epoch complete!
2024-02-02 00:46:04,766 - INFO - evaluating now!
2024-02-02 00:46:19,243 - INFO - Epoch [7/300] (5352) train_loss: 30.8128, val_loss: 225.9105, lr: 0.000998, 209.34s
2024-02-02 00:47:17,839 - INFO - Training: task_level increase from 2 to 3
2024-02-02 00:47:17,839 - INFO - Current batches_seen is 5552
2024-02-02 00:49:34,716 - INFO - epoch complete!
2024-02-02 00:49:34,717 - INFO - evaluating now!
2024-02-02 00:49:49,222 - INFO - Epoch [8/300] (6021) train_loss: 33.7119, val_loss: 169.2485, lr: 0.000998, 209.98s
2024-02-02 00:49:49,275 - INFO - Saved model at 8
2024-02-02 00:49:49,275 - INFO - Val loss decrease from 219.5867 to 169.2485, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch8.tar
2024-02-02 00:53:03,990 - INFO - epoch complete!
2024-02-02 00:53:03,990 - INFO - evaluating now!
2024-02-02 00:53:18,425 - INFO - Epoch [9/300] (6690) train_loss: 30.3197, val_loss: 168.5184, lr: 0.000998, 209.15s
2024-02-02 00:53:18,481 - INFO - Saved model at 9
2024-02-02 00:53:18,481 - INFO - Val loss decrease from 169.2485 to 168.5184, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch9.tar
2024-02-02 00:56:32,384 - INFO - epoch complete!
2024-02-02 00:56:32,385 - INFO - evaluating now!
2024-02-02 00:56:46,850 - INFO - Epoch [10/300] (7359) train_loss: 29.5025, val_loss: 169.7107, lr: 0.000997, 208.37s
2024-02-02 01:00:01,253 - INFO - epoch complete!
2024-02-02 01:00:01,254 - INFO - evaluating now!
2024-02-02 01:00:15,751 - INFO - Epoch [11/300] (8028) train_loss: 29.0958, val_loss: 170.4293, lr: 0.000996, 208.90s
2024-02-02 01:01:42,652 - INFO - Training: task_level increase from 3 to 4
2024-02-02 01:01:42,652 - INFO - Current batches_seen is 8328
2024-02-02 01:03:29,241 - INFO - epoch complete!
2024-02-02 01:03:29,242 - INFO - evaluating now!
2024-02-02 01:03:43,671 - INFO - Epoch [12/300] (8697) train_loss: 30.6010, val_loss: 159.0167, lr: 0.000996, 207.92s
2024-02-02 01:03:43,724 - INFO - Saved model at 12
2024-02-02 01:03:43,724 - INFO - Val loss decrease from 168.5184 to 159.0167, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch12.tar
2024-02-02 01:06:56,948 - INFO - epoch complete!
2024-02-02 01:06:56,948 - INFO - evaluating now!
2024-02-02 01:07:11,385 - INFO - Epoch [13/300] (9366) train_loss: 29.5209, val_loss: 164.2215, lr: 0.000995, 207.66s
2024-02-02 01:10:24,530 - INFO - epoch complete!
2024-02-02 01:10:24,531 - INFO - evaluating now!
2024-02-02 01:10:39,004 - INFO - Epoch [14/300] (10035) train_loss: 29.2791, val_loss: 166.1629, lr: 0.000994, 207.62s
2024-02-02 01:13:52,560 - INFO - epoch complete!
2024-02-02 01:13:52,560 - INFO - evaluating now!
2024-02-02 01:14:07,060 - INFO - Epoch [15/300] (10704) train_loss: 29.0017, val_loss: 169.0801, lr: 0.000994, 208.06s
2024-02-02 01:16:02,725 - INFO - Training: task_level increase from 4 to 5
2024-02-02 01:16:02,725 - INFO - Current batches_seen is 11104
2024-02-02 01:17:20,502 - INFO - epoch complete!
2024-02-02 01:17:20,502 - INFO - evaluating now!
2024-02-02 01:17:35,091 - INFO - Epoch [16/300] (11373) train_loss: 29.5823, val_loss: 149.0650, lr: 0.000993, 208.03s
2024-02-02 01:17:35,146 - INFO - Saved model at 16
2024-02-02 01:17:35,147 - INFO - Val loss decrease from 159.0167 to 149.0650, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch16.tar
2024-02-02 01:20:48,952 - INFO - epoch complete!
2024-02-02 01:20:48,953 - INFO - evaluating now!
2024-02-02 01:21:03,385 - INFO - Epoch [17/300] (12042) train_loss: 29.3538, val_loss: 146.0941, lr: 0.000992, 208.24s
2024-02-02 01:21:03,440 - INFO - Saved model at 17
2024-02-02 01:21:03,440 - INFO - Val loss decrease from 149.0650 to 146.0941, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch17.tar
2024-02-02 01:24:16,839 - INFO - epoch complete!
2024-02-02 01:24:16,839 - INFO - evaluating now!
2024-02-02 01:24:30,653 - INFO - Epoch [18/300] (12711) train_loss: 28.9146, val_loss: 142.4760, lr: 0.000991, 207.21s
2024-02-02 01:24:30,709 - INFO - Saved model at 18
2024-02-02 01:24:30,710 - INFO - Val loss decrease from 146.0941 to 142.4760, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch18.tar
2024-02-02 01:27:37,617 - INFO - epoch complete!
2024-02-02 01:27:37,618 - INFO - evaluating now!
2024-02-02 01:27:51,471 - INFO - Epoch [19/300] (13380) train_loss: 28.6696, val_loss: 145.7101, lr: 0.000990, 200.76s
2024-02-02 01:30:12,734 - INFO - Training: task_level increase from 5 to 6
2024-02-02 01:30:12,734 - INFO - Current batches_seen is 13880
2024-02-02 01:30:59,878 - INFO - epoch complete!
2024-02-02 01:30:59,879 - INFO - evaluating now!
2024-02-02 01:31:13,657 - INFO - Epoch [20/300] (14049) train_loss: 28.9518, val_loss: 111.3131, lr: 0.000989, 202.18s
2024-02-02 01:31:13,709 - INFO - Saved model at 20
2024-02-02 01:31:13,710 - INFO - Val loss decrease from 142.4760 to 111.3131, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch20.tar
2024-02-02 01:34:20,687 - INFO - epoch complete!
2024-02-02 01:34:20,688 - INFO - evaluating now!
2024-02-02 01:34:34,447 - INFO - Epoch [21/300] (14718) train_loss: 28.8225, val_loss: 117.0515, lr: 0.000988, 200.74s
2024-02-02 01:37:41,411 - INFO - epoch complete!
2024-02-02 01:37:41,411 - INFO - evaluating now!
2024-02-02 01:37:55,166 - INFO - Epoch [22/300] (15387) train_loss: 28.6872, val_loss: 120.3470, lr: 0.000987, 200.72s
2024-02-02 01:41:02,838 - INFO - epoch complete!
2024-02-02 01:41:02,838 - INFO - evaluating now!
2024-02-02 01:41:16,817 - INFO - Epoch [23/300] (16056) train_loss: 28.3794, val_loss: 119.5932, lr: 0.000986, 201.65s
2024-02-02 01:44:04,603 - INFO - Training: task_level increase from 6 to 7
2024-02-02 01:44:04,603 - INFO - Current batches_seen is 16656
2024-02-02 01:44:23,968 - INFO - epoch complete!
2024-02-02 01:44:23,969 - INFO - evaluating now!
2024-02-02 01:44:38,223 - INFO - Epoch [24/300] (16725) train_loss: 28.6994, val_loss: 99.7487, lr: 0.000985, 201.41s
2024-02-02 01:44:38,276 - INFO - Saved model at 24
2024-02-02 01:44:38,277 - INFO - Val loss decrease from 111.3131 to 99.7487, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch24.tar
2024-02-02 01:47:44,909 - INFO - epoch complete!
2024-02-02 01:47:44,910 - INFO - evaluating now!
2024-02-02 01:47:58,673 - INFO - Epoch [25/300] (17394) train_loss: 28.6291, val_loss: 92.9658, lr: 0.000983, 200.40s
2024-02-02 01:47:58,728 - INFO - Saved model at 25
2024-02-02 01:47:58,728 - INFO - Val loss decrease from 99.7487 to 92.9658, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch25.tar
2024-02-02 01:51:05,667 - INFO - epoch complete!
2024-02-02 01:51:05,668 - INFO - evaluating now!
2024-02-02 01:51:19,428 - INFO - Epoch [26/300] (18063) train_loss: 28.2975, val_loss: 93.9667, lr: 0.000982, 200.70s
2024-02-02 01:54:26,335 - INFO - epoch complete!
2024-02-02 01:54:26,336 - INFO - evaluating now!
2024-02-02 01:54:40,116 - INFO - Epoch [27/300] (18732) train_loss: 28.0981, val_loss: 93.8508, lr: 0.000981, 200.69s
2024-02-02 01:57:47,065 - INFO - epoch complete!
2024-02-02 01:57:47,065 - INFO - evaluating now!
2024-02-02 01:58:00,870 - INFO - Epoch [28/300] (19401) train_loss: 27.9991, val_loss: 94.5993, lr: 0.000979, 200.75s
2024-02-02 01:58:09,592 - INFO - Training: task_level increase from 7 to 8
2024-02-02 01:58:09,592 - INFO - Current batches_seen is 19432
2024-02-02 02:01:07,756 - INFO - epoch complete!
2024-02-02 02:01:07,757 - INFO - evaluating now!
2024-02-02 02:01:21,555 - INFO - Epoch [29/300] (20070) train_loss: 28.6742, val_loss: 76.8895, lr: 0.000978, 200.68s
2024-02-02 02:01:21,610 - INFO - Saved model at 29
2024-02-02 02:01:21,610 - INFO - Val loss decrease from 92.9658 to 76.8895, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch29.tar
2024-02-02 02:04:28,534 - INFO - epoch complete!
2024-02-02 02:04:28,534 - INFO - evaluating now!
2024-02-02 02:04:42,293 - INFO - Epoch [30/300] (20739) train_loss: 28.1534, val_loss: 77.6915, lr: 0.000976, 200.68s
2024-02-02 02:07:49,054 - INFO - epoch complete!
2024-02-02 02:07:49,055 - INFO - evaluating now!
2024-02-02 02:08:02,836 - INFO - Epoch [31/300] (21408) train_loss: 27.9630, val_loss: 77.1825, lr: 0.000975, 200.54s
2024-02-02 02:11:09,871 - INFO - epoch complete!
2024-02-02 02:11:09,872 - INFO - evaluating now!
2024-02-02 02:11:23,668 - INFO - Epoch [32/300] (22077) train_loss: 27.7931, val_loss: 76.5143, lr: 0.000973, 200.83s
2024-02-02 02:11:23,722 - INFO - Saved model at 32
2024-02-02 02:11:23,722 - INFO - Val loss decrease from 76.8895 to 76.5143, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch32.tar
2024-02-02 02:12:00,730 - INFO - Training: task_level increase from 8 to 9
2024-02-02 02:12:00,730 - INFO - Current batches_seen is 22208
2024-02-02 02:14:30,581 - INFO - epoch complete!
2024-02-02 02:14:30,582 - INFO - evaluating now!
2024-02-02 02:14:44,328 - INFO - Epoch [33/300] (22746) train_loss: 28.1451, val_loss: 76.6955, lr: 0.000972, 200.61s
2024-02-02 02:17:50,871 - INFO - epoch complete!
2024-02-02 02:17:50,872 - INFO - evaluating now!
2024-02-02 02:18:05,389 - INFO - Epoch [34/300] (23415) train_loss: 28.0571, val_loss: 76.4991, lr: 0.000970, 201.06s
2024-02-02 02:18:05,444 - INFO - Saved model at 34
2024-02-02 02:18:05,445 - INFO - Val loss decrease from 76.5143 to 76.4991, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch34.tar
2024-02-02 02:21:13,209 - INFO - epoch complete!
2024-02-02 02:21:13,210 - INFO - evaluating now!
2024-02-02 02:21:27,698 - INFO - Epoch [35/300] (24084) train_loss: 27.7580, val_loss: 76.3447, lr: 0.000968, 202.25s
2024-02-02 02:21:27,753 - INFO - Saved model at 35
2024-02-02 02:21:27,753 - INFO - Val loss decrease from 76.4991 to 76.3447, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch35.tar
2024-02-02 02:24:41,416 - INFO - epoch complete!
2024-02-02 02:24:41,417 - INFO - evaluating now!
2024-02-02 02:24:55,913 - INFO - Epoch [36/300] (24753) train_loss: 27.6933, val_loss: 77.3975, lr: 0.000967, 208.16s
2024-02-02 02:26:02,914 - INFO - Training: task_level increase from 9 to 10
2024-02-02 02:26:02,914 - INFO - Current batches_seen is 24984
2024-02-02 02:28:09,477 - INFO - epoch complete!
2024-02-02 02:28:09,477 - INFO - evaluating now!
2024-02-02 02:28:23,919 - INFO - Epoch [37/300] (25422) train_loss: 28.4549, val_loss: 57.9466, lr: 0.000965, 208.00s
2024-02-02 02:28:23,974 - INFO - Saved model at 37
2024-02-02 02:28:23,975 - INFO - Val loss decrease from 76.3447 to 57.9466, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch37.tar
2024-02-02 02:31:37,361 - INFO - epoch complete!
2024-02-02 02:31:37,362 - INFO - evaluating now!
2024-02-02 02:31:51,780 - INFO - Epoch [38/300] (26091) train_loss: 28.0671, val_loss: 58.8007, lr: 0.000963, 207.81s
2024-02-02 02:35:05,556 - INFO - epoch complete!
2024-02-02 02:35:05,557 - INFO - evaluating now!
2024-02-02 02:35:20,006 - INFO - Epoch [39/300] (26760) train_loss: 27.8825, val_loss: 58.9193, lr: 0.000961, 208.22s
2024-02-02 02:38:33,802 - INFO - epoch complete!
2024-02-02 02:38:33,803 - INFO - evaluating now!
2024-02-02 02:38:48,336 - INFO - Epoch [40/300] (27429) train_loss: 27.7736, val_loss: 59.8172, lr: 0.000959, 208.33s
2024-02-02 02:40:24,324 - INFO - Training: task_level increase from 10 to 11
2024-02-02 02:40:24,324 - INFO - Current batches_seen is 27760
2024-02-02 02:42:02,086 - INFO - epoch complete!
2024-02-02 02:42:02,087 - INFO - evaluating now!
2024-02-02 02:42:16,479 - INFO - Epoch [41/300] (28098) train_loss: 28.1358, val_loss: 45.8916, lr: 0.000957, 208.14s
2024-02-02 02:42:16,532 - INFO - Saved model at 41
2024-02-02 02:42:16,532 - INFO - Val loss decrease from 57.9466 to 45.8916, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch41.tar
2024-02-02 02:45:30,073 - INFO - epoch complete!
2024-02-02 02:45:30,074 - INFO - evaluating now!
2024-02-02 02:45:44,476 - INFO - Epoch [42/300] (28767) train_loss: 27.9819, val_loss: 45.4543, lr: 0.000955, 207.94s
2024-02-02 02:45:44,532 - INFO - Saved model at 42
2024-02-02 02:45:44,532 - INFO - Val loss decrease from 45.8916 to 45.4543, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch42.tar
2024-02-02 02:48:57,778 - INFO - epoch complete!
2024-02-02 02:48:57,779 - INFO - evaluating now!
2024-02-02 02:49:12,167 - INFO - Epoch [43/300] (29436) train_loss: 27.7850, val_loss: 45.5955, lr: 0.000953, 207.63s
2024-02-02 02:52:25,183 - INFO - epoch complete!
2024-02-02 02:52:25,184 - INFO - evaluating now!
2024-02-02 02:52:39,655 - INFO - Epoch [44/300] (30105) train_loss: 27.8089, val_loss: 45.5012, lr: 0.000951, 207.49s
2024-02-02 02:54:44,230 - INFO - Training: task_level increase from 11 to 12
2024-02-02 02:54:44,231 - INFO - Current batches_seen is 30536
2024-02-02 02:55:52,887 - INFO - epoch complete!
2024-02-02 02:55:52,888 - INFO - evaluating now!
2024-02-02 02:56:07,829 - INFO - Epoch [45/300] (30774) train_loss: 28.4710, val_loss: 28.1886, lr: 0.000949, 208.17s
2024-02-02 02:56:07,884 - INFO - Saved model at 45
2024-02-02 02:56:07,884 - INFO - Val loss decrease from 45.4543 to 28.1886, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch45.tar
2024-02-02 02:59:20,828 - INFO - epoch complete!
2024-02-02 02:59:20,829 - INFO - evaluating now!
2024-02-02 02:59:35,278 - INFO - Epoch [46/300] (31443) train_loss: 28.1667, val_loss: 28.0367, lr: 0.000947, 207.39s
2024-02-02 02:59:35,332 - INFO - Saved model at 46
2024-02-02 02:59:35,332 - INFO - Val loss decrease from 28.1886 to 28.0367, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch46.tar
2024-02-02 03:02:48,548 - INFO - epoch complete!
2024-02-02 03:02:48,548 - INFO - evaluating now!
2024-02-02 03:03:02,951 - INFO - Epoch [47/300] (32112) train_loss: 28.1321, val_loss: 27.8412, lr: 0.000944, 207.62s
2024-02-02 03:03:03,006 - INFO - Saved model at 47
2024-02-02 03:03:03,006 - INFO - Val loss decrease from 28.0367 to 27.8412, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch47.tar
2024-02-02 03:06:15,867 - INFO - epoch complete!
2024-02-02 03:06:15,867 - INFO - evaluating now!
2024-02-02 03:06:30,298 - INFO - Epoch [48/300] (32781) train_loss: 28.0202, val_loss: 27.8438, lr: 0.000942, 207.29s
2024-02-02 03:09:43,726 - INFO - epoch complete!
2024-02-02 03:09:43,727 - INFO - evaluating now!
2024-02-02 03:09:58,206 - INFO - Epoch [49/300] (33450) train_loss: 27.8871, val_loss: 28.0822, lr: 0.000940, 207.91s
2024-02-02 03:13:12,285 - INFO - epoch complete!
2024-02-02 03:13:12,286 - INFO - evaluating now!
2024-02-02 03:13:26,912 - INFO - Epoch [50/300] (34119) train_loss: 27.7137, val_loss: 27.9190, lr: 0.000937, 208.70s
2024-02-02 03:16:39,962 - INFO - epoch complete!
2024-02-02 03:16:39,962 - INFO - evaluating now!
2024-02-02 03:16:54,201 - INFO - Epoch [51/300] (34788) train_loss: 27.7072, val_loss: 27.5482, lr: 0.000935, 207.29s
2024-02-02 03:16:54,255 - INFO - Saved model at 51
2024-02-02 03:16:54,255 - INFO - Val loss decrease from 27.8412 to 27.5482, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch51.tar
2024-02-02 03:20:07,028 - INFO - epoch complete!
2024-02-02 03:20:07,029 - INFO - evaluating now!
2024-02-02 03:20:21,518 - INFO - Epoch [52/300] (35457) train_loss: 27.5206, val_loss: 27.6581, lr: 0.000932, 207.26s
2024-02-02 03:23:35,595 - INFO - epoch complete!
2024-02-02 03:23:35,596 - INFO - evaluating now!
2024-02-02 03:23:50,064 - INFO - Epoch [53/300] (36126) train_loss: 27.5566, val_loss: 27.3878, lr: 0.000930, 208.55s
2024-02-02 03:23:50,118 - INFO - Saved model at 53
2024-02-02 03:23:50,118 - INFO - Val loss decrease from 27.5482 to 27.3878, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch53.tar
2024-02-02 03:27:03,874 - INFO - epoch complete!
2024-02-02 03:27:03,875 - INFO - evaluating now!
2024-02-02 03:27:17,809 - INFO - Epoch [54/300] (36795) train_loss: 27.4177, val_loss: 27.5813, lr: 0.000927, 207.69s
2024-02-02 03:30:27,835 - INFO - epoch complete!
2024-02-02 03:30:27,835 - INFO - evaluating now!
2024-02-02 03:30:42,266 - INFO - Epoch [55/300] (37464) train_loss: 27.4043, val_loss: 27.4904, lr: 0.000925, 204.46s
2024-02-02 03:33:55,878 - INFO - epoch complete!
2024-02-02 03:33:55,878 - INFO - evaluating now!
2024-02-02 03:34:10,640 - INFO - Epoch [56/300] (38133) train_loss: 27.2588, val_loss: 28.5353, lr: 0.000922, 208.37s
2024-02-02 03:37:24,507 - INFO - epoch complete!
2024-02-02 03:37:24,508 - INFO - evaluating now!
2024-02-02 03:37:38,939 - INFO - Epoch [57/300] (38802) train_loss: 27.2322, val_loss: 27.2435, lr: 0.000920, 208.30s
2024-02-02 03:37:39,160 - INFO - Saved model at 57
2024-02-02 03:37:39,161 - INFO - Val loss decrease from 27.3878 to 27.2435, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch57.tar
2024-02-02 03:40:51,967 - INFO - epoch complete!
2024-02-02 03:40:51,968 - INFO - evaluating now!
2024-02-02 03:41:06,402 - INFO - Epoch [58/300] (39471) train_loss: 27.1593, val_loss: 27.4476, lr: 0.000917, 207.24s
2024-02-02 03:44:19,275 - INFO - epoch complete!
2024-02-02 03:44:19,276 - INFO - evaluating now!
2024-02-02 03:44:33,593 - INFO - Epoch [59/300] (40140) train_loss: 27.1085, val_loss: 27.3004, lr: 0.000914, 207.19s
2024-02-02 03:47:47,087 - INFO - epoch complete!
2024-02-02 03:47:47,088 - INFO - evaluating now!
2024-02-02 03:48:01,495 - INFO - Epoch [60/300] (40809) train_loss: 27.0841, val_loss: 27.2649, lr: 0.000911, 207.90s
2024-02-02 03:51:15,244 - INFO - epoch complete!
2024-02-02 03:51:15,245 - INFO - evaluating now!
2024-02-02 03:51:29,952 - INFO - Epoch [61/300] (41478) train_loss: 26.9694, val_loss: 27.4324, lr: 0.000908, 208.46s
2024-02-02 03:54:43,297 - INFO - epoch complete!
2024-02-02 03:54:43,298 - INFO - evaluating now!
2024-02-02 03:54:57,766 - INFO - Epoch [62/300] (42147) train_loss: 26.8891, val_loss: 26.7573, lr: 0.000906, 207.81s
2024-02-02 03:54:57,820 - INFO - Saved model at 62
2024-02-02 03:54:57,820 - INFO - Val loss decrease from 27.2435 to 26.7573, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch62.tar
2024-02-02 03:58:11,437 - INFO - epoch complete!
2024-02-02 03:58:11,438 - INFO - evaluating now!
2024-02-02 03:58:25,891 - INFO - Epoch [63/300] (42816) train_loss: 26.8678, val_loss: 27.1373, lr: 0.000903, 208.07s
2024-02-02 04:01:39,058 - INFO - epoch complete!
2024-02-02 04:01:39,058 - INFO - evaluating now!
2024-02-02 04:01:53,476 - INFO - Epoch [64/300] (43485) train_loss: 26.7784, val_loss: 27.2224, lr: 0.000900, 207.58s
2024-02-02 04:05:06,406 - INFO - epoch complete!
2024-02-02 04:05:06,407 - INFO - evaluating now!
2024-02-02 04:05:20,849 - INFO - Epoch [65/300] (44154) train_loss: 26.7900, val_loss: 26.6172, lr: 0.000897, 207.37s
2024-02-02 04:05:20,902 - INFO - Saved model at 65
2024-02-02 04:05:20,903 - INFO - Val loss decrease from 26.7573 to 26.6172, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch65.tar
2024-02-02 04:08:34,385 - INFO - epoch complete!
2024-02-02 04:08:34,385 - INFO - evaluating now!
2024-02-02 04:08:48,816 - INFO - Epoch [66/300] (44823) train_loss: 26.6323, val_loss: 26.9763, lr: 0.000894, 207.91s
2024-02-02 04:12:02,385 - INFO - epoch complete!
2024-02-02 04:12:02,386 - INFO - evaluating now!
2024-02-02 04:12:16,858 - INFO - Epoch [67/300] (45492) train_loss: 26.5438, val_loss: 26.6773, lr: 0.000891, 208.04s
2024-02-02 04:15:30,448 - INFO - epoch complete!
2024-02-02 04:15:30,448 - INFO - evaluating now!
2024-02-02 04:15:44,894 - INFO - Epoch [68/300] (46161) train_loss: 26.5906, val_loss: 26.7369, lr: 0.000888, 208.04s
2024-02-02 04:18:58,370 - INFO - epoch complete!
2024-02-02 04:18:58,371 - INFO - evaluating now!
2024-02-02 04:19:12,825 - INFO - Epoch [69/300] (46830) train_loss: 26.5832, val_loss: 26.5907, lr: 0.000884, 207.93s
2024-02-02 04:19:12,880 - INFO - Saved model at 69
2024-02-02 04:19:12,881 - INFO - Val loss decrease from 26.6172 to 26.5907, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch69.tar
2024-02-02 04:22:26,552 - INFO - epoch complete!
2024-02-02 04:22:26,553 - INFO - evaluating now!
2024-02-02 04:22:40,971 - INFO - Epoch [70/300] (47499) train_loss: 26.4492, val_loss: 26.4126, lr: 0.000881, 208.09s
2024-02-02 04:22:41,026 - INFO - Saved model at 70
2024-02-02 04:22:41,026 - INFO - Val loss decrease from 26.5907 to 26.4126, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch70.tar
2024-02-02 04:25:54,538 - INFO - epoch complete!
2024-02-02 04:25:54,539 - INFO - evaluating now!
2024-02-02 04:26:09,060 - INFO - Epoch [71/300] (48168) train_loss: 26.4307, val_loss: 26.3497, lr: 0.000878, 208.03s
2024-02-02 04:26:09,115 - INFO - Saved model at 71
2024-02-02 04:26:09,116 - INFO - Val loss decrease from 26.4126 to 26.3497, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch71.tar
2024-02-02 04:29:22,761 - INFO - epoch complete!
2024-02-02 04:29:22,761 - INFO - evaluating now!
2024-02-02 04:29:37,402 - INFO - Epoch [72/300] (48837) train_loss: 26.3431, val_loss: 26.8715, lr: 0.000875, 208.29s
2024-02-02 04:32:51,079 - INFO - epoch complete!
2024-02-02 04:32:51,080 - INFO - evaluating now!
2024-02-02 04:33:05,502 - INFO - Epoch [73/300] (49506) train_loss: 26.3750, val_loss: 26.6936, lr: 0.000872, 208.10s
2024-02-02 04:36:18,545 - INFO - epoch complete!
2024-02-02 04:36:18,546 - INFO - evaluating now!
2024-02-02 04:36:32,990 - INFO - Epoch [74/300] (50175) train_loss: 26.3228, val_loss: 26.9144, lr: 0.000868, 207.49s
2024-02-02 04:39:45,896 - INFO - epoch complete!
2024-02-02 04:39:45,897 - INFO - evaluating now!
2024-02-02 04:40:00,355 - INFO - Epoch [75/300] (50844) train_loss: 26.2338, val_loss: 26.1572, lr: 0.000865, 207.36s
2024-02-02 04:40:00,408 - INFO - Saved model at 75
2024-02-02 04:40:00,408 - INFO - Val loss decrease from 26.3497 to 26.1572, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch75.tar
2024-02-02 04:43:13,576 - INFO - epoch complete!
2024-02-02 04:43:13,577 - INFO - evaluating now!
2024-02-02 04:43:27,194 - INFO - Epoch [76/300] (51513) train_loss: 26.1417, val_loss: 26.2703, lr: 0.000861, 206.79s
2024-02-02 04:46:39,318 - INFO - epoch complete!
2024-02-02 04:46:39,318 - INFO - evaluating now!
2024-02-02 04:46:54,001 - INFO - Epoch [77/300] (52182) train_loss: 26.1033, val_loss: 26.1986, lr: 0.000858, 206.81s
2024-02-02 04:50:06,999 - INFO - epoch complete!
2024-02-02 04:50:07,000 - INFO - evaluating now!
2024-02-02 04:50:21,370 - INFO - Epoch [78/300] (52851) train_loss: 26.1889, val_loss: 26.3061, lr: 0.000855, 207.37s
2024-02-02 04:53:34,387 - INFO - epoch complete!
2024-02-02 04:53:34,388 - INFO - evaluating now!
2024-02-02 04:53:48,765 - INFO - Epoch [79/300] (53520) train_loss: 26.0371, val_loss: 26.6693, lr: 0.000851, 207.39s
2024-02-02 04:57:01,827 - INFO - epoch complete!
2024-02-02 04:57:01,828 - INFO - evaluating now!
2024-02-02 04:57:16,249 - INFO - Epoch [80/300] (54189) train_loss: 26.1143, val_loss: 26.3050, lr: 0.000848, 207.48s
2024-02-02 05:00:29,656 - INFO - epoch complete!
2024-02-02 05:00:29,657 - INFO - evaluating now!
2024-02-02 05:00:44,078 - INFO - Epoch [81/300] (54858) train_loss: 26.0153, val_loss: 26.1581, lr: 0.000844, 207.83s
2024-02-02 05:03:57,342 - INFO - epoch complete!
2024-02-02 05:03:57,343 - INFO - evaluating now!
2024-02-02 05:04:11,763 - INFO - Epoch [82/300] (55527) train_loss: 26.1092, val_loss: 26.4031, lr: 0.000840, 207.68s
2024-02-02 05:07:24,677 - INFO - epoch complete!
2024-02-02 05:07:24,678 - INFO - evaluating now!
2024-02-02 05:07:39,145 - INFO - Epoch [83/300] (56196) train_loss: 25.9309, val_loss: 26.0872, lr: 0.000837, 207.38s
2024-02-02 05:07:39,203 - INFO - Saved model at 83
2024-02-02 05:07:39,203 - INFO - Val loss decrease from 26.1572 to 26.0872, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch83.tar
2024-02-02 05:10:52,829 - INFO - epoch complete!
2024-02-02 05:10:52,829 - INFO - evaluating now!
2024-02-02 05:11:07,576 - INFO - Epoch [84/300] (56865) train_loss: 25.8839, val_loss: 26.2643, lr: 0.000833, 208.37s
2024-02-02 05:14:24,562 - INFO - epoch complete!
2024-02-02 05:14:24,563 - INFO - evaluating now!
2024-02-02 05:14:39,339 - INFO - Epoch [85/300] (57534) train_loss: 25.9351, val_loss: 26.3302, lr: 0.000830, 211.76s
2024-02-02 05:17:51,101 - INFO - epoch complete!
2024-02-02 05:17:51,102 - INFO - evaluating now!
2024-02-02 05:18:05,518 - INFO - Epoch [86/300] (58203) train_loss: 25.8655, val_loss: 26.6346, lr: 0.000826, 206.18s
2024-02-02 05:21:18,759 - INFO - epoch complete!
2024-02-02 05:21:18,759 - INFO - evaluating now!
2024-02-02 05:21:33,122 - INFO - Epoch [87/300] (58872) train_loss: 25.8980, val_loss: 26.1205, lr: 0.000822, 207.60s
2024-02-02 05:24:45,865 - INFO - epoch complete!
2024-02-02 05:24:45,866 - INFO - evaluating now!
2024-02-02 05:25:00,446 - INFO - Epoch [88/300] (59541) train_loss: 25.8021, val_loss: 26.2123, lr: 0.000818, 207.32s
2024-02-02 05:28:13,728 - INFO - epoch complete!
2024-02-02 05:28:13,729 - INFO - evaluating now!
2024-02-02 05:28:28,193 - INFO - Epoch [89/300] (60210) train_loss: 25.8070, val_loss: 26.2626, lr: 0.000815, 207.75s
2024-02-02 05:31:41,338 - INFO - epoch complete!
2024-02-02 05:31:41,339 - INFO - evaluating now!
2024-02-02 05:31:55,770 - INFO - Epoch [90/300] (60879) train_loss: 25.7190, val_loss: 26.1539, lr: 0.000811, 207.58s
2024-02-02 05:35:09,101 - INFO - epoch complete!
2024-02-02 05:35:09,102 - INFO - evaluating now!
2024-02-02 05:35:23,509 - INFO - Epoch [91/300] (61548) train_loss: 25.8335, val_loss: 26.0863, lr: 0.000807, 207.74s
2024-02-02 05:35:23,565 - INFO - Saved model at 91
2024-02-02 05:35:23,565 - INFO - Val loss decrease from 26.0872 to 26.0863, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch91.tar
2024-02-02 05:38:36,348 - INFO - epoch complete!
2024-02-02 05:38:36,349 - INFO - evaluating now!
2024-02-02 05:38:50,704 - INFO - Epoch [92/300] (62217) train_loss: 25.6165, val_loss: 26.2588, lr: 0.000803, 207.14s
2024-02-02 05:42:03,635 - INFO - epoch complete!
2024-02-02 05:42:03,636 - INFO - evaluating now!
2024-02-02 05:42:18,252 - INFO - Epoch [93/300] (62886) train_loss: 25.7062, val_loss: 26.0661, lr: 0.000799, 207.55s
2024-02-02 05:42:18,306 - INFO - Saved model at 93
2024-02-02 05:42:18,306 - INFO - Val loss decrease from 26.0863 to 26.0661, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch93.tar
2024-02-02 05:45:31,365 - INFO - epoch complete!
2024-02-02 05:45:31,365 - INFO - evaluating now!
2024-02-02 05:45:45,809 - INFO - Epoch [94/300] (63555) train_loss: 25.6223, val_loss: 26.2596, lr: 0.000795, 207.50s
2024-02-02 05:48:59,081 - INFO - epoch complete!
2024-02-02 05:48:59,081 - INFO - evaluating now!
2024-02-02 05:49:13,543 - INFO - Epoch [95/300] (64224) train_loss: 25.6646, val_loss: 26.6240, lr: 0.000791, 207.73s
2024-02-02 05:52:26,711 - INFO - epoch complete!
2024-02-02 05:52:26,712 - INFO - evaluating now!
2024-02-02 05:52:41,150 - INFO - Epoch [96/300] (64893) train_loss: 25.5759, val_loss: 28.5254, lr: 0.000787, 207.61s
2024-02-02 05:55:54,130 - INFO - epoch complete!
2024-02-02 05:55:54,130 - INFO - evaluating now!
2024-02-02 05:56:08,503 - INFO - Epoch [97/300] (65562) train_loss: 25.6099, val_loss: 26.0391, lr: 0.000783, 207.35s
2024-02-02 05:56:08,556 - INFO - Saved model at 97
2024-02-02 05:56:08,557 - INFO - Val loss decrease from 26.0661 to 26.0391, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch97.tar
2024-02-02 05:59:21,408 - INFO - epoch complete!
2024-02-02 05:59:21,409 - INFO - evaluating now!
2024-02-02 05:59:35,748 - INFO - Epoch [98/300] (66231) train_loss: 25.5799, val_loss: 26.1607, lr: 0.000779, 207.19s
2024-02-02 06:02:48,719 - INFO - epoch complete!
2024-02-02 06:02:48,720 - INFO - evaluating now!
2024-02-02 06:03:03,206 - INFO - Epoch [99/300] (66900) train_loss: 25.4634, val_loss: 26.0485, lr: 0.000775, 207.46s
2024-02-02 06:06:16,323 - INFO - epoch complete!
2024-02-02 06:06:16,324 - INFO - evaluating now!
2024-02-02 06:06:30,744 - INFO - Epoch [100/300] (67569) train_loss: 25.4878, val_loss: 25.8956, lr: 0.000771, 207.54s
2024-02-02 06:06:30,798 - INFO - Saved model at 100
2024-02-02 06:06:30,798 - INFO - Val loss decrease from 26.0391 to 25.8956, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch100.tar
2024-02-02 06:09:44,088 - INFO - epoch complete!
2024-02-02 06:09:44,089 - INFO - evaluating now!
2024-02-02 06:09:58,511 - INFO - Epoch [101/300] (68238) train_loss: 25.5058, val_loss: 26.0927, lr: 0.000767, 207.71s
2024-02-02 06:13:11,730 - INFO - epoch complete!
2024-02-02 06:13:11,731 - INFO - evaluating now!
2024-02-02 06:13:26,159 - INFO - Epoch [102/300] (68907) train_loss: 25.4853, val_loss: 26.4996, lr: 0.000763, 207.65s
2024-02-02 06:16:39,566 - INFO - epoch complete!
2024-02-02 06:16:39,566 - INFO - evaluating now!
2024-02-02 06:16:53,979 - INFO - Epoch [103/300] (69576) train_loss: 25.5116, val_loss: 25.8478, lr: 0.000758, 207.82s
2024-02-02 06:16:54,032 - INFO - Saved model at 103
2024-02-02 06:16:54,032 - INFO - Val loss decrease from 25.8956 to 25.8478, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch103.tar
2024-02-02 06:20:07,308 - INFO - epoch complete!
2024-02-02 06:20:07,309 - INFO - evaluating now!
2024-02-02 06:20:22,291 - INFO - Epoch [104/300] (70245) train_loss: 25.3534, val_loss: 26.1340, lr: 0.000754, 208.26s
2024-02-02 06:23:35,348 - INFO - epoch complete!
2024-02-02 06:23:35,349 - INFO - evaluating now!
2024-02-02 06:23:49,798 - INFO - Epoch [105/300] (70914) train_loss: 25.3685, val_loss: 26.7467, lr: 0.000750, 207.51s
2024-02-02 06:27:02,823 - INFO - epoch complete!
2024-02-02 06:27:02,823 - INFO - evaluating now!
2024-02-02 06:27:17,191 - INFO - Epoch [106/300] (71583) train_loss: 25.3446, val_loss: 25.9509, lr: 0.000746, 207.39s
2024-02-02 06:30:29,951 - INFO - epoch complete!
2024-02-02 06:30:29,952 - INFO - evaluating now!
2024-02-02 06:30:44,303 - INFO - Epoch [107/300] (72252) train_loss: 25.3329, val_loss: 25.7185, lr: 0.000742, 207.11s
2024-02-02 06:30:44,359 - INFO - Saved model at 107
2024-02-02 06:30:44,359 - INFO - Val loss decrease from 25.8478 to 25.7185, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch107.tar
2024-02-02 06:33:57,534 - INFO - epoch complete!
2024-02-02 06:33:57,535 - INFO - evaluating now!
2024-02-02 06:34:11,956 - INFO - Epoch [108/300] (72921) train_loss: 25.2765, val_loss: 25.6683, lr: 0.000737, 207.60s
2024-02-02 06:34:12,011 - INFO - Saved model at 108
2024-02-02 06:34:12,012 - INFO - Val loss decrease from 25.7185 to 25.6683, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch108.tar
2024-02-02 06:37:27,308 - INFO - epoch complete!
2024-02-02 06:37:27,309 - INFO - evaluating now!
2024-02-02 06:37:41,775 - INFO - Epoch [109/300] (73590) train_loss: 25.3874, val_loss: 25.9641, lr: 0.000733, 209.76s
2024-02-02 06:40:56,146 - INFO - epoch complete!
2024-02-02 06:40:56,147 - INFO - evaluating now!
2024-02-02 06:41:10,629 - INFO - Epoch [110/300] (74259) train_loss: 25.2192, val_loss: 26.5418, lr: 0.000729, 208.85s
2024-02-02 06:44:24,703 - INFO - epoch complete!
2024-02-02 06:44:24,704 - INFO - evaluating now!
2024-02-02 06:44:39,079 - INFO - Epoch [111/300] (74928) train_loss: 25.2000, val_loss: 26.7196, lr: 0.000724, 208.45s
2024-02-02 06:47:54,457 - INFO - epoch complete!
2024-02-02 06:47:54,457 - INFO - evaluating now!
2024-02-02 06:48:08,866 - INFO - Epoch [112/300] (75597) train_loss: 25.1984, val_loss: 26.6000, lr: 0.000720, 209.79s
2024-02-02 06:51:25,140 - INFO - epoch complete!
2024-02-02 06:51:25,141 - INFO - evaluating now!
2024-02-02 06:51:39,551 - INFO - Epoch [113/300] (76266) train_loss: 25.1970, val_loss: 25.7936, lr: 0.000716, 210.68s
2024-02-02 06:54:55,908 - INFO - epoch complete!
2024-02-02 06:54:55,909 - INFO - evaluating now!
2024-02-02 06:55:10,572 - INFO - Epoch [114/300] (76935) train_loss: 25.2323, val_loss: 25.7093, lr: 0.000711, 211.02s
2024-02-02 06:58:24,360 - INFO - epoch complete!
2024-02-02 06:58:24,360 - INFO - evaluating now!
2024-02-02 06:58:38,841 - INFO - Epoch [115/300] (77604) train_loss: 25.2094, val_loss: 25.6310, lr: 0.000707, 208.27s
2024-02-02 06:58:38,896 - INFO - Saved model at 115
2024-02-02 06:58:38,896 - INFO - Val loss decrease from 25.6683 to 25.6310, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch115.tar
2024-02-02 07:01:54,477 - INFO - epoch complete!
2024-02-02 07:01:54,478 - INFO - evaluating now!
2024-02-02 07:02:08,908 - INFO - Epoch [116/300] (78273) train_loss: 25.1559, val_loss: 26.1129, lr: 0.000702, 210.01s
2024-02-02 07:05:23,284 - INFO - epoch complete!
2024-02-02 07:05:23,285 - INFO - evaluating now!
2024-02-02 07:05:37,730 - INFO - Epoch [117/300] (78942) train_loss: 25.1199, val_loss: 25.6561, lr: 0.000698, 208.82s
2024-02-02 07:08:53,320 - INFO - epoch complete!
2024-02-02 07:08:53,321 - INFO - evaluating now!
2024-02-02 07:09:07,797 - INFO - Epoch [118/300] (79611) train_loss: 25.0797, val_loss: 25.6671, lr: 0.000694, 210.07s
2024-02-02 07:12:24,220 - INFO - epoch complete!
2024-02-02 07:12:24,221 - INFO - evaluating now!
2024-02-02 07:12:39,064 - INFO - Epoch [119/300] (80280) train_loss: 25.0418, val_loss: 25.5733, lr: 0.000689, 211.27s
2024-02-02 07:12:39,119 - INFO - Saved model at 119
2024-02-02 07:12:39,119 - INFO - Val loss decrease from 25.6310 to 25.5733, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch119.tar
2024-02-02 07:15:55,353 - INFO - epoch complete!
2024-02-02 07:15:55,354 - INFO - evaluating now!
2024-02-02 07:16:09,762 - INFO - Epoch [120/300] (80949) train_loss: 25.0355, val_loss: 26.6977, lr: 0.000685, 210.64s
2024-02-02 07:19:25,615 - INFO - epoch complete!
2024-02-02 07:19:25,615 - INFO - evaluating now!
2024-02-02 07:19:39,992 - INFO - Epoch [121/300] (81618) train_loss: 25.0318, val_loss: 26.4441, lr: 0.000680, 210.23s
2024-02-02 07:22:55,941 - INFO - epoch complete!
2024-02-02 07:22:55,941 - INFO - evaluating now!
2024-02-02 07:23:10,338 - INFO - Epoch [122/300] (82287) train_loss: 25.0296, val_loss: 25.9487, lr: 0.000676, 210.35s
2024-02-02 07:26:25,149 - INFO - epoch complete!
2024-02-02 07:26:25,150 - INFO - evaluating now!
2024-02-02 07:26:39,406 - INFO - Epoch [123/300] (82956) train_loss: 25.0248, val_loss: 26.0458, lr: 0.000671, 209.07s
2024-02-02 07:29:50,691 - INFO - epoch complete!
2024-02-02 07:29:50,692 - INFO - evaluating now!
2024-02-02 07:30:05,515 - INFO - Epoch [124/300] (83625) train_loss: 24.9572, val_loss: 26.4098, lr: 0.000666, 206.11s
2024-02-02 07:33:17,207 - INFO - epoch complete!
2024-02-02 07:33:17,207 - INFO - evaluating now!
2024-02-02 07:33:31,417 - INFO - Epoch [125/300] (84294) train_loss: 25.0417, val_loss: 25.5680, lr: 0.000662, 205.90s
2024-02-02 07:33:31,471 - INFO - Saved model at 125
2024-02-02 07:33:31,471 - INFO - Val loss decrease from 25.5733 to 25.5680, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch125.tar
2024-02-02 07:36:42,979 - INFO - epoch complete!
2024-02-02 07:36:42,980 - INFO - evaluating now!
2024-02-02 07:36:57,374 - INFO - Epoch [126/300] (84963) train_loss: 24.9423, val_loss: 25.7540, lr: 0.000657, 205.90s
2024-02-02 07:40:10,677 - INFO - epoch complete!
2024-02-02 07:40:10,677 - INFO - evaluating now!
2024-02-02 07:40:25,127 - INFO - Epoch [127/300] (85632) train_loss: 24.9748, val_loss: 26.6129, lr: 0.000653, 207.75s
2024-02-02 07:43:38,623 - INFO - epoch complete!
2024-02-02 07:43:38,623 - INFO - evaluating now!
2024-02-02 07:43:53,071 - INFO - Epoch [128/300] (86301) train_loss: 24.8995, val_loss: 26.2708, lr: 0.000648, 207.94s
2024-02-02 07:47:06,624 - INFO - epoch complete!
2024-02-02 07:47:06,624 - INFO - evaluating now!
2024-02-02 07:47:20,885 - INFO - Epoch [129/300] (86970) train_loss: 24.8840, val_loss: 25.6958, lr: 0.000644, 207.81s
2024-02-02 07:50:28,491 - INFO - epoch complete!
2024-02-02 07:50:28,492 - INFO - evaluating now!
2024-02-02 07:50:43,291 - INFO - Epoch [130/300] (87639) train_loss: 24.8595, val_loss: 25.5724, lr: 0.000639, 202.41s
2024-02-02 07:53:56,644 - INFO - epoch complete!
2024-02-02 07:53:56,644 - INFO - evaluating now!
2024-02-02 07:54:11,074 - INFO - Epoch [131/300] (88308) train_loss: 24.8944, val_loss: 25.8067, lr: 0.000634, 207.78s
2024-02-02 07:57:24,525 - INFO - epoch complete!
2024-02-02 07:57:24,526 - INFO - evaluating now!
2024-02-02 07:57:38,970 - INFO - Epoch [132/300] (88977) train_loss: 24.7914, val_loss: 25.5511, lr: 0.000630, 207.90s
2024-02-02 07:57:39,025 - INFO - Saved model at 132
2024-02-02 07:57:39,025 - INFO - Val loss decrease from 25.5680 to 25.5511, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch132.tar
2024-02-02 08:00:52,926 - INFO - epoch complete!
2024-02-02 08:00:52,926 - INFO - evaluating now!
2024-02-02 08:01:07,385 - INFO - Epoch [133/300] (89646) train_loss: 24.7938, val_loss: 25.7723, lr: 0.000625, 208.36s
2024-02-02 08:04:21,256 - INFO - epoch complete!
2024-02-02 08:04:21,256 - INFO - evaluating now!
2024-02-02 08:04:35,748 - INFO - Epoch [134/300] (90315) train_loss: 24.7808, val_loss: 25.4927, lr: 0.000620, 208.36s
2024-02-02 08:04:35,805 - INFO - Saved model at 134
2024-02-02 08:04:35,805 - INFO - Val loss decrease from 25.5511 to 25.4927, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch134.tar
2024-02-02 08:07:49,579 - INFO - epoch complete!
2024-02-02 08:07:49,580 - INFO - evaluating now!
2024-02-02 08:08:04,083 - INFO - Epoch [135/300] (90984) train_loss: 24.7679, val_loss: 25.6956, lr: 0.000616, 208.28s
2024-02-02 08:11:17,294 - INFO - epoch complete!
2024-02-02 08:11:17,294 - INFO - evaluating now!
2024-02-02 08:11:31,730 - INFO - Epoch [136/300] (91653) train_loss: 24.7677, val_loss: 26.4764, lr: 0.000611, 207.65s
2024-02-02 08:14:45,456 - INFO - epoch complete!
2024-02-02 08:14:45,456 - INFO - evaluating now!
2024-02-02 08:14:59,927 - INFO - Epoch [137/300] (92322) train_loss: 24.7274, val_loss: 25.6817, lr: 0.000606, 208.20s
2024-02-02 08:18:13,591 - INFO - epoch complete!
2024-02-02 08:18:13,592 - INFO - evaluating now!
2024-02-02 08:18:28,100 - INFO - Epoch [138/300] (92991) train_loss: 24.6598, val_loss: 25.9988, lr: 0.000602, 208.17s
2024-02-02 08:21:41,895 - INFO - epoch complete!
2024-02-02 08:21:41,895 - INFO - evaluating now!
2024-02-02 08:21:56,387 - INFO - Epoch [139/300] (93660) train_loss: 24.7063, val_loss: 25.6308, lr: 0.000597, 208.29s
2024-02-02 08:25:10,097 - INFO - epoch complete!
2024-02-02 08:25:10,098 - INFO - evaluating now!
2024-02-02 08:25:24,549 - INFO - Epoch [140/300] (94329) train_loss: 24.6981, val_loss: 25.9531, lr: 0.000592, 208.16s
2024-02-02 08:28:37,785 - INFO - epoch complete!
2024-02-02 08:28:37,786 - INFO - evaluating now!
2024-02-02 08:28:52,238 - INFO - Epoch [141/300] (94998) train_loss: 24.6715, val_loss: 25.6430, lr: 0.000588, 207.69s
2024-02-02 08:32:05,662 - INFO - epoch complete!
2024-02-02 08:32:05,663 - INFO - evaluating now!
2024-02-02 08:32:20,073 - INFO - Epoch [142/300] (95667) train_loss: 24.6113, val_loss: 25.7956, lr: 0.000583, 207.83s
2024-02-02 08:35:33,923 - INFO - epoch complete!
2024-02-02 08:35:33,923 - INFO - evaluating now!
2024-02-02 08:35:48,425 - INFO - Epoch [143/300] (96336) train_loss: 24.6419, val_loss: 25.7247, lr: 0.000578, 208.35s
2024-02-02 08:39:01,822 - INFO - epoch complete!
2024-02-02 08:39:01,823 - INFO - evaluating now!
2024-02-02 08:39:16,266 - INFO - Epoch [144/300] (97005) train_loss: 24.6205, val_loss: 25.6788, lr: 0.000574, 207.84s
2024-02-02 08:42:29,266 - INFO - epoch complete!
2024-02-02 08:42:29,267 - INFO - evaluating now!
2024-02-02 08:42:43,674 - INFO - Epoch [145/300] (97674) train_loss: 24.5653, val_loss: 25.7026, lr: 0.000569, 207.41s
2024-02-02 08:45:57,190 - INFO - epoch complete!
2024-02-02 08:45:57,190 - INFO - evaluating now!
2024-02-02 08:46:11,991 - INFO - Epoch [146/300] (98343) train_loss: 24.5583, val_loss: 25.6609, lr: 0.000564, 208.32s
2024-02-02 08:49:25,572 - INFO - epoch complete!
2024-02-02 08:49:25,572 - INFO - evaluating now!
2024-02-02 08:49:40,045 - INFO - Epoch [147/300] (99012) train_loss: 24.5037, val_loss: 25.7973, lr: 0.000559, 208.05s
2024-02-02 08:52:53,683 - INFO - epoch complete!
2024-02-02 08:52:53,683 - INFO - evaluating now!
2024-02-02 08:53:08,125 - INFO - Epoch [148/300] (99681) train_loss: 24.5595, val_loss: 25.6980, lr: 0.000555, 208.08s
2024-02-02 08:56:21,688 - INFO - epoch complete!
2024-02-02 08:56:21,688 - INFO - evaluating now!
2024-02-02 08:56:36,109 - INFO - Epoch [149/300] (100350) train_loss: 24.4969, val_loss: 25.4173, lr: 0.000550, 207.98s
2024-02-02 08:56:36,163 - INFO - Saved model at 149
2024-02-02 08:56:36,163 - INFO - Val loss decrease from 25.4927 to 25.4173, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch149.tar
2024-02-02 08:59:49,767 - INFO - epoch complete!
2024-02-02 08:59:49,767 - INFO - evaluating now!
2024-02-02 09:00:04,232 - INFO - Epoch [150/300] (101019) train_loss: 24.4391, val_loss: 25.6433, lr: 0.000545, 208.07s
2024-02-02 09:03:17,712 - INFO - epoch complete!
2024-02-02 09:03:17,713 - INFO - evaluating now!
2024-02-02 09:03:32,605 - INFO - Epoch [151/300] (101688) train_loss: 24.4321, val_loss: 25.6828, lr: 0.000541, 208.37s
2024-02-02 09:06:46,179 - INFO - epoch complete!
2024-02-02 09:06:46,180 - INFO - evaluating now!
2024-02-02 09:07:00,577 - INFO - Epoch [152/300] (102357) train_loss: 24.4041, val_loss: 25.5256, lr: 0.000536, 207.97s
2024-02-02 09:10:13,749 - INFO - epoch complete!
2024-02-02 09:10:13,750 - INFO - evaluating now!
2024-02-02 09:10:28,149 - INFO - Epoch [153/300] (103026) train_loss: 24.4151, val_loss: 25.5450, lr: 0.000531, 207.57s
2024-02-02 09:13:41,434 - INFO - epoch complete!
2024-02-02 09:13:41,435 - INFO - evaluating now!
2024-02-02 09:13:55,866 - INFO - Epoch [154/300] (103695) train_loss: 24.4113, val_loss: 25.5600, lr: 0.000526, 207.72s
2024-02-02 09:17:09,718 - INFO - epoch complete!
2024-02-02 09:17:09,719 - INFO - evaluating now!
2024-02-02 09:17:24,144 - INFO - Epoch [155/300] (104364) train_loss: 24.3595, val_loss: 25.5212, lr: 0.000522, 208.28s
2024-02-02 09:19:33,545 - INFO - epoch complete!
2024-02-02 09:19:33,546 - INFO - evaluating now!
2024-02-02 09:19:40,391 - INFO - Epoch [156/300] (105033) train_loss: 24.3490, val_loss: 25.6678, lr: 0.000517, 136.25s
2024-02-02 09:21:45,833 - INFO - epoch complete!
2024-02-02 09:21:45,834 - INFO - evaluating now!
2024-02-02 09:21:52,721 - INFO - Epoch [157/300] (105702) train_loss: 24.3847, val_loss: 25.3575, lr: 0.000512, 132.33s
2024-02-02 09:21:52,774 - INFO - Saved model at 157
2024-02-02 09:21:52,775 - INFO - Val loss decrease from 25.4173 to 25.3575, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch157.tar
2024-02-02 09:23:45,220 - INFO - epoch complete!
2024-02-02 09:23:45,221 - INFO - evaluating now!
2024-02-02 09:23:52,139 - INFO - Epoch [158/300] (106371) train_loss: 24.3418, val_loss: 25.7044, lr: 0.000508, 119.36s
2024-02-02 09:25:37,143 - INFO - epoch complete!
2024-02-02 09:25:37,143 - INFO - evaluating now!
2024-02-02 09:25:44,042 - INFO - Epoch [159/300] (107040) train_loss: 24.3230, val_loss: 25.2982, lr: 0.000503, 111.90s
2024-02-02 09:25:44,094 - INFO - Saved model at 159
2024-02-02 09:25:44,094 - INFO - Val loss decrease from 25.3575 to 25.2982, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch159.tar
2024-02-02 09:27:32,275 - INFO - epoch complete!
2024-02-02 09:27:32,276 - INFO - evaluating now!
2024-02-02 09:27:39,169 - INFO - Epoch [160/300] (107709) train_loss: 24.2842, val_loss: 25.5906, lr: 0.000498, 115.07s
2024-02-02 09:29:27,269 - INFO - epoch complete!
2024-02-02 09:29:27,269 - INFO - evaluating now!
2024-02-02 09:29:34,166 - INFO - Epoch [161/300] (108378) train_loss: 24.3013, val_loss: 25.6013, lr: 0.000494, 115.00s
2024-02-02 09:31:31,837 - INFO - epoch complete!
2024-02-02 09:31:31,838 - INFO - evaluating now!
2024-02-02 09:31:38,692 - INFO - Epoch [162/300] (109047) train_loss: 24.2226, val_loss: 25.5197, lr: 0.000489, 124.53s
2024-02-02 09:33:26,475 - INFO - epoch complete!
2024-02-02 09:33:26,476 - INFO - evaluating now!
2024-02-02 09:33:33,349 - INFO - Epoch [163/300] (109716) train_loss: 24.2533, val_loss: 26.0931, lr: 0.000484, 114.66s
2024-02-02 09:35:21,222 - INFO - epoch complete!
2024-02-02 09:35:21,223 - INFO - evaluating now!
2024-02-02 09:35:28,081 - INFO - Epoch [164/300] (110385) train_loss: 24.2262, val_loss: 25.6251, lr: 0.000480, 114.73s
2024-02-02 09:37:15,758 - INFO - epoch complete!
2024-02-02 09:37:15,758 - INFO - evaluating now!
2024-02-02 09:37:22,618 - INFO - Epoch [165/300] (111054) train_loss: 24.1810, val_loss: 25.4980, lr: 0.000475, 114.54s
2024-02-02 09:39:10,204 - INFO - epoch complete!
2024-02-02 09:39:10,205 - INFO - evaluating now!
2024-02-02 09:39:17,049 - INFO - Epoch [166/300] (111723) train_loss: 24.1745, val_loss: 25.6462, lr: 0.000470, 114.43s
2024-02-02 09:41:04,780 - INFO - epoch complete!
2024-02-02 09:41:04,780 - INFO - evaluating now!
2024-02-02 09:41:11,644 - INFO - Epoch [167/300] (112392) train_loss: 24.1388, val_loss: 25.5956, lr: 0.000466, 114.59s
2024-02-02 09:43:05,980 - INFO - epoch complete!
2024-02-02 09:43:05,981 - INFO - evaluating now!
2024-02-02 09:43:12,867 - INFO - Epoch [168/300] (113061) train_loss: 24.1360, val_loss: 25.6325, lr: 0.000461, 121.22s
2024-02-02 09:45:00,179 - INFO - epoch complete!
2024-02-02 09:45:00,179 - INFO - evaluating now!
2024-02-02 09:45:07,054 - INFO - Epoch [169/300] (113730) train_loss: 24.1301, val_loss: 25.9090, lr: 0.000456, 114.19s
2024-02-02 09:46:54,521 - INFO - epoch complete!
2024-02-02 09:46:54,522 - INFO - evaluating now!
2024-02-02 09:47:01,374 - INFO - Epoch [170/300] (114399) train_loss: 24.1840, val_loss: 25.7387, lr: 0.000452, 114.32s
2024-02-02 09:48:48,870 - INFO - epoch complete!
2024-02-02 09:48:48,870 - INFO - evaluating now!
2024-02-02 09:48:55,725 - INFO - Epoch [171/300] (115068) train_loss: 24.1006, val_loss: 26.0458, lr: 0.000447, 114.35s
2024-02-02 09:50:43,133 - INFO - epoch complete!
2024-02-02 09:50:43,134 - INFO - evaluating now!
2024-02-02 09:50:49,984 - INFO - Epoch [172/300] (115737) train_loss: 24.0777, val_loss: 25.4792, lr: 0.000443, 114.26s
2024-02-02 09:52:37,446 - INFO - epoch complete!
2024-02-02 09:52:37,447 - INFO - evaluating now!
2024-02-02 09:52:44,308 - INFO - Epoch [173/300] (116406) train_loss: 24.0391, val_loss: 25.2767, lr: 0.000438, 114.32s
2024-02-02 09:52:44,360 - INFO - Saved model at 173
2024-02-02 09:52:44,360 - INFO - Val loss decrease from 25.2982 to 25.2767, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch173.tar
2024-02-02 09:54:31,760 - INFO - epoch complete!
2024-02-02 09:54:31,761 - INFO - evaluating now!
2024-02-02 09:54:38,619 - INFO - Epoch [174/300] (117075) train_loss: 24.0300, val_loss: 25.3689, lr: 0.000434, 114.26s
2024-02-02 09:56:26,302 - INFO - epoch complete!
2024-02-02 09:56:26,302 - INFO - evaluating now!
2024-02-02 09:56:33,168 - INFO - Epoch [175/300] (117744) train_loss: 23.9454, val_loss: 25.6729, lr: 0.000429, 114.55s
2024-02-02 09:58:20,730 - INFO - epoch complete!
2024-02-02 09:58:20,731 - INFO - evaluating now!
2024-02-02 09:58:27,598 - INFO - Epoch [176/300] (118413) train_loss: 24.0051, val_loss: 25.5982, lr: 0.000424, 114.43s
2024-02-02 10:00:15,206 - INFO - epoch complete!
2024-02-02 10:00:15,207 - INFO - evaluating now!
2024-02-02 10:00:22,064 - INFO - Epoch [177/300] (119082) train_loss: 24.0006, val_loss: 25.4814, lr: 0.000420, 114.46s
2024-02-02 10:02:09,632 - INFO - epoch complete!
2024-02-02 10:02:09,633 - INFO - evaluating now!
2024-02-02 10:02:16,495 - INFO - Epoch [178/300] (119751) train_loss: 23.8909, val_loss: 25.8001, lr: 0.000415, 114.43s
2024-02-02 10:04:04,006 - INFO - epoch complete!
2024-02-02 10:04:04,007 - INFO - evaluating now!
2024-02-02 10:04:10,876 - INFO - Epoch [179/300] (120420) train_loss: 23.9625, val_loss: 25.8292, lr: 0.000411, 114.38s
2024-02-02 10:05:58,508 - INFO - epoch complete!
2024-02-02 10:05:58,508 - INFO - evaluating now!
2024-02-02 10:06:05,379 - INFO - Epoch [180/300] (121089) train_loss: 23.9048, val_loss: 25.6056, lr: 0.000406, 114.50s
2024-02-02 10:08:05,483 - INFO - epoch complete!
2024-02-02 10:08:05,484 - INFO - evaluating now!
2024-02-02 10:08:12,365 - INFO - Epoch [181/300] (121758) train_loss: 23.8577, val_loss: 25.5336, lr: 0.000402, 126.99s
2024-02-02 10:10:00,182 - INFO - epoch complete!
2024-02-02 10:10:00,183 - INFO - evaluating now!
2024-02-02 10:10:07,059 - INFO - Epoch [182/300] (122427) train_loss: 23.8445, val_loss: 25.4516, lr: 0.000398, 114.69s
2024-02-02 10:12:07,885 - INFO - epoch complete!
2024-02-02 10:12:07,886 - INFO - evaluating now!
2024-02-02 10:12:14,804 - INFO - Epoch [183/300] (123096) train_loss: 23.8528, val_loss: 25.3813, lr: 0.000393, 127.75s
2024-02-02 10:14:02,661 - INFO - epoch complete!
2024-02-02 10:14:02,662 - INFO - evaluating now!
2024-02-02 10:14:09,559 - INFO - Epoch [184/300] (123765) train_loss: 23.8141, val_loss: 25.4320, lr: 0.000389, 114.75s
2024-02-02 10:15:57,148 - INFO - epoch complete!
2024-02-02 10:15:57,149 - INFO - evaluating now!
2024-02-02 10:16:04,057 - INFO - Epoch [185/300] (124434) train_loss: 23.8177, val_loss: 25.3298, lr: 0.000384, 114.50s
2024-02-02 10:17:52,010 - INFO - epoch complete!
2024-02-02 10:17:52,011 - INFO - evaluating now!
2024-02-02 10:17:58,907 - INFO - Epoch [186/300] (125103) train_loss: 23.7645, val_loss: 25.7382, lr: 0.000380, 114.85s
2024-02-02 10:19:46,983 - INFO - epoch complete!
2024-02-02 10:19:46,984 - INFO - evaluating now!
2024-02-02 10:19:53,870 - INFO - Epoch [187/300] (125772) train_loss: 23.8009, val_loss: 25.4048, lr: 0.000376, 114.96s
2024-02-02 10:21:41,908 - INFO - epoch complete!
2024-02-02 10:21:41,908 - INFO - evaluating now!
2024-02-02 10:21:48,799 - INFO - Epoch [188/300] (126441) train_loss: 23.7549, val_loss: 25.4222, lr: 0.000371, 114.93s
2024-02-02 10:23:36,762 - INFO - epoch complete!
2024-02-02 10:23:36,762 - INFO - evaluating now!
2024-02-02 10:23:43,656 - INFO - Epoch [189/300] (127110) train_loss: 23.7110, val_loss: 25.3312, lr: 0.000367, 114.86s
2024-02-02 10:25:31,668 - INFO - epoch complete!
2024-02-02 10:25:31,669 - INFO - evaluating now!
2024-02-02 10:25:38,564 - INFO - Epoch [190/300] (127779) train_loss: 23.7318, val_loss: 25.9387, lr: 0.000363, 114.91s
2024-02-02 10:27:26,549 - INFO - epoch complete!
2024-02-02 10:27:26,549 - INFO - evaluating now!
2024-02-02 10:27:33,437 - INFO - Epoch [191/300] (128448) train_loss: 23.6844, val_loss: 25.2193, lr: 0.000358, 114.87s
2024-02-02 10:27:33,490 - INFO - Saved model at 191
2024-02-02 10:27:33,490 - INFO - Val loss decrease from 25.2767 to 25.2193, saving to ./libcity/cache/99976/model_cache/PDFormer_PeMS08_epoch191.tar
2024-02-02 10:29:21,502 - INFO - epoch complete!
2024-02-02 10:29:21,503 - INFO - evaluating now!
2024-02-02 10:29:28,400 - INFO - Epoch [192/300] (129117) train_loss: 23.6693, val_loss: 25.4688, lr: 0.000354, 114.91s
2024-02-02 10:31:16,428 - INFO - epoch complete!
2024-02-02 10:31:16,428 - INFO - evaluating now!
2024-02-02 10:31:23,326 - INFO - Epoch [193/300] (129786) train_loss: 23.6449, val_loss: 25.3046, lr: 0.000350, 114.93s
2024-02-02 10:33:11,356 - INFO - epoch complete!
2024-02-02 10:33:11,357 - INFO - evaluating now!
2024-02-02 10:33:18,255 - INFO - Epoch [194/300] (130455) train_loss: 23.6295, val_loss: 25.3344, lr: 0.000346, 114.93s
2024-02-02 10:35:06,387 - INFO - epoch complete!
2024-02-02 10:35:06,387 - INFO - evaluating now!
2024-02-02 10:35:13,278 - INFO - Epoch [195/300] (131124) train_loss: 23.6012, val_loss: 25.3898, lr: 0.000342, 115.02s
2024-02-02 10:36:57,017 - INFO - epoch complete!
2024-02-02 10:36:57,018 - INFO - evaluating now!
2024-02-02 10:37:03,927 - INFO - Epoch [196/300] (131793) train_loss: 23.6218, val_loss: 25.3382, lr: 0.000337, 110.65s
2024-02-02 10:38:55,736 - INFO - epoch complete!
2024-02-02 10:38:55,737 - INFO - evaluating now!
2024-02-02 10:39:02,612 - INFO - Epoch [197/300] (132462) train_loss: 23.5840, val_loss: 25.6801, lr: 0.000333, 118.68s
2024-02-02 10:40:50,287 - INFO - epoch complete!
2024-02-02 10:40:50,288 - INFO - evaluating now!
2024-02-02 10:40:57,134 - INFO - Epoch [198/300] (133131) train_loss: 23.5327, val_loss: 25.5008, lr: 0.000329, 114.52s
2024-02-02 10:42:44,862 - INFO - epoch complete!
2024-02-02 10:42:44,863 - INFO - evaluating now!
2024-02-02 10:42:51,707 - INFO - Epoch [199/300] (133800) train_loss: 23.5371, val_loss: 25.3094, lr: 0.000325, 114.57s
2024-02-02 10:44:52,683 - INFO - epoch complete!
2024-02-02 10:44:52,684 - INFO - evaluating now!
2024-02-02 10:44:59,560 - INFO - Epoch [200/300] (134469) train_loss: 23.5015, val_loss: 25.4895, lr: 0.000321, 127.85s
2024-02-02 10:46:46,982 - INFO - epoch complete!
2024-02-02 10:46:46,982 - INFO - evaluating now!
2024-02-02 10:46:53,843 - INFO - Epoch [201/300] (135138) train_loss: 23.4728, val_loss: 25.4911, lr: 0.000317, 114.28s
2024-02-02 10:48:41,370 - INFO - epoch complete!
2024-02-02 10:48:41,371 - INFO - evaluating now!
2024-02-02 10:48:48,222 - INFO - Epoch [202/300] (135807) train_loss: 23.4536, val_loss: 25.4437, lr: 0.000313, 114.38s
2024-02-02 10:50:52,974 - INFO - epoch complete!
2024-02-02 10:50:52,975 - INFO - evaluating now!
2024-02-02 10:51:07,276 - INFO - Epoch [203/300] (136476) train_loss: 23.4076, val_loss: 25.6344, lr: 0.000309, 139.05s
2024-02-02 10:54:21,170 - INFO - epoch complete!
2024-02-02 10:54:21,170 - INFO - evaluating now!
2024-02-02 10:54:35,619 - INFO - Epoch [204/300] (137145) train_loss: 23.3984, val_loss: 25.7394, lr: 0.000305, 208.34s
2024-02-02 10:57:49,576 - INFO - epoch complete!
2024-02-02 10:57:49,576 - INFO - evaluating now!
2024-02-02 10:58:03,782 - INFO - Epoch [205/300] (137814) train_loss: 23.3841, val_loss: 25.7442, lr: 0.000301, 208.16s
2024-02-02 11:01:10,003 - INFO - epoch complete!
2024-02-02 11:01:10,003 - INFO - evaluating now!
2024-02-02 11:01:23,784 - INFO - Epoch [206/300] (138483) train_loss: 23.3524, val_loss: 25.5512, lr: 0.000297, 200.00s
2024-02-02 11:04:30,948 - INFO - epoch complete!
2024-02-02 11:04:30,948 - INFO - evaluating now!
2024-02-02 11:04:45,391 - INFO - Epoch [207/300] (139152) train_loss: 23.3528, val_loss: 25.4548, lr: 0.000293, 201.61s
2024-02-02 11:07:59,293 - INFO - epoch complete!
2024-02-02 11:07:59,293 - INFO - evaluating now!
2024-02-02 11:08:13,730 - INFO - Epoch [208/300] (139821) train_loss: 23.3309, val_loss: 25.6477, lr: 0.000289, 208.34s
2024-02-02 11:11:27,617 - INFO - epoch complete!
2024-02-02 11:11:27,618 - INFO - evaluating now!
2024-02-02 11:11:42,039 - INFO - Epoch [209/300] (140490) train_loss: 23.2929, val_loss: 25.3494, lr: 0.000285, 208.31s
2024-02-02 11:14:56,010 - INFO - epoch complete!
2024-02-02 11:14:56,011 - INFO - evaluating now!
2024-02-02 11:15:10,408 - INFO - Epoch [210/300] (141159) train_loss: 23.2788, val_loss: 25.4632, lr: 0.000282, 208.37s
2024-02-02 11:18:19,571 - INFO - epoch complete!
2024-02-02 11:18:19,572 - INFO - evaluating now!
2024-02-02 11:18:33,604 - INFO - Epoch [211/300] (141828) train_loss: 23.2566, val_loss: 25.5707, lr: 0.000278, 203.20s
2024-02-02 11:21:46,636 - INFO - epoch complete!
2024-02-02 11:21:46,636 - INFO - evaluating now!
2024-02-02 11:22:00,811 - INFO - Epoch [212/300] (142497) train_loss: 23.2312, val_loss: 25.6350, lr: 0.000274, 207.21s
2024-02-02 11:25:14,925 - INFO - epoch complete!
2024-02-02 11:25:14,925 - INFO - evaluating now!
2024-02-02 11:25:29,435 - INFO - Epoch [213/300] (143166) train_loss: 23.2510, val_loss: 25.4569, lr: 0.000270, 208.62s
2024-02-02 11:28:43,790 - INFO - epoch complete!
2024-02-02 11:28:43,791 - INFO - evaluating now!
2024-02-02 11:28:58,124 - INFO - Epoch [214/300] (143835) train_loss: 23.2020, val_loss: 25.4860, lr: 0.000267, 208.69s
2024-02-02 11:32:12,587 - INFO - epoch complete!
2024-02-02 11:32:12,587 - INFO - evaluating now!
2024-02-02 11:32:26,316 - INFO - Epoch [215/300] (144504) train_loss: 23.1722, val_loss: 25.6221, lr: 0.000263, 208.19s
2024-02-02 11:35:38,677 - INFO - epoch complete!
2024-02-02 11:35:38,678 - INFO - evaluating now!
2024-02-02 11:35:52,865 - INFO - Epoch [216/300] (145173) train_loss: 23.1628, val_loss: 25.6013, lr: 0.000260, 206.55s
2024-02-02 11:39:06,708 - INFO - epoch complete!
2024-02-02 11:39:06,709 - INFO - evaluating now!
2024-02-02 11:39:20,362 - INFO - Epoch [217/300] (145842) train_loss: 23.1210, val_loss: 25.4344, lr: 0.000256, 207.50s
2024-02-02 11:42:34,895 - INFO - epoch complete!
2024-02-02 11:42:34,896 - INFO - evaluating now!
2024-02-02 11:42:48,587 - INFO - Epoch [218/300] (146511) train_loss: 23.0891, val_loss: 25.4895, lr: 0.000252, 208.22s
2024-02-02 11:46:02,740 - INFO - epoch complete!
2024-02-02 11:46:02,740 - INFO - evaluating now!
2024-02-02 11:46:16,548 - INFO - Epoch [219/300] (147180) train_loss: 23.0850, val_loss: 25.7153, lr: 0.000249, 207.96s
2024-02-02 11:49:31,715 - INFO - epoch complete!
2024-02-02 11:49:31,715 - INFO - evaluating now!
2024-02-02 11:49:46,025 - INFO - Epoch [220/300] (147849) train_loss: 23.0666, val_loss: 25.5433, lr: 0.000245, 209.48s
2024-02-02 11:52:58,998 - INFO - epoch complete!
2024-02-02 11:52:58,998 - INFO - evaluating now!
2024-02-02 11:53:13,951 - INFO - Epoch [221/300] (148518) train_loss: 23.0379, val_loss: 25.5463, lr: 0.000242, 207.92s
2024-02-02 11:56:27,538 - INFO - epoch complete!
2024-02-02 11:56:27,538 - INFO - evaluating now!
2024-02-02 11:56:41,788 - INFO - Epoch [222/300] (149187) train_loss: 23.0024, val_loss: 25.6234, lr: 0.000239, 207.84s
2024-02-02 11:59:58,992 - INFO - epoch complete!
2024-02-02 11:59:58,993 - INFO - evaluating now!
2024-02-02 12:00:13,168 - INFO - Epoch [223/300] (149856) train_loss: 22.9937, val_loss: 25.5944, lr: 0.000235, 211.38s
2024-02-02 12:03:24,468 - INFO - epoch complete!
2024-02-02 12:03:24,468 - INFO - evaluating now!
2024-02-02 12:03:38,472 - INFO - Epoch [224/300] (150525) train_loss: 22.9828, val_loss: 25.6559, lr: 0.000232, 205.30s
2024-02-02 12:06:50,714 - INFO - epoch complete!
2024-02-02 12:06:50,714 - INFO - evaluating now!
2024-02-02 12:07:04,446 - INFO - Epoch [225/300] (151194) train_loss: 22.9463, val_loss: 25.7320, lr: 0.000228, 205.97s
2024-02-02 12:10:17,326 - INFO - epoch complete!
2024-02-02 12:10:17,327 - INFO - evaluating now!
2024-02-02 12:10:31,325 - INFO - Epoch [226/300] (151863) train_loss: 22.9338, val_loss: 25.6153, lr: 0.000225, 206.88s
2024-02-02 12:13:44,711 - INFO - epoch complete!
2024-02-02 12:13:44,712 - INFO - evaluating now!
2024-02-02 12:13:59,552 - INFO - Epoch [227/300] (152532) train_loss: 22.8823, val_loss: 25.6010, lr: 0.000222, 208.23s
2024-02-02 12:17:12,857 - INFO - epoch complete!
2024-02-02 12:17:12,858 - INFO - evaluating now!
2024-02-02 12:17:26,817 - INFO - Epoch [228/300] (153201) train_loss: 22.9152, val_loss: 25.6560, lr: 0.000219, 207.26s
2024-02-02 12:20:41,162 - INFO - epoch complete!
2024-02-02 12:20:41,162 - INFO - evaluating now!
2024-02-02 12:20:54,930 - INFO - Epoch [229/300] (153870) train_loss: 22.8727, val_loss: 25.6451, lr: 0.000216, 208.11s
2024-02-02 12:24:10,166 - INFO - epoch complete!
2024-02-02 12:24:10,167 - INFO - evaluating now!
2024-02-02 12:24:24,200 - INFO - Epoch [230/300] (154539) train_loss: 22.8413, val_loss: 25.6892, lr: 0.000212, 209.27s
2024-02-02 12:27:37,630 - INFO - epoch complete!
2024-02-02 12:27:37,631 - INFO - evaluating now!
2024-02-02 12:27:51,443 - INFO - Epoch [231/300] (155208) train_loss: 22.8456, val_loss: 25.7229, lr: 0.000209, 207.24s
2024-02-02 12:31:05,041 - INFO - epoch complete!
2024-02-02 12:31:05,041 - INFO - evaluating now!
2024-02-02 12:31:19,027 - INFO - Epoch [232/300] (155877) train_loss: 22.8364, val_loss: 25.6646, lr: 0.000206, 207.58s
2024-02-02 12:34:33,588 - INFO - epoch complete!
2024-02-02 12:34:33,588 - INFO - evaluating now!
2024-02-02 12:34:47,306 - INFO - Epoch [233/300] (156546) train_loss: 22.7849, val_loss: 25.8187, lr: 0.000203, 208.28s
2024-02-02 12:37:58,900 - INFO - epoch complete!
2024-02-02 12:37:58,900 - INFO - evaluating now!
2024-02-02 12:38:12,628 - INFO - Epoch [234/300] (157215) train_loss: 22.7651, val_loss: 25.7405, lr: 0.000200, 205.32s
2024-02-02 12:41:28,175 - INFO - epoch complete!
2024-02-02 12:41:28,175 - INFO - evaluating now!
2024-02-02 12:41:42,811 - INFO - Epoch [235/300] (157884) train_loss: 22.7647, val_loss: 25.7526, lr: 0.000197, 210.18s
2024-02-02 12:44:54,889 - INFO - epoch complete!
2024-02-02 12:44:54,889 - INFO - evaluating now!
2024-02-02 12:45:08,542 - INFO - Epoch [236/300] (158553) train_loss: 22.7613, val_loss: 25.7596, lr: 0.000194, 205.73s
2024-02-02 12:48:21,038 - INFO - epoch complete!
2024-02-02 12:48:21,039 - INFO - evaluating now!
2024-02-02 12:48:36,039 - INFO - Epoch [237/300] (159222) train_loss: 22.7329, val_loss: 25.8503, lr: 0.000192, 207.50s
2024-02-02 12:51:50,593 - INFO - epoch complete!
2024-02-02 12:51:50,594 - INFO - evaluating now!
2024-02-02 12:52:04,742 - INFO - Epoch [238/300] (159891) train_loss: 22.6838, val_loss: 25.7761, lr: 0.000189, 208.70s
2024-02-02 12:55:18,824 - INFO - epoch complete!
2024-02-02 12:55:18,825 - INFO - evaluating now!
2024-02-02 12:55:32,680 - INFO - Epoch [239/300] (160560) train_loss: 22.6747, val_loss: 25.7556, lr: 0.000186, 207.94s
2024-02-02 12:58:47,126 - INFO - epoch complete!
2024-02-02 12:58:47,127 - INFO - evaluating now!
2024-02-02 12:59:00,777 - INFO - Epoch [240/300] (161229) train_loss: 22.6572, val_loss: 25.9317, lr: 0.000183, 208.10s
2024-02-02 13:02:13,575 - INFO - epoch complete!
2024-02-02 13:02:13,576 - INFO - evaluating now!
2024-02-02 13:02:28,147 - INFO - Epoch [241/300] (161898) train_loss: 22.6418, val_loss: 25.7911, lr: 0.000180, 207.37s
2024-02-02 13:02:28,147 - WARNING - Early stopping at epoch: 241
2024-02-02 13:02:28,147 - INFO - Trained totally 242 epochs, average train time is 176.448s, average eval time is 12.897s
2024-02-02 13:02:28,406 - INFO - Loaded model at 191
2024-02-02 13:02:28,407 - INFO - Saved model at ./libcity/cache/99976/model_cache/PDFormer_PeMS08.m
2024-02-02 13:02:28,465 - INFO - Start evaluating ...
2024-02-02 13:02:50,702 - INFO - Note that you select the average mode to evaluate!
2024-02-02 13:02:50,710 - INFO - Evaluate result is saved at ./libcity/cache/99976/evaluate_cache/2024_02_02_13_02_50_PDFormer_PeMS08_average.csv
2024-02-02 13:02:50,718 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   11.686678   inf  19.735121   11.702398     0.077867    19.639376
2   11.903101   inf  20.295126   11.919389     0.078881    20.195885
3   12.145579   inf  20.843710   12.162804     0.080321    20.748859
4   12.320778   inf  21.248514   12.338469     0.081341    21.155928
5   12.496044   inf  21.639194   12.514178     0.082579    21.548208
6   12.647837   inf  21.970369   12.666282     0.083574    21.878893
7   12.798103   inf  22.281250   12.816877     0.084597    22.191265
8   12.930132   inf  22.562752   12.949260     0.085497    22.473804
9   13.050033   inf  22.815405   13.069474     0.086316    22.726881
10  13.159817   inf  23.042812   13.179536     0.087074    22.954844
11  13.267318   inf  23.252241   13.287295     0.087821    23.164854
12  13.378004   inf  23.447636   13.398189     0.088664    23.360773
