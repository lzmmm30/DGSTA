2024-03-26 10:29:15,239 - INFO - Log directory: ./libcity/log
2024-03-26 10:29:15,239 - INFO - Begin pipeline, task=traffic_state_pred, model_name=PDFormer, dataset_name=PeMS03, exp_id=82263
2024-03-26 10:29:15,239 - INFO - {'task': 'traffic_state_pred', 'model': 'PDFormer', 'dataset': 'PeMS03', 'saved_model': True, 'train': True, 'local_rank': 0, 'initial_ckpt': None, 'dataset_class': 'PDFormerDataset', 'input_window': 12, 'output_window': 12, 'train_rate': 0.6, 'eval_rate': 0.2, 'batch_size': 16, 'add_time_in_day': True, 'add_day_in_week': True, 'step_size': 1964, 'max_epoch': 300, 'bidir': True, 'far_mask_delta': 7, 'geo_num_heads': 4, 'sem_num_heads': 2, 't_num_heads': 2, 'cluster_method': 'kshape', 'cand_key_days': 14, 'seed': 1, 'type_ln': 'pre', 'set_loss': 'huber', 'huber_delta': 2, 'mode': 'average', 'executor': 'PDFormerExecutor', 'evaluator': 'TrafficStateEvaluator', 'embed_dim': 64, 'skip_dim': 256, 'mlp_ratio': 4, 'qkv_bias': True, 'drop': 0, 'attn_drop': 0, 'drop_path': 0.3, 's_attn_size': 3, 't_attn_size': 1, 'enc_depth': 5, 'type_short_path': 'hop', 'scaler': 'standard', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'learner': 'adamw', 'learning_rate': 0.001, 'weight_decay': 0.05, 'lr_decay': True, 'lr_scheduler': 'cosinelr', 'lr_eta_min': 0.0001, 'lr_decay_ratio': 0.1, 'lr_warmup_epoch': 5, 'lr_warmup_init': 1e-06, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': True, 'patience': 50, 'task_level': 0, 'use_curriculum_learning': True, 'random_flip': True, 'quan_delta': 0.25, 'dtw_delta': 5, 'cache_dataset': True, 'num_workers': 0, 'pad_with_last_sample': True, 'lape_dim': 8, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0, 'steps': [5, 20, 40, 70], 'lr_T_max': 30, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'grad_accmu_steps': 1, 'metrics': ['MAE', 'MAPE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_RMSE'], 'save_modes': ['csv'], 'geo': {'including_types': ['Point'], 'Point': {}}, 'rel': {'including_types': ['geo'], 'geo': {'cost': 'num'}}, 'dyna': {'including_types': ['state'], 'state': {'entity_id': 'geo_id', 'traffic_flow': 'num', 'traffic_occupancy': 'num', 'traffic_speed': 'num'}}, 'data_col': ['traffic_flow'], 'weight_col': 'cost', 'data_files': ['PeMS03'], 'geo_file': 'PeMS03', 'rel_file': 'PeMS03', 'output_dim': 1, 'time_intervals': 300, 'init_weight_inf_or_zero': 'zero', 'set_weight_link_or_dist': 'link', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'distributed': False, 'device': device(type='cuda', index=0), 'exp_id': 82263}
2024-03-26 10:29:15,524 - INFO - Loaded file PeMS03.geo, num_nodes=358
2024-03-26 10:29:15,526 - INFO - set_weight_link_or_dist: link
2024-03-26 10:29:15,526 - INFO - init_weight_inf_or_zero: zero
2024-03-26 10:29:15,529 - INFO - Loaded file PeMS03.rel, shape=(358, 358)
2024-03-26 10:29:15,529 - INFO - Max adj_mx value = 1.0
2024-03-26 10:30:47,493 - INFO - Loading file PeMS03.dyna
2024-03-26 10:30:51,839 - INFO - Loaded file PeMS03.dyna, shape=(26208, 358, 1)
2024-03-26 10:30:51,904 - INFO - Load DTW matrix from ./libcity/cache/dataset_cache/dtw_PeMS03.npy
2024-03-26 10:30:51,905 - INFO - Loading ./libcity/cache/dataset_cache/pdformer_point_based_PeMS03_12_12_0.6_1_0.2_standard_16_True_True_True_True_traffic_flow.npz
2024-03-26 10:31:12,545 - INFO - train	x: (15711, 12, 358, 9), y: (15711, 12, 358, 9), ind: (15711,)
2024-03-26 10:31:12,545 - INFO - eval	x: (5237, 12, 358, 9), y: (5237, 12, 358, 9), ind: (5237,)
2024-03-26 10:31:12,545 - INFO - test	x: (5237, 12, 358, 9), y: (5237, 12, 358, 9), ind: (5237,)
2024-03-26 10:31:13,848 - INFO - StandardScaler mean: 181.37526799238148, std: 144.4083626200602
2024-03-26 10:31:13,848 - INFO - NoneScaler
2024-03-26 10:31:17,578 - INFO - Loaded file ./libcity/cache/dataset_cache/pattern_keys_kshape_PeMS03_14_3_16_5.npy
2024-03-26 10:31:17,581 - INFO - Use use_curriculum_learning!
2024-03-26 10:31:21,071 - INFO - Number of isolated points: 0
2024-03-26 10:31:21,102 - INFO - Number of isolated points: 0
2024-03-26 10:31:21,175 - INFO - PDFormer(
  (pattern_embeddings): ModuleList(
    (0): TokenEmbedding(
      (token_embed): Linear(in_features=3, out_features=64, bias=True)
      (norm): Identity()
    )
  )
  (enc_embed_layer): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (token_embed): Linear(in_features=1, out_features=64, bias=True)
      (norm): Identity()
    )
    (position_encoding): PositionalEncoding()
    (daytime_embedding): Embedding(1440, 64)
    (weekday_embedding): Embedding(7, 64)
    (spatial_embedding): LaplacianPE(
      (embedding_lap_pos_enc): Linear(in_features=8, out_features=64, bias=True)
    )
    (tempp_embedding): Linear(in_features=8, out_features=64, bias=True)
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder_blocks): ModuleList(
    (0): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (2): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (3): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (4): STEncoderBlock(
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (st_attn): STSelfAttention(
        (pattern_q_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_k_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (pattern_v_linears): ModuleList(
          (0): Linear(in_features=64, out_features=32, bias=True)
        )
        (geo_q_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_k_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_v_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (geo_attn_drop): Dropout(p=0, inplace=False)
        (sem_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (sem_attn_drop): Dropout(p=0, inplace=False)
        (t_q_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_k_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_v_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (t_attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=48, out_features=64, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
        (gconv): gcn(
          (nconv): nconv()
          (mlp): linear(
            (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (reshape1): Linear(in_features=64, out_features=32, bias=True)
        (reshape2): Linear(in_features=32, out_features=64, bias=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
  (skip_convs): ModuleList(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (end_conv1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
  (end_conv2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
)
2024-03-26 10:31:21,177 - INFO - pattern_embeddings.0.token_embed.weight	torch.Size([64, 3])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - pattern_embeddings.0.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.value_embedding.token_embed.weight	torch.Size([64, 1])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.value_embedding.token_embed.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.daytime_embedding.weight	torch.Size([1440, 64])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.weekday_embedding.weight	torch.Size([7, 64])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.weight	torch.Size([64, 8])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.spatial_embedding.embedding_lap_pos_enc.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.tempp_embedding.weight	torch.Size([64, 8])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - enc_embed_layer.tempp_embedding.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,177 - INFO - encoder_blocks.0.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,178 - INFO - encoder_blocks.0.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.0.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,179 - INFO - encoder_blocks.1.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,180 - INFO - encoder_blocks.1.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.1.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,181 - INFO - encoder_blocks.2.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-26 10:31:21,182 - INFO - encoder_blocks.2.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,183 - INFO - encoder_blocks.3.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.3.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.norm1.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.norm1.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.nodevec_p1	torch.Size([288, 40])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.nodevec_p2	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.nodevec_p3	torch.Size([358, 40])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.nodevec_pk	torch.Size([40, 40, 40])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.pattern_q_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,184 - INFO - encoder_blocks.4.st_attn.pattern_k_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.pattern_v_linears.0.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.geo_q_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.geo_q_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.geo_k_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.geo_k_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.geo_v_conv.weight	torch.Size([32, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.geo_v_conv.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.sem_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.sem_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.sem_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.sem_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.sem_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.sem_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.t_q_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.t_q_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.t_k_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.t_k_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.t_v_conv.weight	torch.Size([16, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.t_v_conv.bias	torch.Size([16])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.proj.weight	torch.Size([64, 48])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.proj.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,185 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.weight	torch.Size([32, 96, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.st_attn.gconv.mlp.mlp.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.st_attn.reshape1.weight	torch.Size([32, 64])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.st_attn.reshape1.bias	torch.Size([32])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.st_attn.reshape2.weight	torch.Size([64, 32])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.st_attn.reshape2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.norm2.weight	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.norm2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.mlp.fc1.weight	torch.Size([256, 64])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.mlp.fc1.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.mlp.fc2.weight	torch.Size([64, 256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - encoder_blocks.4.mlp.fc2.bias	torch.Size([64])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.0.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.0.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.1.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.1.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.2.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.2.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.3.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.3.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.4.weight	torch.Size([256, 64, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - skip_convs.4.bias	torch.Size([256])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - end_conv1.weight	torch.Size([12, 12, 1, 1])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - end_conv1.bias	torch.Size([12])	cuda:0	True
2024-03-26 10:31:21,186 - INFO - end_conv2.weight	torch.Size([1, 256, 1, 1])	cuda:0	True
2024-03-26 10:31:21,187 - INFO - end_conv2.bias	torch.Size([1])	cuda:0	True
2024-03-26 10:31:21,187 - INFO - Total parameter numbers: 1011037
2024-03-26 10:31:21,189 - INFO - You select `adamw` optimizer.
2024-03-26 10:31:21,189 - INFO - You select `cosinelr` lr_scheduler.
2024-03-26 10:31:21,189 - WARNING - Received none train loss func and will use the loss func defined in the model.module.
2024-03-26 10:31:21,192 - INFO - Number of isolated points: 0
2024-03-26 10:31:21,235 - INFO - Start training ...
2024-03-26 10:31:21,236 - INFO - num_batches:982
2024-03-26 10:31:21,323 - INFO - Training: task_level increase from 0 to 1
2024-03-26 10:31:21,324 - INFO - Current batches_seen is 0
2024-03-26 10:35:06,005 - INFO - epoch complete!
2024-03-26 10:35:06,006 - INFO - evaluating now!
2024-03-26 10:35:23,838 - INFO - Epoch [0/300] (982) train_loss: 239.5806, val_loss: 258.3429, lr: 0.000201, 242.60s
2024-03-26 10:35:23,884 - INFO - Saved model at 0
2024-03-26 10:35:23,884 - INFO - Val loss decrease from inf to 258.3429, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch0.tar
2024-03-26 10:39:09,790 - INFO - epoch complete!
2024-03-26 10:39:09,791 - INFO - evaluating now!
2024-03-26 10:39:27,669 - INFO - Epoch [1/300] (1964) train_loss: 58.2976, val_loss: 244.1254, lr: 0.000401, 243.78s
2024-03-26 10:39:27,711 - INFO - Saved model at 1
2024-03-26 10:39:27,711 - INFO - Val loss decrease from 258.3429 to 244.1254, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch1.tar
2024-03-26 10:39:27,761 - INFO - Training: task_level increase from 1 to 2
2024-03-26 10:39:27,761 - INFO - Current batches_seen is 1964
2024-03-26 10:43:17,829 - INFO - epoch complete!
2024-03-26 10:43:17,830 - INFO - evaluating now!
2024-03-26 10:43:35,673 - INFO - Epoch [2/300] (2946) train_loss: 40.8724, val_loss: 176.8255, lr: 0.000600, 247.96s
2024-03-26 10:43:35,717 - INFO - Saved model at 2
2024-03-26 10:43:35,718 - INFO - Val loss decrease from 244.1254 to 176.8255, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch2.tar
2024-03-26 10:47:32,516 - INFO - epoch complete!
2024-03-26 10:47:32,516 - INFO - evaluating now!
2024-03-26 10:47:50,356 - INFO - Epoch [3/300] (3928) train_loss: 33.9334, val_loss: 197.3568, lr: 0.000800, 254.64s
2024-03-26 10:47:50,406 - INFO - Training: task_level increase from 2 to 3
2024-03-26 10:47:50,406 - INFO - Current batches_seen is 3928
2024-03-26 10:52:45,747 - INFO - epoch complete!
2024-03-26 10:52:45,748 - INFO - evaluating now!
2024-03-26 10:53:29,636 - INFO - Epoch [4/300] (4910) train_loss: 32.7697, val_loss: 166.2174, lr: 0.000999, 339.28s
2024-03-26 10:53:29,682 - INFO - Saved model at 4
2024-03-26 10:53:29,683 - INFO - Val loss decrease from 176.8255 to 166.2174, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch4.tar
2024-03-26 11:02:07,080 - INFO - epoch complete!
2024-03-26 11:02:07,080 - INFO - evaluating now!
2024-03-26 11:02:51,191 - INFO - Epoch [5/300] (5892) train_loss: 30.4155, val_loss: 163.6459, lr: 0.000999, 561.51s
2024-03-26 11:02:51,235 - INFO - Saved model at 5
2024-03-26 11:02:51,235 - INFO - Val loss decrease from 166.2174 to 163.6459, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch5.tar
2024-03-26 11:02:51,374 - INFO - Training: task_level increase from 3 to 4
2024-03-26 11:02:51,374 - INFO - Current batches_seen is 5892
2024-03-26 11:11:16,812 - INFO - epoch complete!
2024-03-26 11:11:16,813 - INFO - evaluating now!
2024-03-26 11:12:01,068 - INFO - Epoch [6/300] (6874) train_loss: 31.3331, val_loss: 135.2170, lr: 0.000999, 549.83s
2024-03-26 11:12:01,114 - INFO - Saved model at 6
2024-03-26 11:12:01,114 - INFO - Val loss decrease from 163.6459 to 135.2170, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch6.tar
2024-03-26 11:20:41,047 - INFO - epoch complete!
2024-03-26 11:20:41,048 - INFO - evaluating now!
2024-03-26 11:21:25,191 - INFO - Epoch [7/300] (7856) train_loss: 29.7036, val_loss: 132.1049, lr: 0.000998, 564.08s
2024-03-26 11:21:25,236 - INFO - Saved model at 7
2024-03-26 11:21:25,237 - INFO - Val loss decrease from 135.2170 to 132.1049, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch7.tar
2024-03-26 11:21:25,368 - INFO - Training: task_level increase from 4 to 5
2024-03-26 11:21:25,368 - INFO - Current batches_seen is 7856
2024-03-26 11:29:52,146 - INFO - epoch complete!
2024-03-26 11:29:52,146 - INFO - evaluating now!
2024-03-26 11:30:36,275 - INFO - Epoch [8/300] (8838) train_loss: 30.5231, val_loss: 114.3775, lr: 0.000998, 551.04s
2024-03-26 11:30:36,321 - INFO - Saved model at 8
2024-03-26 11:30:36,321 - INFO - Val loss decrease from 132.1049 to 114.3775, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch8.tar
2024-03-26 11:39:16,503 - INFO - epoch complete!
2024-03-26 11:39:16,504 - INFO - evaluating now!
2024-03-26 11:40:00,712 - INFO - Epoch [9/300] (9820) train_loss: 29.3104, val_loss: 114.2811, lr: 0.000998, 564.39s
2024-03-26 11:40:00,758 - INFO - Saved model at 9
2024-03-26 11:40:00,758 - INFO - Val loss decrease from 114.3775 to 114.2811, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch9.tar
2024-03-26 11:40:00,893 - INFO - Training: task_level increase from 5 to 6
2024-03-26 11:40:00,893 - INFO - Current batches_seen is 9820
2024-03-26 11:48:42,556 - INFO - epoch complete!
2024-03-26 11:48:42,557 - INFO - evaluating now!
2024-03-26 11:49:26,687 - INFO - Epoch [10/300] (10802) train_loss: 29.8147, val_loss: 112.7339, lr: 0.000997, 565.93s
2024-03-26 11:49:26,733 - INFO - Saved model at 10
2024-03-26 11:49:26,734 - INFO - Val loss decrease from 114.2811 to 112.7339, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch10.tar
2024-03-26 11:56:26,555 - INFO - epoch complete!
2024-03-26 11:56:26,556 - INFO - evaluating now!
2024-03-26 11:57:10,572 - INFO - Epoch [11/300] (11784) train_loss: 29.1853, val_loss: 113.6940, lr: 0.000996, 463.84s
2024-03-26 11:57:10,698 - INFO - Training: task_level increase from 6 to 7
2024-03-26 11:57:10,698 - INFO - Current batches_seen is 11784
2024-03-26 12:05:41,407 - INFO - epoch complete!
2024-03-26 12:05:41,407 - INFO - evaluating now!
2024-03-26 12:06:25,675 - INFO - Epoch [12/300] (12766) train_loss: 30.0692, val_loss: 99.2310, lr: 0.000996, 555.10s
2024-03-26 12:06:25,721 - INFO - Saved model at 12
2024-03-26 12:06:25,722 - INFO - Val loss decrease from 112.7339 to 99.2310, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch12.tar
2024-03-26 12:15:05,882 - INFO - epoch complete!
2024-03-26 12:15:05,882 - INFO - evaluating now!
2024-03-26 12:15:49,987 - INFO - Epoch [13/300] (13748) train_loss: 29.0536, val_loss: 98.9217, lr: 0.000995, 564.27s
2024-03-26 12:15:50,034 - INFO - Saved model at 13
2024-03-26 12:15:50,034 - INFO - Val loss decrease from 99.2310 to 98.9217, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch13.tar
2024-03-26 12:15:50,166 - INFO - Training: task_level increase from 7 to 8
2024-03-26 12:15:50,167 - INFO - Current batches_seen is 13748
2024-03-26 12:24:29,352 - INFO - epoch complete!
2024-03-26 12:24:29,352 - INFO - evaluating now!
2024-03-26 12:25:13,383 - INFO - Epoch [14/300] (14730) train_loss: 29.6254, val_loss: 92.2925, lr: 0.000994, 563.35s
2024-03-26 12:25:13,429 - INFO - Saved model at 14
2024-03-26 12:25:13,429 - INFO - Val loss decrease from 98.9217 to 92.2925, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch14.tar
2024-03-26 12:30:06,689 - INFO - epoch complete!
2024-03-26 12:30:06,690 - INFO - evaluating now!
2024-03-26 12:30:24,601 - INFO - Epoch [15/300] (15712) train_loss: 28.9723, val_loss: 92.6499, lr: 0.000994, 311.17s
2024-03-26 12:30:24,653 - INFO - Training: task_level increase from 8 to 9
2024-03-26 12:30:24,653 - INFO - Current batches_seen is 15712
2024-03-26 12:34:09,735 - INFO - epoch complete!
2024-03-26 12:34:09,736 - INFO - evaluating now!
2024-03-26 12:34:27,635 - INFO - Epoch [16/300] (16694) train_loss: 29.8287, val_loss: 75.7614, lr: 0.000993, 243.03s
2024-03-26 12:34:27,677 - INFO - Saved model at 16
2024-03-26 12:34:27,677 - INFO - Val loss decrease from 92.2925 to 75.7614, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch16.tar
2024-03-26 12:42:39,630 - INFO - epoch complete!
2024-03-26 12:42:39,631 - INFO - evaluating now!
2024-03-26 12:43:23,785 - INFO - Epoch [17/300] (17676) train_loss: 28.9265, val_loss: 78.0077, lr: 0.000992, 536.11s
2024-03-26 12:43:23,906 - INFO - Training: task_level increase from 9 to 10
2024-03-26 12:43:23,906 - INFO - Current batches_seen is 17676
2024-03-26 12:52:02,817 - INFO - epoch complete!
2024-03-26 12:52:02,817 - INFO - evaluating now!
2024-03-26 12:52:46,753 - INFO - Epoch [18/300] (18658) train_loss: 29.5451, val_loss: 59.5360, lr: 0.000991, 562.97s
2024-03-26 12:52:46,799 - INFO - Saved model at 18
2024-03-26 12:52:46,799 - INFO - Val loss decrease from 75.7614 to 59.5360, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch18.tar
2024-03-26 13:01:27,218 - INFO - epoch complete!
2024-03-26 13:01:27,219 - INFO - evaluating now!
2024-03-26 13:02:11,292 - INFO - Epoch [19/300] (19640) train_loss: 28.6625, val_loss: 58.7447, lr: 0.000990, 564.49s
2024-03-26 13:02:11,338 - INFO - Saved model at 19
2024-03-26 13:02:11,338 - INFO - Val loss decrease from 59.5360 to 58.7447, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch19.tar
2024-03-26 13:02:11,473 - INFO - Training: task_level increase from 10 to 11
2024-03-26 13:02:11,473 - INFO - Current batches_seen is 19640
2024-03-26 13:10:50,911 - INFO - epoch complete!
2024-03-26 13:10:50,911 - INFO - evaluating now!
2024-03-26 13:11:35,081 - INFO - Epoch [20/300] (20622) train_loss: 28.9757, val_loss: 41.6228, lr: 0.000989, 563.74s
2024-03-26 13:11:35,126 - INFO - Saved model at 20
2024-03-26 13:11:35,127 - INFO - Val loss decrease from 58.7447 to 41.6228, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch20.tar
2024-03-26 13:16:38,386 - INFO - epoch complete!
2024-03-26 13:16:38,386 - INFO - evaluating now!
2024-03-26 13:16:56,273 - INFO - Epoch [21/300] (21604) train_loss: 28.7339, val_loss: 41.7189, lr: 0.000988, 321.15s
2024-03-26 13:16:56,323 - INFO - Training: task_level increase from 11 to 12
2024-03-26 13:16:56,323 - INFO - Current batches_seen is 21604
2024-03-26 13:20:41,158 - INFO - epoch complete!
2024-03-26 13:20:41,159 - INFO - evaluating now!
2024-03-26 13:20:59,037 - INFO - Epoch [22/300] (22586) train_loss: 29.1264, val_loss: 28.9807, lr: 0.000987, 242.76s
2024-03-26 13:20:59,079 - INFO - Saved model at 22
2024-03-26 13:20:59,079 - INFO - Val loss decrease from 41.6228 to 28.9807, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch22.tar
2024-03-26 13:24:52,667 - INFO - epoch complete!
2024-03-26 13:24:52,667 - INFO - evaluating now!
2024-03-26 13:25:10,556 - INFO - Epoch [23/300] (23568) train_loss: 28.6785, val_loss: 29.9899, lr: 0.000986, 251.48s
2024-03-26 13:28:56,272 - INFO - epoch complete!
2024-03-26 13:28:56,272 - INFO - evaluating now!
2024-03-26 13:29:14,187 - INFO - Epoch [24/300] (24550) train_loss: 28.3605, val_loss: 27.8293, lr: 0.000985, 243.63s
2024-03-26 13:29:14,231 - INFO - Saved model at 24
2024-03-26 13:29:14,231 - INFO - Val loss decrease from 28.9807 to 27.8293, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch24.tar
2024-03-26 13:32:59,124 - INFO - epoch complete!
2024-03-26 13:32:59,125 - INFO - evaluating now!
2024-03-26 13:33:17,040 - INFO - Epoch [25/300] (25532) train_loss: 28.1607, val_loss: 27.5736, lr: 0.000983, 242.81s
2024-03-26 13:33:17,085 - INFO - Saved model at 25
2024-03-26 13:33:17,085 - INFO - Val loss decrease from 27.8293 to 27.5736, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch25.tar
2024-03-26 13:37:02,272 - INFO - epoch complete!
2024-03-26 13:37:02,273 - INFO - evaluating now!
2024-03-26 13:37:20,177 - INFO - Epoch [26/300] (26514) train_loss: 28.0528, val_loss: 27.9580, lr: 0.000982, 243.09s
2024-03-26 13:41:05,963 - INFO - epoch complete!
2024-03-26 13:41:05,964 - INFO - evaluating now!
2024-03-26 13:41:23,861 - INFO - Epoch [27/300] (27496) train_loss: 27.8558, val_loss: 27.6460, lr: 0.000981, 243.68s
2024-03-26 13:47:56,254 - INFO - epoch complete!
2024-03-26 13:47:56,254 - INFO - evaluating now!
2024-03-26 13:48:40,271 - INFO - Epoch [28/300] (28478) train_loss: 27.7347, val_loss: 27.6289, lr: 0.000979, 436.41s
2024-03-26 13:57:20,190 - INFO - epoch complete!
2024-03-26 13:57:20,191 - INFO - evaluating now!
2024-03-26 13:58:04,408 - INFO - Epoch [29/300] (29460) train_loss: 27.5469, val_loss: 29.0376, lr: 0.000978, 564.14s
2024-03-26 14:06:44,539 - INFO - epoch complete!
2024-03-26 14:06:44,540 - INFO - evaluating now!
2024-03-26 14:07:28,565 - INFO - Epoch [30/300] (30442) train_loss: 27.5220, val_loss: 27.0987, lr: 0.000976, 564.16s
2024-03-26 14:07:28,610 - INFO - Saved model at 30
2024-03-26 14:07:28,610 - INFO - Val loss decrease from 27.5736 to 27.0987, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch30.tar
2024-03-26 14:16:09,295 - INFO - epoch complete!
2024-03-26 14:16:09,296 - INFO - evaluating now!
2024-03-26 14:16:53,486 - INFO - Epoch [31/300] (31424) train_loss: 27.3565, val_loss: 28.6794, lr: 0.000975, 564.88s
2024-03-26 14:25:30,855 - INFO - epoch complete!
2024-03-26 14:25:30,855 - INFO - evaluating now!
2024-03-26 14:26:10,667 - INFO - Epoch [32/300] (32406) train_loss: 27.2816, val_loss: 26.8297, lr: 0.000973, 557.18s
2024-03-26 14:26:10,711 - INFO - Saved model at 32
2024-03-26 14:26:10,711 - INFO - Val loss decrease from 27.0987 to 26.8297, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch32.tar
2024-03-26 14:34:52,481 - INFO - epoch complete!
2024-03-26 14:34:52,481 - INFO - evaluating now!
2024-03-26 14:35:36,787 - INFO - Epoch [33/300] (33388) train_loss: 27.1431, val_loss: 27.0766, lr: 0.000972, 566.08s
2024-03-26 14:44:12,593 - INFO - epoch complete!
2024-03-26 14:44:12,593 - INFO - evaluating now!
2024-03-26 14:44:56,630 - INFO - Epoch [34/300] (34370) train_loss: 27.0199, val_loss: 27.3035, lr: 0.000970, 559.84s
2024-03-26 14:53:38,784 - INFO - epoch complete!
2024-03-26 14:53:38,784 - INFO - evaluating now!
2024-03-26 14:54:22,614 - INFO - Epoch [35/300] (35352) train_loss: 27.1523, val_loss: 27.6937, lr: 0.000968, 565.98s
2024-03-26 15:02:42,628 - INFO - epoch complete!
2024-03-26 15:02:42,629 - INFO - evaluating now!
2024-03-26 15:03:26,893 - INFO - Epoch [36/300] (36334) train_loss: 26.9642, val_loss: 27.1160, lr: 0.000967, 544.28s
2024-03-26 15:08:51,788 - INFO - epoch complete!
2024-03-26 15:08:51,789 - INFO - evaluating now!
2024-03-26 15:09:09,679 - INFO - Epoch [37/300] (37316) train_loss: 26.8772, val_loss: 27.0467, lr: 0.000965, 342.79s
2024-03-26 15:12:55,603 - INFO - epoch complete!
2024-03-26 15:12:55,604 - INFO - evaluating now!
2024-03-26 15:13:13,469 - INFO - Epoch [38/300] (38298) train_loss: 26.8071, val_loss: 27.1938, lr: 0.000963, 243.79s
2024-03-26 15:17:02,637 - INFO - epoch complete!
2024-03-26 15:17:02,638 - INFO - evaluating now!
2024-03-26 15:17:20,504 - INFO - Epoch [39/300] (39280) train_loss: 26.8731, val_loss: 27.0253, lr: 0.000961, 247.03s
2024-03-26 15:21:04,858 - INFO - epoch complete!
2024-03-26 15:21:04,859 - INFO - evaluating now!
2024-03-26 15:21:22,679 - INFO - Epoch [40/300] (40262) train_loss: 26.6652, val_loss: 27.6870, lr: 0.000959, 242.17s
2024-03-26 15:25:07,233 - INFO - epoch complete!
2024-03-26 15:25:07,234 - INFO - evaluating now!
2024-03-26 15:25:25,049 - INFO - Epoch [41/300] (41244) train_loss: 26.7379, val_loss: 27.8878, lr: 0.000957, 242.37s
2024-03-26 15:29:10,178 - INFO - epoch complete!
2024-03-26 15:29:10,178 - INFO - evaluating now!
2024-03-26 15:29:28,023 - INFO - Epoch [42/300] (42226) train_loss: 26.7017, val_loss: 26.6886, lr: 0.000955, 242.97s
2024-03-26 15:29:28,066 - INFO - Saved model at 42
2024-03-26 15:29:28,066 - INFO - Val loss decrease from 26.8297 to 26.6886, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch42.tar
2024-03-26 15:33:19,045 - INFO - epoch complete!
2024-03-26 15:33:19,046 - INFO - evaluating now!
2024-03-26 15:33:36,919 - INFO - Epoch [43/300] (43208) train_loss: 26.6759, val_loss: 26.9951, lr: 0.000953, 248.85s
2024-03-26 15:37:21,754 - INFO - epoch complete!
2024-03-26 15:37:21,754 - INFO - evaluating now!
2024-03-26 15:37:39,613 - INFO - Epoch [44/300] (44190) train_loss: 26.5127, val_loss: 27.3549, lr: 0.000951, 242.69s
2024-03-26 15:41:24,210 - INFO - epoch complete!
2024-03-26 15:41:24,210 - INFO - evaluating now!
2024-03-26 15:41:42,081 - INFO - Epoch [45/300] (45172) train_loss: 26.5415, val_loss: 26.8454, lr: 0.000949, 242.47s
2024-03-26 15:45:26,690 - INFO - epoch complete!
2024-03-26 15:45:26,690 - INFO - evaluating now!
2024-03-26 15:45:44,536 - INFO - Epoch [46/300] (46154) train_loss: 26.4602, val_loss: 27.0169, lr: 0.000947, 242.45s
2024-03-26 15:49:28,822 - INFO - epoch complete!
2024-03-26 15:49:28,823 - INFO - evaluating now!
2024-03-26 15:49:46,661 - INFO - Epoch [47/300] (47136) train_loss: 26.3499, val_loss: 26.8573, lr: 0.000944, 242.12s
2024-03-26 15:53:30,799 - INFO - epoch complete!
2024-03-26 15:53:30,799 - INFO - evaluating now!
2024-03-26 15:53:48,629 - INFO - Epoch [48/300] (48118) train_loss: 26.5131, val_loss: 27.2397, lr: 0.000942, 241.97s
2024-03-26 15:57:32,877 - INFO - epoch complete!
2024-03-26 15:57:32,877 - INFO - evaluating now!
2024-03-26 15:57:50,675 - INFO - Epoch [49/300] (49100) train_loss: 26.4460, val_loss: 26.8280, lr: 0.000940, 242.04s
2024-03-26 16:01:34,600 - INFO - epoch complete!
2024-03-26 16:01:34,601 - INFO - evaluating now!
2024-03-26 16:01:52,396 - INFO - Epoch [50/300] (50082) train_loss: 26.3000, val_loss: 26.6200, lr: 0.000937, 241.72s
2024-03-26 16:01:52,439 - INFO - Saved model at 50
2024-03-26 16:01:52,439 - INFO - Val loss decrease from 26.6886 to 26.6200, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch50.tar
2024-03-26 16:05:43,467 - INFO - epoch complete!
2024-03-26 16:05:43,468 - INFO - evaluating now!
2024-03-26 16:06:01,268 - INFO - Epoch [51/300] (51064) train_loss: 26.2984, val_loss: 26.8381, lr: 0.000935, 248.83s
2024-03-26 16:09:45,060 - INFO - epoch complete!
2024-03-26 16:09:45,061 - INFO - evaluating now!
2024-03-26 16:10:02,845 - INFO - Epoch [52/300] (52046) train_loss: 26.2764, val_loss: 27.1990, lr: 0.000932, 241.58s
2024-03-26 16:13:52,063 - INFO - epoch complete!
2024-03-26 16:13:52,064 - INFO - evaluating now!
2024-03-26 16:14:09,851 - INFO - Epoch [53/300] (53028) train_loss: 26.2383, val_loss: 26.4217, lr: 0.000930, 247.01s
2024-03-26 16:14:09,892 - INFO - Saved model at 53
2024-03-26 16:14:09,892 - INFO - Val loss decrease from 26.6200 to 26.4217, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch53.tar
2024-03-26 16:17:54,175 - INFO - epoch complete!
2024-03-26 16:17:54,176 - INFO - evaluating now!
2024-03-26 16:18:11,940 - INFO - Epoch [54/300] (54010) train_loss: 26.1916, val_loss: 26.8416, lr: 0.000927, 242.05s
2024-03-26 16:21:55,826 - INFO - epoch complete!
2024-03-26 16:21:55,827 - INFO - evaluating now!
2024-03-26 16:22:13,582 - INFO - Epoch [55/300] (54992) train_loss: 26.0855, val_loss: 27.3522, lr: 0.000925, 241.64s
2024-03-26 16:26:05,531 - INFO - epoch complete!
2024-03-26 16:26:05,532 - INFO - evaluating now!
2024-03-26 16:26:23,355 - INFO - Epoch [56/300] (55974) train_loss: 26.1091, val_loss: 26.6947, lr: 0.000922, 249.77s
2024-03-26 16:30:11,315 - INFO - epoch complete!
2024-03-26 16:30:11,316 - INFO - evaluating now!
2024-03-26 16:30:29,134 - INFO - Epoch [57/300] (56956) train_loss: 26.0826, val_loss: 26.9844, lr: 0.000920, 245.78s
2024-03-26 16:34:27,741 - INFO - epoch complete!
2024-03-26 16:34:27,742 - INFO - evaluating now!
2024-03-26 16:34:45,515 - INFO - Epoch [58/300] (57938) train_loss: 26.0492, val_loss: 26.5095, lr: 0.000917, 256.38s
2024-03-26 16:38:32,988 - INFO - epoch complete!
2024-03-26 16:38:32,989 - INFO - evaluating now!
2024-03-26 16:38:50,769 - INFO - Epoch [59/300] (58920) train_loss: 26.0152, val_loss: 26.2523, lr: 0.000914, 245.25s
2024-03-26 16:38:50,813 - INFO - Saved model at 59
2024-03-26 16:38:50,813 - INFO - Val loss decrease from 26.4217 to 26.2523, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch59.tar
2024-03-26 16:42:36,940 - INFO - epoch complete!
2024-03-26 16:42:36,940 - INFO - evaluating now!
2024-03-26 16:42:54,712 - INFO - Epoch [60/300] (59902) train_loss: 25.9943, val_loss: 26.4240, lr: 0.000911, 243.90s
2024-03-26 16:46:42,920 - INFO - epoch complete!
2024-03-26 16:46:42,920 - INFO - evaluating now!
2024-03-26 16:47:00,728 - INFO - Epoch [61/300] (60884) train_loss: 25.9493, val_loss: 27.0328, lr: 0.000908, 246.01s
2024-03-26 16:50:48,604 - INFO - epoch complete!
2024-03-26 16:50:48,605 - INFO - evaluating now!
2024-03-26 16:51:06,325 - INFO - Epoch [62/300] (61866) train_loss: 25.9111, val_loss: 26.3252, lr: 0.000906, 245.60s
2024-03-26 16:54:49,690 - INFO - epoch complete!
2024-03-26 16:54:49,691 - INFO - evaluating now!
2024-03-26 16:55:07,409 - INFO - Epoch [63/300] (62848) train_loss: 25.9481, val_loss: 26.5720, lr: 0.000903, 241.08s
2024-03-26 16:58:54,892 - INFO - epoch complete!
2024-03-26 16:58:54,893 - INFO - evaluating now!
2024-03-26 16:59:12,616 - INFO - Epoch [64/300] (63830) train_loss: 25.8709, val_loss: 26.1209, lr: 0.000900, 245.21s
2024-03-26 16:59:12,657 - INFO - Saved model at 64
2024-03-26 16:59:12,657 - INFO - Val loss decrease from 26.2523 to 26.1209, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch64.tar
2024-03-26 17:02:59,174 - INFO - epoch complete!
2024-03-26 17:02:59,174 - INFO - evaluating now!
2024-03-26 17:03:16,944 - INFO - Epoch [65/300] (64812) train_loss: 25.7981, val_loss: 26.1301, lr: 0.000897, 244.29s
2024-03-26 17:07:08,936 - INFO - epoch complete!
2024-03-26 17:07:08,937 - INFO - evaluating now!
2024-03-26 17:07:26,724 - INFO - Epoch [66/300] (65794) train_loss: 25.7476, val_loss: 26.0333, lr: 0.000894, 249.78s
2024-03-26 17:07:26,767 - INFO - Saved model at 66
2024-03-26 17:07:26,767 - INFO - Val loss decrease from 26.1209 to 26.0333, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch66.tar
2024-03-26 17:11:15,524 - INFO - epoch complete!
2024-03-26 17:11:15,525 - INFO - evaluating now!
2024-03-26 17:11:33,242 - INFO - Epoch [67/300] (66776) train_loss: 25.7621, val_loss: 26.3178, lr: 0.000891, 246.47s
2024-03-26 17:15:17,083 - INFO - epoch complete!
2024-03-26 17:15:17,083 - INFO - evaluating now!
2024-03-26 17:15:34,815 - INFO - Epoch [68/300] (67758) train_loss: 25.6122, val_loss: 26.5095, lr: 0.000888, 241.57s
2024-03-26 17:19:17,851 - INFO - epoch complete!
2024-03-26 17:19:17,852 - INFO - evaluating now!
2024-03-26 17:19:35,626 - INFO - Epoch [69/300] (68740) train_loss: 25.6782, val_loss: 26.4822, lr: 0.000884, 240.81s
2024-03-26 17:23:18,973 - INFO - epoch complete!
2024-03-26 17:23:18,974 - INFO - evaluating now!
2024-03-26 17:23:36,734 - INFO - Epoch [70/300] (69722) train_loss: 25.5995, val_loss: 26.0999, lr: 0.000881, 241.11s
2024-03-26 17:27:20,259 - INFO - epoch complete!
2024-03-26 17:27:20,259 - INFO - evaluating now!
2024-03-26 17:27:38,081 - INFO - Epoch [71/300] (70704) train_loss: 25.5680, val_loss: 26.2233, lr: 0.000878, 241.35s
2024-03-26 17:31:23,306 - INFO - epoch complete!
2024-03-26 17:31:23,307 - INFO - evaluating now!
2024-03-26 17:31:41,156 - INFO - Epoch [72/300] (71686) train_loss: 25.5334, val_loss: 26.3253, lr: 0.000875, 243.07s
2024-03-26 17:35:28,908 - INFO - epoch complete!
2024-03-26 17:35:28,909 - INFO - evaluating now!
2024-03-26 17:35:46,710 - INFO - Epoch [73/300] (72668) train_loss: 25.4947, val_loss: 26.3117, lr: 0.000872, 245.55s
2024-03-26 17:39:30,304 - INFO - epoch complete!
2024-03-26 17:39:30,305 - INFO - evaluating now!
2024-03-26 17:39:48,101 - INFO - Epoch [74/300] (73650) train_loss: 25.4271, val_loss: 26.2751, lr: 0.000868, 241.39s
2024-03-26 17:43:32,503 - INFO - epoch complete!
2024-03-26 17:43:32,504 - INFO - evaluating now!
2024-03-26 17:43:50,305 - INFO - Epoch [75/300] (74632) train_loss: 25.4536, val_loss: 26.4054, lr: 0.000865, 242.20s
2024-03-26 17:47:34,076 - INFO - epoch complete!
2024-03-26 17:47:34,076 - INFO - evaluating now!
2024-03-26 17:47:51,885 - INFO - Epoch [76/300] (75614) train_loss: 25.3870, val_loss: 26.3287, lr: 0.000861, 241.58s
2024-03-26 17:51:35,693 - INFO - epoch complete!
2024-03-26 17:51:35,694 - INFO - evaluating now!
2024-03-26 17:51:53,490 - INFO - Epoch [77/300] (76596) train_loss: 25.3494, val_loss: 26.3294, lr: 0.000858, 241.60s
2024-03-26 17:55:40,408 - INFO - epoch complete!
2024-03-26 17:55:40,408 - INFO - evaluating now!
2024-03-26 17:55:58,207 - INFO - Epoch [78/300] (77578) train_loss: 25.2285, val_loss: 26.3758, lr: 0.000855, 244.72s
2024-03-26 17:59:43,041 - INFO - epoch complete!
2024-03-26 17:59:43,042 - INFO - evaluating now!
2024-03-26 18:00:00,846 - INFO - Epoch [79/300] (78560) train_loss: 25.1824, val_loss: 25.8635, lr: 0.000851, 242.64s
2024-03-26 18:00:00,888 - INFO - Saved model at 79
2024-03-26 18:00:00,888 - INFO - Val loss decrease from 26.0333 to 25.8635, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch79.tar
2024-03-26 18:03:45,582 - INFO - epoch complete!
2024-03-26 18:03:45,582 - INFO - evaluating now!
2024-03-26 18:04:03,392 - INFO - Epoch [80/300] (79542) train_loss: 25.1329, val_loss: 25.9914, lr: 0.000848, 242.50s
2024-03-26 18:07:47,994 - INFO - epoch complete!
2024-03-26 18:07:47,995 - INFO - evaluating now!
2024-03-26 18:08:05,810 - INFO - Epoch [81/300] (80524) train_loss: 25.0876, val_loss: 25.7565, lr: 0.000844, 242.42s
2024-03-26 18:08:05,960 - INFO - Saved model at 81
2024-03-26 18:08:05,961 - INFO - Val loss decrease from 25.8635 to 25.7565, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch81.tar
2024-03-26 18:12:01,273 - INFO - epoch complete!
2024-03-26 18:12:01,273 - INFO - evaluating now!
2024-03-26 18:12:19,044 - INFO - Epoch [82/300] (81506) train_loss: 25.0126, val_loss: 25.9766, lr: 0.000840, 253.08s
2024-03-26 18:16:03,187 - INFO - epoch complete!
2024-03-26 18:16:03,188 - INFO - evaluating now!
2024-03-26 18:16:20,991 - INFO - Epoch [83/300] (82488) train_loss: 25.0421, val_loss: 25.7506, lr: 0.000837, 241.95s
2024-03-26 18:16:21,032 - INFO - Saved model at 83
2024-03-26 18:16:21,032 - INFO - Val loss decrease from 25.7565 to 25.7506, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch83.tar
2024-03-26 18:20:05,033 - INFO - epoch complete!
2024-03-26 18:20:05,033 - INFO - evaluating now!
2024-03-26 18:20:22,822 - INFO - Epoch [84/300] (83470) train_loss: 24.9782, val_loss: 25.8747, lr: 0.000833, 241.79s
2024-03-26 18:24:06,827 - INFO - epoch complete!
2024-03-26 18:24:06,828 - INFO - evaluating now!
2024-03-26 18:24:24,634 - INFO - Epoch [85/300] (84452) train_loss: 24.8884, val_loss: 25.8496, lr: 0.000830, 241.81s
2024-03-26 18:28:08,797 - INFO - epoch complete!
2024-03-26 18:28:08,798 - INFO - evaluating now!
2024-03-26 18:28:26,606 - INFO - Epoch [86/300] (85434) train_loss: 24.8508, val_loss: 26.3552, lr: 0.000826, 241.97s
2024-03-26 18:32:10,677 - INFO - epoch complete!
2024-03-26 18:32:10,678 - INFO - evaluating now!
2024-03-26 18:32:28,481 - INFO - Epoch [87/300] (86416) train_loss: 24.8023, val_loss: 26.5154, lr: 0.000822, 241.87s
2024-03-26 18:36:12,579 - INFO - epoch complete!
2024-03-26 18:36:12,580 - INFO - evaluating now!
2024-03-26 18:36:30,386 - INFO - Epoch [88/300] (87398) train_loss: 24.6855, val_loss: 25.9682, lr: 0.000818, 241.90s
2024-03-26 18:40:14,349 - INFO - epoch complete!
2024-03-26 18:40:14,349 - INFO - evaluating now!
2024-03-26 18:40:32,132 - INFO - Epoch [89/300] (88380) train_loss: 24.7033, val_loss: 25.7656, lr: 0.000815, 241.75s
2024-03-26 18:44:15,963 - INFO - epoch complete!
2024-03-26 18:44:15,964 - INFO - evaluating now!
2024-03-26 18:44:33,757 - INFO - Epoch [90/300] (89362) train_loss: 24.7050, val_loss: 25.6138, lr: 0.000811, 241.62s
2024-03-26 18:44:33,799 - INFO - Saved model at 90
2024-03-26 18:44:33,799 - INFO - Val loss decrease from 25.7506 to 25.6138, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch90.tar
2024-03-26 18:48:17,600 - INFO - epoch complete!
2024-03-26 18:48:17,600 - INFO - evaluating now!
2024-03-26 18:48:35,404 - INFO - Epoch [91/300] (90344) train_loss: 24.5942, val_loss: 26.2328, lr: 0.000807, 241.60s
2024-03-26 18:52:19,263 - INFO - epoch complete!
2024-03-26 18:52:19,263 - INFO - evaluating now!
2024-03-26 18:52:37,056 - INFO - Epoch [92/300] (91326) train_loss: 24.5275, val_loss: 25.8084, lr: 0.000803, 241.65s
2024-03-26 18:56:21,124 - INFO - epoch complete!
2024-03-26 18:56:21,125 - INFO - evaluating now!
2024-03-26 18:56:38,949 - INFO - Epoch [93/300] (92308) train_loss: 24.4962, val_loss: 25.6522, lr: 0.000799, 241.89s
2024-03-26 19:00:23,147 - INFO - epoch complete!
2024-03-26 19:00:23,148 - INFO - evaluating now!
2024-03-26 19:00:40,970 - INFO - Epoch [94/300] (93290) train_loss: 24.4331, val_loss: 26.1378, lr: 0.000795, 242.02s
2024-03-26 19:04:25,119 - INFO - epoch complete!
2024-03-26 19:04:25,120 - INFO - evaluating now!
2024-03-26 19:04:42,938 - INFO - Epoch [95/300] (94272) train_loss: 24.4101, val_loss: 25.6985, lr: 0.000791, 241.97s
2024-03-26 19:08:27,123 - INFO - epoch complete!
2024-03-26 19:08:27,124 - INFO - evaluating now!
2024-03-26 19:08:44,939 - INFO - Epoch [96/300] (95254) train_loss: 24.3256, val_loss: 25.7274, lr: 0.000787, 242.00s
2024-03-26 19:12:29,240 - INFO - epoch complete!
2024-03-26 19:12:29,240 - INFO - evaluating now!
2024-03-26 19:12:47,055 - INFO - Epoch [97/300] (96236) train_loss: 24.3192, val_loss: 26.1530, lr: 0.000783, 242.12s
2024-03-26 19:16:31,341 - INFO - epoch complete!
2024-03-26 19:16:31,341 - INFO - evaluating now!
2024-03-26 19:16:49,164 - INFO - Epoch [98/300] (97218) train_loss: 24.3009, val_loss: 25.8158, lr: 0.000779, 242.11s
2024-03-26 19:20:33,497 - INFO - epoch complete!
2024-03-26 19:20:33,498 - INFO - evaluating now!
2024-03-26 19:20:51,316 - INFO - Epoch [99/300] (98200) train_loss: 24.2289, val_loss: 25.5991, lr: 0.000775, 242.15s
2024-03-26 19:20:51,358 - INFO - Saved model at 99
2024-03-26 19:20:51,359 - INFO - Val loss decrease from 25.6138 to 25.5991, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch99.tar
2024-03-26 19:24:41,818 - INFO - epoch complete!
2024-03-26 19:24:41,819 - INFO - evaluating now!
2024-03-26 19:24:59,613 - INFO - Epoch [100/300] (99182) train_loss: 24.2365, val_loss: 25.9960, lr: 0.000771, 248.25s
2024-03-26 19:28:43,826 - INFO - epoch complete!
2024-03-26 19:28:43,827 - INFO - evaluating now!
2024-03-26 19:29:01,605 - INFO - Epoch [101/300] (100164) train_loss: 24.1573, val_loss: 25.5901, lr: 0.000767, 241.99s
2024-03-26 19:29:01,648 - INFO - Saved model at 101
2024-03-26 19:29:01,648 - INFO - Val loss decrease from 25.5991 to 25.5901, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch101.tar
2024-03-26 19:32:45,730 - INFO - epoch complete!
2024-03-26 19:32:45,730 - INFO - evaluating now!
2024-03-26 19:33:03,504 - INFO - Epoch [102/300] (101146) train_loss: 24.0621, val_loss: 25.6886, lr: 0.000763, 241.86s
2024-03-26 19:36:47,523 - INFO - epoch complete!
2024-03-26 19:36:47,524 - INFO - evaluating now!
2024-03-26 19:37:05,296 - INFO - Epoch [103/300] (102128) train_loss: 24.0843, val_loss: 25.6327, lr: 0.000758, 241.79s
2024-03-26 19:40:48,674 - INFO - epoch complete!
2024-03-26 19:40:48,675 - INFO - evaluating now!
2024-03-26 19:41:06,382 - INFO - Epoch [104/300] (103110) train_loss: 24.0248, val_loss: 25.7436, lr: 0.000754, 241.09s
2024-03-26 19:44:49,663 - INFO - epoch complete!
2024-03-26 19:44:49,663 - INFO - evaluating now!
2024-03-26 19:45:07,379 - INFO - Epoch [105/300] (104092) train_loss: 23.9572, val_loss: 25.7684, lr: 0.000750, 241.00s
2024-03-26 19:48:50,674 - INFO - epoch complete!
2024-03-26 19:48:50,675 - INFO - evaluating now!
2024-03-26 19:49:08,385 - INFO - Epoch [106/300] (105074) train_loss: 23.9121, val_loss: 25.7255, lr: 0.000746, 241.01s
2024-03-26 19:52:51,660 - INFO - epoch complete!
2024-03-26 19:52:51,661 - INFO - evaluating now!
2024-03-26 19:53:09,363 - INFO - Epoch [107/300] (106056) train_loss: 23.8008, val_loss: 25.4478, lr: 0.000742, 240.98s
2024-03-26 19:53:09,405 - INFO - Saved model at 107
2024-03-26 19:53:09,405 - INFO - Val loss decrease from 25.5901 to 25.4478, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch107.tar
2024-03-26 19:56:52,765 - INFO - epoch complete!
2024-03-26 19:56:52,766 - INFO - evaluating now!
2024-03-26 19:57:10,470 - INFO - Epoch [108/300] (107038) train_loss: 23.7379, val_loss: 25.5162, lr: 0.000737, 241.07s
2024-03-26 20:00:53,827 - INFO - epoch complete!
2024-03-26 20:00:53,828 - INFO - evaluating now!
2024-03-26 20:01:11,539 - INFO - Epoch [109/300] (108020) train_loss: 23.8376, val_loss: 25.6049, lr: 0.000733, 241.07s
2024-03-26 20:04:54,845 - INFO - epoch complete!
2024-03-26 20:04:54,846 - INFO - evaluating now!
2024-03-26 20:05:12,540 - INFO - Epoch [110/300] (109002) train_loss: 23.6938, val_loss: 25.7194, lr: 0.000729, 241.00s
2024-03-26 20:08:57,592 - INFO - epoch complete!
2024-03-26 20:08:57,593 - INFO - evaluating now!
2024-03-26 20:09:15,317 - INFO - Epoch [111/300] (109984) train_loss: 23.6830, val_loss: 25.5260, lr: 0.000724, 242.78s
2024-03-26 20:12:58,482 - INFO - epoch complete!
2024-03-26 20:12:58,482 - INFO - evaluating now!
2024-03-26 20:13:16,206 - INFO - Epoch [112/300] (110966) train_loss: 23.6609, val_loss: 25.7236, lr: 0.000720, 240.89s
2024-03-26 20:16:59,409 - INFO - epoch complete!
2024-03-26 20:16:59,409 - INFO - evaluating now!
2024-03-26 20:17:17,139 - INFO - Epoch [113/300] (111948) train_loss: 23.5573, val_loss: 25.4346, lr: 0.000716, 240.93s
2024-03-26 20:17:17,180 - INFO - Saved model at 113
2024-03-26 20:17:17,181 - INFO - Val loss decrease from 25.4478 to 25.4346, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch113.tar
2024-03-26 20:21:00,453 - INFO - epoch complete!
2024-03-26 20:21:00,453 - INFO - evaluating now!
2024-03-26 20:21:18,175 - INFO - Epoch [114/300] (112930) train_loss: 23.6200, val_loss: 25.6852, lr: 0.000711, 240.99s
2024-03-26 20:25:01,354 - INFO - epoch complete!
2024-03-26 20:25:01,355 - INFO - evaluating now!
2024-03-26 20:25:19,074 - INFO - Epoch [115/300] (113912) train_loss: 23.4999, val_loss: 25.4244, lr: 0.000707, 240.90s
2024-03-26 20:25:19,115 - INFO - Saved model at 115
2024-03-26 20:25:19,116 - INFO - Val loss decrease from 25.4346 to 25.4244, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch115.tar
2024-03-26 20:29:02,130 - INFO - epoch complete!
2024-03-26 20:29:02,131 - INFO - evaluating now!
2024-03-26 20:29:19,845 - INFO - Epoch [116/300] (114894) train_loss: 23.4174, val_loss: 25.4046, lr: 0.000702, 240.73s
2024-03-26 20:29:19,887 - INFO - Saved model at 116
2024-03-26 20:29:19,887 - INFO - Val loss decrease from 25.4244 to 25.4046, saving to ./libcity/cache/82263/model_cache/PDFormer_PeMS03_epoch116.tar
2024-03-26 20:33:10,260 - INFO - epoch complete!
2024-03-26 20:33:10,261 - INFO - evaluating now!
2024-03-26 20:33:27,961 - INFO - Epoch [117/300] (115876) train_loss: 23.4411, val_loss: 25.7027, lr: 0.000698, 248.07s
2024-03-26 20:37:10,938 - INFO - epoch complete!
2024-03-26 20:37:10,939 - INFO - evaluating now!
2024-03-26 20:37:28,654 - INFO - Epoch [118/300] (116858) train_loss: 23.3334, val_loss: 25.7975, lr: 0.000694, 240.69s
2024-03-26 20:41:12,944 - INFO - epoch complete!
2024-03-26 20:41:12,944 - INFO - evaluating now!
2024-03-26 20:41:30,706 - INFO - Epoch [119/300] (117840) train_loss: 23.3178, val_loss: 25.8513, lr: 0.000689, 242.05s
2024-03-26 20:45:14,699 - INFO - epoch complete!
2024-03-26 20:45:14,700 - INFO - evaluating now!
2024-03-26 20:45:32,437 - INFO - Epoch [120/300] (118822) train_loss: 23.3272, val_loss: 25.7449, lr: 0.000685, 241.73s
2024-03-26 20:49:16,270 - INFO - epoch complete!
2024-03-26 20:49:16,271 - INFO - evaluating now!
2024-03-26 20:49:34,010 - INFO - Epoch [121/300] (119804) train_loss: 23.2410, val_loss: 25.8692, lr: 0.000680, 241.57s
2024-03-26 20:53:19,478 - INFO - epoch complete!
2024-03-26 20:53:19,479 - INFO - evaluating now!
2024-03-26 20:53:37,203 - INFO - Epoch [122/300] (120786) train_loss: 23.2150, val_loss: 25.4396, lr: 0.000676, 243.19s
2024-03-26 20:57:21,190 - INFO - epoch complete!
2024-03-26 20:57:21,190 - INFO - evaluating now!
2024-03-26 20:57:38,921 - INFO - Epoch [123/300] (121768) train_loss: 23.1813, val_loss: 26.3273, lr: 0.000671, 241.72s
2024-03-26 21:01:28,622 - INFO - epoch complete!
2024-03-26 21:01:28,623 - INFO - evaluating now!
2024-03-26 21:01:46,484 - INFO - Epoch [124/300] (122750) train_loss: 23.1559, val_loss: 25.6943, lr: 0.000666, 247.56s
2024-03-26 21:05:33,840 - INFO - epoch complete!
2024-03-26 21:05:33,840 - INFO - evaluating now!
2024-03-26 21:05:51,785 - INFO - Epoch [125/300] (123732) train_loss: 23.1159, val_loss: 25.6716, lr: 0.000662, 245.30s
2024-03-26 21:09:49,231 - INFO - epoch complete!
2024-03-26 21:09:49,231 - INFO - evaluating now!
2024-03-26 21:10:07,155 - INFO - Epoch [126/300] (124714) train_loss: 23.0490, val_loss: 26.3435, lr: 0.000657, 255.37s
2024-03-26 21:14:01,971 - INFO - epoch complete!
2024-03-26 21:14:01,972 - INFO - evaluating now!
2024-03-26 21:14:19,852 - INFO - Epoch [127/300] (125696) train_loss: 22.9959, val_loss: 26.0886, lr: 0.000653, 252.70s
2024-03-26 21:18:05,016 - INFO - epoch complete!
2024-03-26 21:18:05,017 - INFO - evaluating now!
2024-03-26 21:18:22,874 - INFO - Epoch [128/300] (126678) train_loss: 23.0091, val_loss: 25.7508, lr: 0.000648, 243.02s
2024-03-26 21:22:07,717 - INFO - epoch complete!
2024-03-26 21:22:07,718 - INFO - evaluating now!
2024-03-26 21:22:25,572 - INFO - Epoch [129/300] (127660) train_loss: 22.9567, val_loss: 25.7943, lr: 0.000644, 242.70s
2024-03-26 21:26:13,127 - INFO - epoch complete!
2024-03-26 21:26:13,127 - INFO - evaluating now!
2024-03-26 21:26:31,050 - INFO - Epoch [130/300] (128642) train_loss: 22.9796, val_loss: 25.6231, lr: 0.000639, 245.48s
2024-03-26 21:30:17,387 - INFO - epoch complete!
2024-03-26 21:30:17,387 - INFO - evaluating now!
2024-03-26 21:30:35,320 - INFO - Epoch [131/300] (129624) train_loss: 22.9158, val_loss: 26.1166, lr: 0.000634, 244.27s
2024-03-26 21:34:28,213 - INFO - epoch complete!
2024-03-26 21:34:28,214 - INFO - evaluating now!
2024-03-26 21:34:46,161 - INFO - Epoch [132/300] (130606) train_loss: 22.8579, val_loss: 25.9069, lr: 0.000630, 250.84s
2024-03-26 21:38:32,905 - INFO - epoch complete!
2024-03-26 21:38:32,905 - INFO - evaluating now!
2024-03-26 21:38:50,845 - INFO - Epoch [133/300] (131588) train_loss: 22.8099, val_loss: 25.7654, lr: 0.000625, 244.68s
2024-03-26 21:42:36,388 - INFO - epoch complete!
2024-03-26 21:42:36,389 - INFO - evaluating now!
2024-03-26 21:42:54,364 - INFO - Epoch [134/300] (132570) train_loss: 22.7895, val_loss: 25.5586, lr: 0.000620, 243.52s
2024-03-26 21:46:40,621 - INFO - epoch complete!
2024-03-26 21:46:40,622 - INFO - evaluating now!
2024-03-26 21:46:58,620 - INFO - Epoch [135/300] (133552) train_loss: 22.7962, val_loss: 26.1692, lr: 0.000616, 244.26s
2024-03-26 21:50:48,018 - INFO - epoch complete!
2024-03-26 21:50:48,018 - INFO - evaluating now!
2024-03-26 21:51:05,921 - INFO - Epoch [136/300] (134534) train_loss: 22.7079, val_loss: 25.9529, lr: 0.000611, 247.30s
2024-03-26 21:54:57,771 - INFO - epoch complete!
2024-03-26 21:54:57,772 - INFO - evaluating now!
2024-03-26 21:55:15,724 - INFO - Epoch [137/300] (135516) train_loss: 22.7185, val_loss: 26.1418, lr: 0.000606, 249.80s
2024-03-26 21:59:02,050 - INFO - epoch complete!
2024-03-26 21:59:02,051 - INFO - evaluating now!
2024-03-26 21:59:20,014 - INFO - Epoch [138/300] (136498) train_loss: 22.6777, val_loss: 25.7094, lr: 0.000602, 244.29s
2024-03-26 22:03:11,429 - INFO - epoch complete!
2024-03-26 22:03:11,430 - INFO - evaluating now!
2024-03-26 22:03:29,323 - INFO - Epoch [139/300] (137480) train_loss: 22.6825, val_loss: 25.6014, lr: 0.000597, 249.31s
2024-03-26 22:07:16,705 - INFO - epoch complete!
2024-03-26 22:07:16,705 - INFO - evaluating now!
2024-03-26 22:07:34,564 - INFO - Epoch [140/300] (138462) train_loss: 22.6867, val_loss: 26.5890, lr: 0.000592, 245.24s
2024-03-26 22:11:20,035 - INFO - epoch complete!
2024-03-26 22:11:20,035 - INFO - evaluating now!
2024-03-26 22:11:37,905 - INFO - Epoch [141/300] (139444) train_loss: 22.6633, val_loss: 25.6201, lr: 0.000588, 243.34s
2024-03-26 22:15:22,800 - INFO - epoch complete!
2024-03-26 22:15:22,800 - INFO - evaluating now!
2024-03-26 22:15:40,686 - INFO - Epoch [142/300] (140426) train_loss: 22.5949, val_loss: 25.6562, lr: 0.000583, 242.78s
2024-03-26 22:19:27,384 - INFO - epoch complete!
2024-03-26 22:19:27,384 - INFO - evaluating now!
2024-03-26 22:19:45,460 - INFO - Epoch [143/300] (141408) train_loss: 22.5600, val_loss: 25.8883, lr: 0.000578, 244.77s
2024-03-26 22:23:30,751 - INFO - epoch complete!
2024-03-26 22:23:30,751 - INFO - evaluating now!
2024-03-26 22:23:48,779 - INFO - Epoch [144/300] (142390) train_loss: 22.5223, val_loss: 25.6261, lr: 0.000574, 243.32s
2024-03-26 22:27:33,926 - INFO - epoch complete!
2024-03-26 22:27:33,926 - INFO - evaluating now!
2024-03-26 22:27:51,808 - INFO - Epoch [145/300] (143372) train_loss: 22.4857, val_loss: 25.8000, lr: 0.000569, 243.03s
2024-03-26 22:31:40,870 - INFO - epoch complete!
2024-03-26 22:31:40,871 - INFO - evaluating now!
2024-03-26 22:31:58,777 - INFO - Epoch [146/300] (144354) train_loss: 22.4620, val_loss: 25.6856, lr: 0.000564, 246.97s
2024-03-26 22:35:45,202 - INFO - epoch complete!
2024-03-26 22:35:45,202 - INFO - evaluating now!
2024-03-26 22:36:03,068 - INFO - Epoch [147/300] (145336) train_loss: 22.4580, val_loss: 25.8912, lr: 0.000559, 244.29s
2024-03-26 22:39:51,928 - INFO - epoch complete!
2024-03-26 22:39:51,929 - INFO - evaluating now!
2024-03-26 22:40:09,905 - INFO - Epoch [148/300] (146318) train_loss: 22.4252, val_loss: 25.9458, lr: 0.000555, 246.84s
2024-03-26 22:44:06,994 - INFO - epoch complete!
2024-03-26 22:44:06,994 - INFO - evaluating now!
2024-03-26 22:44:25,143 - INFO - Epoch [149/300] (147300) train_loss: 22.3826, val_loss: 25.9926, lr: 0.000550, 255.24s
2024-03-26 22:48:18,229 - INFO - epoch complete!
2024-03-26 22:48:18,230 - INFO - evaluating now!
2024-03-26 22:48:36,286 - INFO - Epoch [150/300] (148282) train_loss: 22.3673, val_loss: 26.2065, lr: 0.000545, 251.14s
2024-03-26 22:52:34,935 - INFO - epoch complete!
2024-03-26 22:52:34,935 - INFO - evaluating now!
2024-03-26 22:52:53,028 - INFO - Epoch [151/300] (149264) train_loss: 22.3369, val_loss: 25.7467, lr: 0.000541, 256.74s
2024-03-26 22:56:48,091 - INFO - epoch complete!
2024-03-26 22:56:48,091 - INFO - evaluating now!
2024-03-26 22:57:06,157 - INFO - Epoch [152/300] (150246) train_loss: 22.3538, val_loss: 25.8815, lr: 0.000536, 253.13s
2024-03-26 23:00:58,346 - INFO - epoch complete!
2024-03-26 23:00:58,347 - INFO - evaluating now!
2024-03-26 23:01:16,327 - INFO - Epoch [153/300] (151228) train_loss: 22.2851, val_loss: 25.9187, lr: 0.000531, 250.17s
2024-03-26 23:05:04,336 - INFO - epoch complete!
2024-03-26 23:05:04,336 - INFO - evaluating now!
2024-03-26 23:05:22,279 - INFO - Epoch [154/300] (152210) train_loss: 22.2931, val_loss: 25.8597, lr: 0.000526, 245.95s
2024-03-26 23:09:07,909 - INFO - epoch complete!
2024-03-26 23:09:07,910 - INFO - evaluating now!
2024-03-26 23:09:25,837 - INFO - Epoch [155/300] (153192) train_loss: 22.2900, val_loss: 26.0125, lr: 0.000522, 243.56s
2024-03-26 23:13:15,661 - INFO - epoch complete!
2024-03-26 23:13:15,662 - INFO - evaluating now!
2024-03-26 23:13:33,536 - INFO - Epoch [156/300] (154174) train_loss: 22.2470, val_loss: 25.9716, lr: 0.000517, 247.70s
2024-03-26 23:17:19,346 - INFO - epoch complete!
2024-03-26 23:17:19,347 - INFO - evaluating now!
2024-03-26 23:17:37,549 - INFO - Epoch [157/300] (155156) train_loss: 22.1934, val_loss: 25.9997, lr: 0.000512, 244.01s
2024-03-26 23:21:22,818 - INFO - epoch complete!
2024-03-26 23:21:22,818 - INFO - evaluating now!
2024-03-26 23:21:40,714 - INFO - Epoch [158/300] (156138) train_loss: 22.1875, val_loss: 25.9157, lr: 0.000508, 243.16s
2024-03-26 23:25:26,426 - INFO - epoch complete!
2024-03-26 23:25:26,427 - INFO - evaluating now!
2024-03-26 23:25:44,326 - INFO - Epoch [159/300] (157120) train_loss: 22.1577, val_loss: 25.9174, lr: 0.000503, 243.61s
2024-03-26 23:29:29,751 - INFO - epoch complete!
2024-03-26 23:29:29,751 - INFO - evaluating now!
2024-03-26 23:29:47,650 - INFO - Epoch [160/300] (158102) train_loss: 22.1430, val_loss: 25.9236, lr: 0.000498, 243.32s
2024-03-26 23:33:35,243 - INFO - epoch complete!
2024-03-26 23:33:35,244 - INFO - evaluating now!
2024-03-26 23:33:53,159 - INFO - Epoch [161/300] (159084) train_loss: 22.1142, val_loss: 25.8872, lr: 0.000494, 245.51s
2024-03-26 23:37:47,806 - INFO - epoch complete!
2024-03-26 23:37:47,807 - INFO - evaluating now!
2024-03-26 23:38:06,135 - INFO - Epoch [162/300] (160066) train_loss: 22.1013, val_loss: 26.0263, lr: 0.000489, 252.97s
2024-03-26 23:42:01,362 - INFO - epoch complete!
2024-03-26 23:42:01,363 - INFO - evaluating now!
2024-03-26 23:42:19,641 - INFO - Epoch [163/300] (161048) train_loss: 22.0653, val_loss: 26.2103, lr: 0.000484, 253.51s
2024-03-26 23:46:10,109 - INFO - epoch complete!
2024-03-26 23:46:10,109 - INFO - evaluating now!
2024-03-26 23:46:28,616 - INFO - Epoch [164/300] (162030) train_loss: 22.0737, val_loss: 25.9658, lr: 0.000480, 248.97s
2024-03-26 23:50:19,929 - INFO - epoch complete!
2024-03-26 23:50:19,930 - INFO - evaluating now!
2024-03-26 23:50:38,220 - INFO - Epoch [165/300] (163012) train_loss: 22.0419, val_loss: 25.7911, lr: 0.000475, 249.60s
2024-03-26 23:54:33,487 - INFO - epoch complete!
2024-03-26 23:54:33,487 - INFO - evaluating now!
2024-03-26 23:54:51,796 - INFO - Epoch [166/300] (163994) train_loss: 21.9985, val_loss: 26.0760, lr: 0.000470, 253.58s
2024-03-26 23:54:51,796 - WARNING - Early stopping at epoch: 166
2024-03-26 23:54:51,796 - INFO - Trained totally 167 epochs, average train time is 267.071s, average eval time is 21.602s
2024-03-26 23:54:51,845 - INFO - Loaded model at 116
2024-03-26 23:54:51,846 - INFO - Saved model at ./libcity/cache/82263/model_cache/PDFormer_PeMS03.m
2024-03-26 23:54:51,887 - INFO - Start evaluating ...
2024-03-26 23:55:37,905 - INFO - Note that you select the average mode to evaluate!
2024-03-26 23:55:37,911 - INFO - Evaluate result is saved at ./libcity/cache/82263/evaluate_cache/2024_03_26_23_55_37_PDFormer_PeMS03_average.csv
2024-03-26 23:55:37,920 - INFO - 
          MAE  MAPE       RMSE  masked_MAE  masked_MAPE  masked_RMSE
1   12.252642   inf  20.554005   12.297082     0.133518    20.569910
2   12.634356   inf  21.362906   12.680154     0.136329    21.377508
3   12.968006   inf  22.058403   13.013792     0.140033    22.071157
4   13.265333   inf  22.662127   13.311244     0.143065    22.673225
5   13.533265   inf  23.196260   13.580376     0.144820    23.206097
6   13.774722   inf  23.663961   13.822412     0.146556    23.672579
7   13.999992   inf  24.087744   14.047762     0.148543    24.095190
8   14.216702   inf  24.485403   14.264273     0.150841    24.491703
9   14.419967   inf  24.853302   14.467460     0.152907    24.858650
10  14.617017   inf  25.198315   14.664185     0.155419    25.202696
11  14.806656   inf  25.519869   14.853522     0.157973    25.523449
12  14.996381   inf  25.841072   15.043115     0.160324    25.843996
